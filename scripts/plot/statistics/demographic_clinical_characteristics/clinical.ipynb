{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system is macOS\n"
     ]
    }
   ],
   "source": [
    "# first make sure the input data are good \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer, f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from scipy import stats\n",
    "\n",
    "if sys.platform == 'darwin':\n",
    "    print(\"Current system is macOS\")\n",
    "    main_fold_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction'\n",
    "elif sys.platform == 'linux':\n",
    "    print(\"Current system is Ubuntu\")\n",
    "    main_fold_path = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning'\n",
    "else:\n",
    "    print(\"Current system is neither macOS nor Ubuntu\")\n",
    "    \n",
    "sys.path.append(main_fold_path)    \n",
    "os.chdir(main_fold_path)\n",
    "from utils.hyperopt_utils import get_best_hyperparameters, get_best_hyperparameters_skf_inside_loocv_monitoring_recall_bacc\n",
    "from utils.fnirs_utils import print_md_table_val_test_AUC\n",
    "\n",
    "from scripts.fusion_model.fusion_model_utils import derive_average_MMDR_score\n",
    "from scripts.fusion_model.fusion_model_utils import replace_nan_with_mean\n",
    "from scripts.fusion_model.fusion_model_utils import impute_nan_data\n",
    "from scripts.fusion_model.fusion_model_utils import process_with_nan_using_imputation_zscore\n",
    "from scripts.fusion_model.fusion_model_utils import read_base_T2_SDS_CGI \n",
    "from scripts.fusion_model.fusion_model_utils import read_pychiatry\n",
    "from scripts.fusion_model.fusion_model_utils import read_HAMD_score\n",
    "from scripts.fusion_model.fusion_model_utils import read_demographic\n",
    "from scripts.fusion_model.fusion_model_utils import plot_avg_auc\n",
    "from scripts.fusion_model.fusion_model_utils import train_xgboost_shuffle_feature \n",
    "from scripts.fusion_model.fusion_model_utils import save_shap\n",
    "from scripts.fusion_model.fusion_model_utils import read_dose_information\n",
    "import time\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "K_FOLD = 5\n",
    "fold_path = 'allData/prognosis_mix_hb/pretreatment_response'\n",
    "MMDR_path = 'allData/prognosis_mix_hb/pretreatment_response/MDDR/MDDR_derived_from_load_evaluate.npy'\n",
    "HAMD_path = 'allData/prognosis_mix_hb/pretreatment_response/label_hamd.npy'\n",
    "\n",
    "base_T2_SDS_CGI = read_base_T2_SDS_CGI(fold_path)\n",
    "pyschiatry = read_pychiatry(fold_path)\n",
    "HAMD_score = np.load(HAMD_path, allow_pickle=True)\n",
    "demographic = read_demographic(fold_path)\n",
    "dose = read_dose_information(fold_path)\n",
    "\n",
    "\n",
    "\n",
    "pro_base_T2_SDS_CGI = process_with_nan_using_imputation_zscore(base_T2_SDS_CGI)\n",
    "pro_pyschiatry = process_with_nan_using_imputation_zscore(pyschiatry)\n",
    "pro_pyschiatry = np.concatenate((pro_pyschiatry[:, :-3], pro_pyschiatry[:, -2:]), axis=1) # must remove the -3rd column, because its existen will cause nan value of that column which is On antidpressant(s) ONLY\n",
    "# pro_pyschiatry = np.concatenate((pro_pyschiatry[:, :1], pro_pyschiatry[:, 2:]), axis=1) # delete Current psychiatric comorbidities — Binary because already have Current psychiatric comorbidities — Coded\n",
    "pro_HAMD_score = HAMD_score# process_with_nan_using_imputation_zscore(HAMD_score)\n",
    "pro_demographic = process_with_nan_using_imputation_zscore(demographic)\n",
    "pro_dose = process_with_nan_using_imputation_zscore(dose)\n",
    "\n",
    "\n",
    "label = np.load(fold_path + '/label.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.40625"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(HAMD_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84 68 108 2 12 5 1 6 18 74 12 56.328125 12 48 276 168 121 4 48 48 3 120 6\n",
      " 10 96 26 36 6 0 72 144 3 24 24 15 60 108 96 22 84 180 63 24 144 6 120 60\n",
      " 192 120 60 36 48 8 96 72 48 180 36 3 12 3 0 0 24]\n",
      "[3 1 1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1 nan 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1\n",
      " 1 1 1 3 1 1 3 1 1 1 3 1 3 1 '1' 3 '1' '1' '1' '1' '1' '1' 3 '1' '1' '1'\n",
      " '1' '1']\n",
      "18 nan\n"
     ]
    }
   ],
   "source": [
    "# Process nan value \n",
    "\n",
    "\n",
    "dose_0 = dose[:, 0].copy()\n",
    "dose_0 = np.nan_to_num(dose_0)\n",
    "median_dose_0 = np.median(dose_0)\n",
    "dose_0 = dose[:, 0].copy()\n",
    "dose_0 = np.nan_to_num(dose_0, nan=median_dose_0)\n",
    "\n",
    "dose_1 = dose[:, 1].copy()  \n",
    "dose_1 = np.nan_to_num(dose_1)\n",
    "mean_dose_1 = np.mean(dose_1)\n",
    "dose_1 = dose[:, 1].copy()\n",
    "dose_1 = np.nan_to_num(dose_1, nan=mean_dose_1)\n",
    "\n",
    "pys_6 = pyschiatry[:, 6].copy()\n",
    "pys_6 = np.nan_to_num(pys_6)\n",
    "pys_6[11]=0\n",
    "\n",
    "mean_pys_6 = np.mean(pys_6)\n",
    "pys_6[11] = mean_pys_6\n",
    "# print(mean_pys_6)\n",
    "# pys_6 = pyschiatry[:, 6].copy()\n",
    "# pys_6 = np.nan_to_num(pys_6, nan=mean_pys_6)\n",
    "pyschiatry[:, 6] = pys_6\n",
    "print(pyschiatry[:, 6])\n",
    "\n",
    "\n",
    "dem_3 = demographic[:, 3].copy()\n",
    "dem_3 = np.nan_to_num(dem_3)\n",
    "print(dem_3)\n",
    "for i in range(len(dem_3)):\n",
    "    if type(dem_3[i]) in [str, int]:\n",
    "        pass\n",
    "    else:\n",
    "        print(i, dem_3[i])\n",
    "        dem_3[i] = 1\n",
    "dem_3 = np.array(dem_3, dtype=int)        \n",
    "demographic[:, 3] = dem_3\n",
    "\n",
    "\n",
    "# hamd_1 = HAMD_score[:, 1].copy()\n",
    "# hamd_1 = np.nan_to_num(hamd_1)\n",
    "# for i in range(len(hamd_1)):\n",
    "#     if type(hamd_1[i]) in [str, int]:\n",
    "#         pass\n",
    "#     else:\n",
    "#         hamd_1[i] = 0\n",
    "# mean_hamd_1 = np.mean(hamd_1)\n",
    "\n",
    "# hamd_1 = HAMD_score[:, 1]\n",
    "# for i in range(len(hamd_1)):\n",
    "#     if type(hamd_1[i]) in [str, int]:\n",
    "#         pass\n",
    "#     else:\n",
    "#         hamd_1[i] = mean_hamd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_val = {}\n",
    "\n",
    "chi_analysis_name = ['Current psychiatric comorbidities - coded', ]\n",
    "\n",
    "\n",
    "chi_analysis_arr = ['Perceived social support', \n",
    "            'Past trauma', \n",
    "            'Current psychiatric comorbidities - coded',\n",
    "            'Family history of psychiatric illness',\n",
    "            'Past EmD visit(s) because of depression',\n",
    "            'Type of episode',\n",
    "            'Antidepressant',\n",
    "            'Sex',\n",
    "            'Ethnicity',\n",
    "            'Handedness'] \n",
    "\n",
    "\n",
    "ttest_analysis_arr = ['Age of depression onset (years)',\n",
    "            'Duration of depression (years)',\n",
    "            'Duration of untreated depression (months)',\n",
    "            'Fluoxetine equivalent dose (mg/day)',\n",
    "            'Age (years)',\n",
    "            'Education (years)',\n",
    "            'Baseline HAM-D score',\n",
    "            'HAM-D score at 6 month'] \n",
    "\n",
    "name_to_val['Age (years)'] = demographic[:, 0] # ttest\n",
    "name_to_val['Sex'] = demographic[:, 1] # chi \n",
    "name_to_val['Ethnicity'] = demographic[:, 2] # chi \n",
    "name_to_val['Handedness'] = demographic[:, 3] # chi \n",
    "name_to_val['Education (years)'] = demographic[:, 4] # ttest\n",
    "name_to_val['Baseline HAM-D score'] = HAMD_score[:, 0] # chi \n",
    "name_to_val['HAM-D score at 6 month'] = HAMD_score[:, 1] # chi \n",
    "\n",
    "# print(name_to_val)\n",
    "\n",
    "\n",
    "name_to_val['Perceived social support'] = demographic[:, -1] # chi \n",
    "name_to_val['Past trauma'] = pyschiatry[:, 0] # chi\n",
    "name_to_val['Current psychiatric comorbidities - coded'] = pyschiatry[:, 2] # chi \n",
    "name_to_val['Family history of psychiatric illness'] = pyschiatry[:, 3] # chi \n",
    "name_to_val['Age of depression onset (years)'] = pyschiatry[:, 4] # ttest\n",
    "name_to_val['Duration of depression (years)'] = pyschiatry[:, 5] # ttest\n",
    "name_to_val['Duration of untreated depression (months)'] = pyschiatry[:, 6] # ttest\n",
    "name_to_val['Past EmD visit(s) because of depression'] = pyschiatry[:, 7] # chi\n",
    "name_to_val['Type of episode'] = pyschiatry[:, 8] # chi\n",
    "name_to_val['Antidepressant'] = dose_0 # pyschiatry[:, ] # chi\n",
    "name_to_val['Fluoxetine equivalent dose (mg/day)'] = dose_1 # pyschiatry[:, ] # ttest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 12, -0.52],\n",
       "       [21, 14, -0.3333333333333333],\n",
       "       [27, 17, -0.37037037037037035],\n",
       "       [20, 20, 0.0],\n",
       "       [17, 18, 0.058823529411764705],\n",
       "       [10, 8, -0.2],\n",
       "       [15, 17, 0.13333333333333333],\n",
       "       [21, 21, 0.0],\n",
       "       [22, 19, -0.13636363636363635],\n",
       "       [21, 11, -0.47619047619047616],\n",
       "       [21, 21, 0.0],\n",
       "       [21, 23, 0.09523809523809523],\n",
       "       [19, 19, 0.0],\n",
       "       [15, 16, 0.06666666666666667],\n",
       "       [23, 15, -0.34782608695652173],\n",
       "       [21, 18, -0.14285714285714285],\n",
       "       [19, 21, 0.10526315789473684],\n",
       "       [27, 21, -0.2222222222222222],\n",
       "       [15, 11, -0.26666666666666666],\n",
       "       [19, 25, 0.3157894736842105],\n",
       "       [15, 19, 0.26666666666666666],\n",
       "       [25, 22, -0.12],\n",
       "       [22, 11, -0.5],\n",
       "       [24, 16, -0.3333333333333333],\n",
       "       [30, 22, -0.26666666666666666],\n",
       "       [25, 24, -0.04],\n",
       "       [26, 28, 0.07692307692307693],\n",
       "       [24, 16, -0.3333333333333333],\n",
       "       [26, 26, 0.0],\n",
       "       [20, 9, -0.55],\n",
       "       [23, 26, 0.13043478260869565],\n",
       "       [16, 17, 0.0625],\n",
       "       [20, 24, 0.2],\n",
       "       [20, 24, 0.2],\n",
       "       [24, 16, -0.3333333333333333],\n",
       "       [25, 19, -0.24],\n",
       "       [22, 20, -0.09090909090909091],\n",
       "       [21, 20, -0.047619047619047616],\n",
       "       [16, 12, -0.25],\n",
       "       [31, 24, -0.22580645161290322],\n",
       "       [26, 19, -0.2692307692307692],\n",
       "       [23, 9, -0.6086956521739131],\n",
       "       [16, 14, -0.125],\n",
       "       [12, 14, 0.16666666666666666],\n",
       "       [23, 24, 0.043478260869565216],\n",
       "       [21, 16.25, nan],\n",
       "       [15, 12, -0.2],\n",
       "       [14, 5, -0.6428571428571429],\n",
       "       [23, 26, 0.13043478260869565],\n",
       "       [26, 22, -0.15384615384615385],\n",
       "       [18, 17, -0.05555555555555555],\n",
       "       [19, 13, -0.3157894736842105],\n",
       "       [16, 10, -0.375],\n",
       "       [10, 10, 0.0],\n",
       "       [24, 24, 0.0],\n",
       "       [14, 9, -0.35714285714285715],\n",
       "       [13, 6, -0.5384615384615384],\n",
       "       [17, 14, -0.17647058823529413],\n",
       "       [14, 6, -0.5714285714285714],\n",
       "       [8, 9, 0.125],\n",
       "       [7, 4, -0.42857142857142855],\n",
       "       [11, 3, -0.7272727272727273],\n",
       "       [8, 8, 0.0],\n",
       "       [19, 20, 0.05263157894736842]], dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAMD_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Sex | 50 | 78.12 | 39 | 78.00 | 11 | 78.57 | 1.0000 |\n",
      " | Ethnicity | | | | | | | 1.0000 |\n",
      " | 1 | 44 | 68.75 | 35 | 70.00 | 9 | 64.29 |  |\n",
      " | 2 | 12 | 18.75 | 9 | 18.00 | 3 | 21.43 |  |\n",
      " | 3 | 7 | 10.94 | 5 | 10.00 | 2 | 14.29 |  |\n",
      " | 4 | 1 | 1.56 | 1 | 2.00 | 0 | 0.00 |  |\n",
      " | Handedness | 11 | 17.19 | 7 | 14.00 | 4 | 28.57 | 1.0000 |\n",
      " | Perceived social support | | | | | | | 1.0000 |\n",
      " | 1 | 15 | 23.44 | 10 | 20.00 | 5 | 35.71 |  |\n",
      " | 2 | 41 | 64.06 | 35 | 70.00 | 6 | 42.86 |  |\n",
      " | 3 | 8 | 12.50 | 5 | 10.00 | 3 | 21.43 |  |\n",
      " | Past trauma | 31 | 48.44 | 25 | 50.00 | 6 | 42.86 | 1.0000 |\n",
      " | Current psychiatric comorbidities - coded | | | | | | | 1.0000 |\n",
      " | 0 | 45 | 70.31 | 32 | 64.00 | 13 | 92.86 |  |\n",
      " | 1 | 10 | 15.62 | 10 | 20.00 | 0 | 0.00 |  |\n",
      " | 2 | 5 | 7.81 | 5 | 10.00 | 0 | 0.00 |  |\n",
      " | 3 | 2 | 3.12 | 2 | 4.00 | 0 | 0.00 |  |\n",
      " | 4 | 2 | 3.12 | 1 | 2.00 | 1 | 7.14 |  |\n",
      " | Family history of psychiatric illness | 28 | 43.75 | 21 | 42.00 | 7 | 50.00 | 1.0000 |\n",
      " | Past EmD visit(s) because of depression | 33 | 51.56 | 26 | 52.00 | 7 | 50.00 | 1.0000 |\n",
      " | Type of episode | 56 | 87.50 | 44 | 88.00 | 12 | 85.71 | 1.0000 |\n",
      " | Antidepressant | 63 | 98.44 | 49 | 98.00 | 14 | 100.00 | 1.0000 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      " | Age (years) | 28.69 | 7.30 | 28.46 | 6.95 | 29.50 | 8.39 | 0.6440 |\n",
      " | Education (years) | 14.48 | 1.85 | 14.42 | 1.94 | 14.71 | 1.48 | 0.6064 |\n",
      " | Baseline HAM-D score | 19.55 | 5.38 | 19.56 | 5.12 | 19.50 | 6.22 | 0.9712 |\n",
      " | HAM-D score at 6 month | 15.27 | 6.44 | 17.52 | 5.09 | 7.21 | 3.75 | 0.0000 |\n",
      " | Age of depression onset (years) | 20.89 | 7.49 | 20.50 | 6.84 | 22.29 | 9.32 | 0.4383 |\n",
      " | Duration of depression (years) | 8.11 | 6.64 | 8.22 | 6.74 | 7.71 | 6.23 | 0.8049 |\n",
      " | Duration of untreated depression (months) | 57.21 | 58.60 | 49.65 | 48.15 | 84.21 | 80.53 | 0.0522 |\n",
      " | Fluoxetine equivalent dose (mg/day) | 32.18 | 17.85 | 32.77 | 18.91 | 30.10 | 13.21 | 0.6273 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ttest_anlysis(key, value, label):\n",
    "    \"\"\" compute statistics of values using ttest_in from scipy.stats \n",
    "    Args: \n",
    "        key: str, the name of the value\n",
    "        value: np.array, the value to be analyzed\n",
    "        label: np.array, the label of the value, either 0 or 1\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Outputs:\n",
    "        | key | all_mean | all_std | non_responder_mean | non_responder_std | responder_mean | responder_std | p_value |\n",
    "    \n",
    "    \"\"\"\n",
    "    all_mean = np.mean(value)\n",
    "    all_std = np.std(value)\n",
    "    \n",
    "    responder_value = value[label == 1].tolist()\n",
    "    non_responder_value = value[label == 0].tolist()\n",
    "    \n",
    "    responder_mean = np.mean(responder_value)\n",
    "    responder_std = np.std(responder_value)\n",
    "    \n",
    "    non_responder_mean = np.mean(non_responder_value)\n",
    "    non_responder_std = np.std(non_responder_value)\n",
    "\n",
    "    _, p_value = stats.ttest_ind(responder_value, non_responder_value)\n",
    "    \n",
    "    print(f' | {key} | {all_mean:.2f} | {all_std:.2f} | {non_responder_mean:.2f} | {non_responder_std:.2f} | {responder_mean:.2f} | {responder_std:.2f} | {p_value:.4f} |')\n",
    "\n",
    "    # # check if the data is normal distribution\n",
    "    # plt.figure()\n",
    "    # plt.title(key)\n",
    "    # plt.hist(value)\n",
    "\n",
    "def chi_anlysis(key, value, label):\n",
    "    \"\"\" compute statistics of values using chi2_contingency from scipy.stats \n",
    "    Args: \n",
    "        key: str, the name of the value\n",
    "        value: np.array, the value to be analyzed\n",
    "        label: np.array, the label of the value, either 0 or 1\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Outputs:\n",
    "        | key | all_mean | all_std | non_responder_mean | non_responder_std | responder_mean | responder_std | p_value |\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # make sure the value is integer\n",
    "    value = value.astype(int)\n",
    "    \n",
    "    responder_value = value[label == 1]\n",
    "    non_responder_value = value[label == 0]\n",
    "    \n",
    "    unique = np.unique(value)\n",
    "    if len(unique) == 2:\n",
    "        # pass\n",
    "        max_value = np.max(unique)\n",
    "        \n",
    "        value_max_arr = value[value == max_value]\n",
    "        N_value_max_arr = len(value_max_arr)\n",
    "        percent_value_max_arr = N_value_max_arr / len(value) * 100\n",
    "        \n",
    "        responder_max_arr = responder_value[responder_value == max_value]\n",
    "        N_responder_value_max_arr = len(responder_max_arr)\n",
    "        percent_responder_value_max_arr = N_responder_value_max_arr / len(responder_value) * 100\n",
    "        \n",
    "        non_responder_max_arr = non_responder_value[non_responder_value == max_value]\n",
    "        N_non_responder_value_max_arr = len(non_responder_max_arr)\n",
    "        percent_non_responder_value_max_arr = N_non_responder_value_max_arr / len(non_responder_value) * 100\n",
    "        \n",
    "        if max_value == 1:\n",
    "            responder_value +=1 \n",
    "            non_responder_value += 1\n",
    "        p_value = stats.chi2_contingency(responder_value.tolist(), non_responder_value.tolist())\n",
    "        print(f' | {key} | {N_value_max_arr} | {percent_value_max_arr:.2f} | {N_non_responder_value_max_arr} | {percent_non_responder_value_max_arr:.2f} | {N_responder_value_max_arr} | {percent_responder_value_max_arr:.2f} | {p_value[1]:.4f} |')\n",
    "    else:\n",
    "        \n",
    "        if np.min(unique) == 0:\n",
    "            s_responder_value = responder_value+1\n",
    "            s_non_responder_value = non_responder_value+1    \n",
    "        else:\n",
    "            s_responder_value = responder_value\n",
    "            s_non_responder_value = non_responder_value\n",
    "        _, p_value, _, _ = stats.chi2_contingency(s_responder_value.tolist(), s_non_responder_value.tolist())\n",
    "        \n",
    "        print(f' | {key} | | | | | | | {p_value:.4f} |')\n",
    "        for unique_val in unique:\n",
    "            value_unique = value[value == unique_val]\n",
    "            N_value_unique = len(value_unique)\n",
    "            percent_value_unique = N_value_unique / len(value) * 100\n",
    "            \n",
    "            responder_unique = responder_value[responder_value == unique_val]\n",
    "            N_responder_value_unique = len(responder_unique)\n",
    "            percent_responder_value_unique = N_responder_value_unique / len(responder_value) * 100\n",
    "            \n",
    "            non_responder_unique = non_responder_value[non_responder_value == unique_val]\n",
    "            N_non_responder_value_unique = len(non_responder_unique)\n",
    "            percent_non_responder_value_unique = N_non_responder_value_unique / len(non_responder_value) * 100\n",
    "            \n",
    "            print(f' | {unique_val} | {N_value_unique} | {percent_value_unique:.2f} | {N_non_responder_value_unique} | {percent_non_responder_value_unique:.2f} | {N_responder_value_unique} | {percent_responder_value_unique:.2f} |  |')\n",
    "        \n",
    "        \n",
    "    all_mean = np.mean(value)\n",
    "    all_std = np.std(value)\n",
    "    \n",
    "\n",
    "    \n",
    "    responder_mean = np.mean(responder_value)\n",
    "    responder_std = np.std(responder_value)\n",
    "    \n",
    "    non_responder_mean = np.mean(non_responder_value)\n",
    "    non_responder_std = np.std(non_responder_value)\n",
    "\n",
    "    # _, p_value = stats.chi2_contingency(responder_value, non_responder_value)\n",
    "    \n",
    "    # print(f' | {key} | {all_mean:.2f} | {all_std:.2f} | {non_responder_mean:.2f} | {non_responder_std:.2f} | {responder_mean:.2f} | {responder_std:.2f} | {p_value:.4f} |')\n",
    "    \n",
    "\n",
    "for key, value in name_to_val.items():\n",
    "\n",
    "    if key in chi_analysis_arr:\n",
    "        chi_anlysis(key, value, label)\n",
    "    elif key in ttest_analysis_arr:\n",
    "        # print(f' {key} is using ttest analysis')\n",
    "        # ttest_anlysis(key, value, label)\n",
    "        pass\n",
    "    else:\n",
    "        print(f'- error - {key}' * 100)\n",
    "print('-' * 100)\n",
    "for key, value in name_to_val.items():\n",
    "    mean_val = np.mean(value)\n",
    "    # print(f' | {key} |')\n",
    "    # print(mean_val)\n",
    "    # make sure all values are process \n",
    "    if key in chi_analysis_arr:\n",
    "        pass\n",
    "        # chi_anlysis(key, value, label)\n",
    "        # print(f' {key} is using chi analysis')\n",
    "    elif key in ttest_analysis_arr:\n",
    "        # print(f' {key} is using ttest analysis')\n",
    "        ttest_anlysis(key, value, label)\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        print(f'- error - {key}' * 100)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2565330058.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[97], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    a=[47 22 35 23 20 23 10 19 12 19 14 17 21 30]\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
