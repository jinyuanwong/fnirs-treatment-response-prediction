{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system is Ubuntu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "import subprocess\n",
    "if sys.platform == 'darwin':\n",
    "    print(\"Current system is macOS\")\n",
    "    main_fold_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction'\n",
    "elif sys.platform == 'linux':\n",
    "    print(\"Current system is Ubuntu\")\n",
    "    main_fold_path = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning'\n",
    "else:\n",
    "    print(\"Current system is neither macOS nor Ubuntu\")\n",
    "os.chdir(main_fold_path)\n",
    "from utils.fnirs_utils import print_md_table_val_test_AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for itr in range(4, 14):\n",
    "    bash_code = f\"conda run -n tf python scripts/plot/DL/read_LOO_nestedCV_gnntr_auc.py --max 4 --K_FOLD 5 --result_path results/gnn_transformer/prognosis_mix_hb/pretreatment_response/loocv_v{itr}l1_rate_0.01_l2_rate_0.001_d_model_16_batch_size_64_n_layers_6 \"\n",
    "    subprocess.run(bash_code, shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Metrics:\n",
      "Accuracy: 0.6328571428571428 (±0.03142135559299737)\n",
      "Sensitivity: 0.43571428571428567 (±0.05270462766947301)\n",
      "Specificity: 0.8299999999999998 (±0.019436506316150997)\n",
      "AUC: 0.5992857142857143 (±0.04702606986731438)\n",
      "\n",
      "Average Validation Metrics:\n",
      "Accuracy: 0.8298838578125004 (±0.012694089454970334)\n",
      "Sensitivity: 0.7053125000000005 (±0.023664410837326493)\n",
      "Specificity: 0.9544552156250002 (±0.003440104679139837)\n",
      "AUC: 0.829882626645267 (±0.01269407719856165)\n",
      "| Model Name | Testing Set |             |             |             | Validation Set |             |             |             |\n",
      "|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "|            | Balanced Accuracy | Sensitivity | Specificity | AUC | Balanced Accuracy | Sensitivity | Specificity | AUC |\n",
      "| Mean   | 63.2857  | 43.5714  | 83.0000  | 59.9286  | 82.9884  | 70.5313  | 95.4455  | 82.9883  |"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric         | Test Set Average | Validation Set Average |\n",
    "|----------------|------------------|------------------------|\n",
    "| Accuracy       | 0.7437 | 0.9101 |\n",
    "| Sensitivity    | 0.4358 | 0.7063 |\n",
    "| Specificity    | 0.8320 | 0.9552 |\n",
    "| AUC            | 0.5993 | 0.8327 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Metrics:\n",
      "Accuracy: 0.6328571428571428 (±0.030583301748906402)\n",
      "Sensitivity: 0.4357142857142858 (±0.05129891760425773)\n",
      "Specificity: 0.8300000000000001 (±0.01891810605853834)\n",
      "AUC: 0.5992857142857143 (±0.045771815304421606)\n",
      "\n",
      "Average Validation Metrics:\n",
      "Accuracy: 0.8298838578125004 (±0.012355519388077888)\n",
      "Sensitivity: 0.7053125000000003 (±0.023033246137519933)\n",
      "Specificity: 0.9544552156250005 (±0.0033483520193319017)\n",
      "AUC: 0.829882626645267 (±0.012355507458565678)\n",
      "| Model Name | Testing Set |             |             |             | Validation Set |             |             |             |\n",
      "|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "|            | Balanced Accuracy | Sensitivity | Specificity | AUC | Balanced Accuracy | Sensitivity | Specificity | AUC |\n",
      "| Mean   | 63.2857  | 43.5714  | 83.0000  | 59.9286  | 82.9884  | 70.5313  | 95.4455  | 82.9883  |\n",
      "| STD   | 3.0583  | 5.1299  | 1.8918  | 4.5772  | 1.2356  | 2.3033  | 0.3348  | 1.2356  |\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'results/gnn_transformer/prognosis_mix_hb/metrics.txt'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Calculating the balanced accuracy for each metric in the test set\n",
    "balanced_test_accuracy = (df['Test Sensitivity'] + df['Test Specificity']) / 2\n",
    "\n",
    "# Calculating the balanced accuracy for each metric in the validation set\n",
    "balanced_validation_accuracy = (df['Validation Sensitivity'] + df['Validation Specificity']) / 2\n",
    "\n",
    "df['Test Accuracy'] = balanced_test_accuracy\n",
    "df['Validation Accuracy'] = balanced_validation_accuracy\n",
    "\n",
    "\n",
    "# Calculating the mean and standard deviation for each metric in the test set\n",
    "average_test_accuracy = df['Test Accuracy'].mean()\n",
    "std_test_accuracy = df['Test Accuracy'].std()\n",
    "\n",
    "average_test_sensitivity = df['Test Sensitivity'].mean()\n",
    "std_test_sensitivity = df['Test Sensitivity'].std()\n",
    "\n",
    "average_test_specificity = df['Test Specificity'].mean()\n",
    "std_test_specificity = df['Test Specificity'].std()\n",
    "\n",
    "average_test_auc = df['Test AUC'].mean()\n",
    "std_test_auc = df['Test AUC'].std()\n",
    "\n",
    "# Calculating the mean and standard deviation for each metric in the validation set\n",
    "average_validation_accuracy = df['Validation Accuracy'].mean()\n",
    "std_validation_accuracy = df['Validation Accuracy'].std()\n",
    "\n",
    "average_validation_sensitivity = df['Validation Sensitivity'].mean()\n",
    "std_validation_sensitivity = df['Validation Sensitivity'].std()\n",
    "\n",
    "average_validation_specificity = df['Validation Specificity'].mean()\n",
    "std_validation_specificity = df['Validation Specificity'].std()\n",
    "\n",
    "average_validation_auc = df['Validation AUC'].mean()\n",
    "std_validation_auc = df['Validation AUC'].std()\n",
    "\n",
    "# Printing the results with mean and standard deviation\n",
    "print(\"Average Test Metrics:\")\n",
    "print(f\"Accuracy: {average_test_accuracy} (±{std_test_accuracy})\")\n",
    "print(f\"Sensitivity: {average_test_sensitivity} (±{std_test_sensitivity})\")\n",
    "print(f\"Specificity: {average_test_specificity} (±{std_test_specificity})\")\n",
    "print(f\"AUC: {average_test_auc} (±{std_test_auc})\")\n",
    "\n",
    "print(\"\\nAverage Validation Metrics:\")\n",
    "print(f\"Accuracy: {average_validation_accuracy} (±{std_validation_accuracy})\")\n",
    "print(f\"Sensitivity: {average_validation_sensitivity} (±{std_validation_sensitivity})\")\n",
    "print(f\"Specificity: {average_validation_specificity} (±{std_validation_specificity})\")\n",
    "print(f\"AUC: {average_validation_auc} (±{std_validation_auc})\")\n",
    "\n",
    "mean_test_metrics = [average_test_accuracy, average_test_sensitivity, average_test_specificity, average_test_auc]\n",
    "mean_val_metrics = [average_validation_accuracy, average_validation_sensitivity, average_validation_specificity, average_validation_auc]\n",
    "\n",
    "std_test_metrics = [std_test_accuracy, std_test_sensitivity, std_test_specificity, std_test_auc]\n",
    "std_val_metrics = [std_validation_accuracy, std_validation_sensitivity, std_validation_specificity, std_validation_auc]\n",
    "\n",
    "\n",
    "\n",
    "print_md_table_val_test_AUC('Mean', mean_test_metrics, mean_val_metrics, already_balanced_accuracy=True)\n",
    "print_md_table_val_test_AUC('STD', std_test_metrics, std_val_metrics, already_balanced_accuracy=True, print_table_header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
