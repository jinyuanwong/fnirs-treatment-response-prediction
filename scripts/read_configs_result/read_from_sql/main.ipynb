{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system is Ubuntu\n",
      "experiment_args {'model_name': 'jamba_MTL', 'run_itr': 'jamba_20240720', 'launcher_name': 'nested_CV_train.py', 'task_name': '20240720_normalization_method'}\n",
      "config_names ['STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_batchnorm', 'STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_rmsnorm', 'STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_layernorm']\n",
      "| Depression | Testing Set |             |             |             | Validation Set |             |             |      Threshold=0       |             |\n",
      "|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "|      Normalization_method      | bAcc(%) | Sen(%) | Spe(%) | AUC(%) | bAcc(%) | Sen(%) | Spe(%) | AUC(%) | Duration(s) |\n",
      "| batchnorm   | 59.87  | 48.71  | 71.04  | 65.35  | 59.84  | 49.31  | 70.36  | 65.07  | 2.9  |\n",
      "| rmsnorm   | 58.69  | 47.15  | 70.22  | 65.13  | 58.62  | 47.02  | 70.21  | 64.65  | 2.6  |\n",
      "| layernorm   | 58.47  | 47.69  | 69.25  | 65.01  | 58.69  | 48.00  | 69.38  | 64.65  | 2.7  |\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "def set_path():\n",
    "    if sys.platform == 'darwin':\n",
    "        print(\"Current system is macOS\")\n",
    "        main_fold_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction'\n",
    "    elif sys.platform == 'linux':\n",
    "        print(\"Current system is Ubuntu\")\n",
    "        main_fold_path = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning'\n",
    "        # main_fold_path = '/root/autodl-tmp/fnirs-treatment-response-prediction'\n",
    "    else:\n",
    "        print(\"Current system is neither macOS nor Ubuntu\")\n",
    "    sys.path.append(main_fold_path)\n",
    "    os.chdir(main_fold_path)\n",
    "    \n",
    "set_path()    \n",
    "\n",
    "import pandas as pd \n",
    "import sqlite3\n",
    "\n",
    "def connect_to_database_and_fetch_results(database_path, query, params):\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    \n",
    "    # Create a cursor object to execute SQL queries\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Execute the SQL query\n",
    "    cursor.execute(query, params)\n",
    "    \n",
    "    # Fetch all rows from the result set\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Get the column names\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Close the cursor and the database connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert rows to list of dictionaries\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        results.append(dict(zip(column_names, row)))\n",
    "    \n",
    "    # Return the list of dictionaries representing each row\n",
    "    return results    \n",
    "    \n",
    "    \n",
    "def read_table_from_database(database_path, table_name, table_id=None):\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    \n",
    "    # Create a cursor object to execute SQL queries\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    if table_id is None:\n",
    "        # Fetch all rows if no ID is specified\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    else:\n",
    "        # Fetch specific row based on ID\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} WHERE {table_name[:-1]}_id=?\", (table_id,))\n",
    "    \n",
    "    # Fetch all rows from the result set\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Get the column names\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Close the cursor and the database connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert rows to list of dictionaries\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        results.append(dict(zip(column_names, row)))\n",
    "    \n",
    "    # Return the list of dictionaries representing each row\n",
    "    return results\n",
    "DATABASE_PATH = \"results/experiment_results.db\"\n",
    "\n",
    "\n",
    "def read_result_from_sql(result_id, DATABASE_PATH):\n",
    "    \"\"\" \n",
    "    \n",
    "    \n",
    "    return val_metrics, test_metrics\n",
    "    \n",
    "    val_metrics: list: (5) [AUC, Accuracy, Sensitivity, Specificity, Duration]\n",
    "    test_metrics: list: (5) [AUC, Accuracy, Sensitivity, Specificity, Duration]\n",
    "\n",
    "    \"\"\"\n",
    "    result_val_metrics = []\n",
    "    result_test_metrics = []\n",
    "    for result in  read_table_from_database(DATABASE_PATH, \"results\", result_id):\n",
    "        fold_test_metrics = []\n",
    "        fold_val_metrics = []\n",
    "        for performance_id in result['performance_ids'].split(','):\n",
    "            performance = read_table_from_database(DATABASE_PATH, \"performances\", performance_id)[0]\n",
    "            history = pd.read_json(performance['history'])\n",
    "            \n",
    "            # Append val_metrics DataFrame to the list\n",
    "            result_val_metrics.append(pd.read_json(performance['val_performance_metrics']))\n",
    "            result_test_metrics.append(pd.read_json(performance['test_performance_metrics']))\n",
    "\n",
    "        # # Concatenate all val_metrics DataFrames along rows (axis=0)\n",
    "        # concatenated_val_metrics = pd.concat(fold_val_metrics, axis=0)\n",
    "        # concatenated_test_metrics = pd.concat(fold_test_metrics, axis=0)\n",
    "\n",
    "        # # Calculate the mean across all rows (axis=0 means across rows)\n",
    "        # mean_folds_val_metrics = concatenated_val_metrics.mean(axis=0)\n",
    "        # mean_folds_test_metrics = concatenated_test_metrics.mean(axis=0)\n",
    "        # result_val_metrics.append(mean_folds_val_metrics)\n",
    "        # result_test_metrics.append(mean_folds_test_metrics)\n",
    "\n",
    "    conc_result_val_metrics = pd.concat(result_val_metrics, axis=0)\n",
    "    conc_result_test_metrics = pd.concat(result_test_metrics, axis=0)\n",
    "    avg_result_val_metrics = conc_result_val_metrics.mean(axis=0)\n",
    "    avg_result_test_metrics = conc_result_test_metrics.mean(axis=0)\n",
    "    \n",
    "    val_metrics = [\n",
    "                   avg_result_val_metrics['accuracy'], \n",
    "                   avg_result_val_metrics['sensitivity'], \n",
    "                   avg_result_val_metrics['specificity'], \n",
    "                   avg_result_val_metrics['AUC'], \n",
    "                   avg_result_val_metrics['duration']]\n",
    "    test_metrics = [\n",
    "                   avg_result_test_metrics['accuracy'], \n",
    "                   avg_result_test_metrics['sensitivity'], \n",
    "                   avg_result_test_metrics['specificity'], \n",
    "                   avg_result_test_metrics['AUC'], \n",
    "                   avg_result_test_metrics['duration']]    \n",
    "    # print(\"Mean of result val_metrics\")\n",
    "    # print(val_metrics)\n",
    "    # print(\"Mean of result test_metrics\")\n",
    "    # print(test_metrics)\n",
    "    return val_metrics, test_metrics\n",
    "    \n",
    "def read_config_from_experiment(experiment_args, DATABASE_PATH):\n",
    "    # Create the SQL query with placeholders for parameters\n",
    "    args_all = [f\"{key} = ?\" for key in experiment_args.keys()]\n",
    "    str_args_all = ' AND '.join(args_all)\n",
    "    sql_query = f\"SELECT * FROM experiments WHERE {str_args_all}\"\n",
    "    params = tuple(experiment_args.values())    \n",
    "    config_experiment = connect_to_database_and_fetch_results(DATABASE_PATH, sql_query, params)\n",
    "    config_result = { \n",
    "        'val': [],\n",
    "        'test': []\n",
    "        }\n",
    "    for experiment in config_experiment:\n",
    "        val_metric, test_metric = read_result_from_sql(experiment['result_id'], DATABASE_PATH)\n",
    "        \n",
    "        config_result['val'].append(val_metric)\n",
    "        \n",
    "        config_result['test'].append(test_metric)\n",
    "    avg_val = np.mean(config_result['val'], axis=0)\n",
    "    avg_test = np.mean(config_result['test'], axis=0)\n",
    "    return avg_val, avg_test\n",
    "\n",
    "\n",
    "def read_task_file(task_file_path):\n",
    "    experiment_args = {}\n",
    "    config_names = []\n",
    "\n",
    "    with open(task_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    in_model_names = False\n",
    "    in_config_names = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith('model_names'):\n",
    "            in_model_names = True\n",
    "            continue\n",
    "\n",
    "        if in_model_names:\n",
    "            if line.startswith(')'):\n",
    "                in_model_names = False\n",
    "                continue\n",
    "            model_name = line.strip().strip(\"'\")\n",
    "            experiment_args['model_name'] = model_name\n",
    "            in_model_names = False\n",
    "            continue\n",
    "\n",
    "        if line.startswith('config_names'):\n",
    "            in_config_names = True\n",
    "            continue\n",
    "\n",
    "        if in_config_names:\n",
    "            if line.startswith(')'):\n",
    "                in_config_names = False\n",
    "                continue\n",
    "            config_name = line.strip().strip(\"'\")\n",
    "            if config_name:\n",
    "                config_names.append(config_name)\n",
    "            continue\n",
    "\n",
    "        if line.startswith('run_itr'):\n",
    "            run_itr = line.split('=')[1].strip().strip(\"'\")\n",
    "            experiment_args['run_itr'] = run_itr\n",
    "\n",
    "        if line.startswith('launcher_name'):\n",
    "            launcher_name = line.split('=')[1].strip().strip('\"')\n",
    "            experiment_args['launcher_name'] = launcher_name\n",
    "\n",
    "    # Derive task_name from the file name\n",
    "    experiment_args['task_name'] = task_file_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    return experiment_args, config_names\n",
    "    \n",
    "task_file_path = 'tasks/20240720_normalization_method.sh'    \n",
    "\n",
    "experiment_args, config_names = read_task_file(task_file_path)\n",
    "\n",
    "print('experiment_args', experiment_args)\n",
    "print('config_names', config_names)\n",
    "\n",
    "from utils.utils_mine import plot_evaluation_metrics_header\n",
    "from utils.fnirs_utils import print_md_table_val_test_AUC\n",
    "PARAMETER_NAME = 'Normalization_method'\n",
    "plot_evaluation_metrics_header(table_name = 'Depression', parameter_name=PARAMETER_NAME, val_auc_threshold=0)     \n",
    "\n",
    "for config_name in config_names:\n",
    "    experiment_args['config_name'] = config_name\n",
    "    \n",
    "    val_metrics, test_metrics = read_config_from_experiment(experiment_args, DATABASE_PATH)\n",
    "    print_md_table_val_test_AUC(config_name.split('_')[-1], val_metrics, test_metrics, print_table_header=False, already_balanced_accuracy=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system is Ubuntu\n",
      "experiment_args {'model_name': 'jamba_MTL', 'run_itr': 'jamba_20240720', 'launcher_name': 'nested_CV_train.py', 'task_name': '20240720_normalization_method'}\n",
      "config_names ['STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_batchnorm', 'STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_rmsnorm', 'STL_depression_NCV_best_v1_wo_mamba_w_mlp_w_conv_norm_layernorm']\n",
      "| Depression | Testing Set |             |             |             | Validation Set |             |             |      Threshold=0       |             |\n",
      "|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "|      Normalization_method      | bAcc(%) | Sen(%) | Spe(%) | AUC(%) | bAcc(%) | Sen(%) | Spe(%) | AUC(%) | Duration(s) |\n",
      "| batchnorm   | 59.87  | 48.71  | 71.04  | 65.35  | 59.84  | 49.31  | 70.36  | 65.07  | 2.9  |\n",
      "| rmsnorm   | 58.69  | 47.15  | 70.22  | 65.13  | 58.62  | 47.02  | 70.21  | 64.65  | 2.6  |\n",
      "| layernorm   | 58.47  | 47.69  | 69.25  | 65.01  | 58.69  | 48.00  | 69.38  | 64.65  | 2.7  |\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
