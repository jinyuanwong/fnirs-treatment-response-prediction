{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What if you can achieve a better result!\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "# Packages \n",
    "\n",
    "\n",
    "\n",
    "def read_hb_from_file(example_path): # Open the file and read through the first few lines to find where the data starts\n",
    "    with open(example_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data_start_line = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Data' in line:  # This should match the unique identifier of the data section\n",
    "                data_start_line = i + 1\n",
    "                # print(lines[data_start_line])\n",
    "                break\n",
    "\n",
    "    if data_start_line is not None:\n",
    "\n",
    "        # Read the data section, skipping the lines of the metadata\n",
    "        data = pd.read_csv(example_path, skiprows=data_start_line)\n",
    "\n",
    "        # Now you have metadata and data as separate DataFrames\n",
    "        # print(data)\n",
    "    else:\n",
    "        print(\"Data section not found.\")\n",
    "        \n",
    "    np_data = data.to_numpy()\n",
    "    ch_data = np_data[:, 1:1+52*2]\n",
    "    \n",
    "    return ch_data\n",
    "\n",
    "\n",
    "\n",
    "def compute_delta(data):\n",
    "    data = np.nan_to_num(data, 0)\n",
    "    data += np.abs(np.min(data))\n",
    "    \n",
    "    light_700 = data[:,0::2].tolist()\n",
    "    # light_700_baseline = np.mean(light_700, (0,1)) + 1e-15\n",
    "    # delta_700 = -np.log10(light_700/light_700_baseline)\n",
    "\n",
    "    light_830 = data[:,1::2].tolist()\n",
    "    # light_830_baseline = np.mean(light_830, (0,1)) + 1e-15\n",
    "    # delta_830 = -np.log10(light_830/light_830_baseline)\n",
    "    # return delta_700, delta_830\n",
    "    return np.array(light_700), np.array(light_830)\n",
    "\n",
    "\n",
    "def get_closest_index_of_wavelength(wavelength, ref_ec):\n",
    "    index_closest = np.argmin(np.abs(ref_ec[:,0] - wavelength))\n",
    "    return index_closest\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def extinction_coefficient_of_Hb(ref_file='extinction_coefficient.mat', low_wavelength=695, high_wavelength=830):\n",
    "    ref_ec = loadmat(ref_file)['data']\n",
    "    low_index = get_closest_index_of_wavelength(low_wavelength, ref_ec)\n",
    "    high_index = get_closest_index_of_wavelength(high_wavelength, ref_ec)\n",
    "    \n",
    "    low_e_hbo, low_e_hbr = ref_ec[low_index, 1], ref_ec[low_index, 2]\n",
    "    high_e_hbo, high_e_hbr = ref_ec[high_index, 1], ref_ec[high_index, 2]\n",
    "     \n",
    "    print(f'low_e_hbo: {low_e_hbo}, low_e_hbr: {low_e_hbr}')\n",
    "    print(f'high_e_hbo: {high_e_hbo}, high_e_hbr: {high_e_hbr}')\n",
    "    # e_HbO_704 = 298\n",
    "    # e_HbR_704 = 1687.76\n",
    "    # e_HbO_8279 = 965.2\n",
    "    # e_HbR_8279 = 693.2\n",
    "    E = np.array([[low_e_hbo, low_e_hbr],\n",
    "                [high_e_hbo, high_e_hbr]])\n",
    "    E_inv = np.linalg.inv(E)\n",
    "    \n",
    "    return E_inv \n",
    "\n",
    "def calculate_hb(delta_700, delta_830, E_inv, TOTAL_CHANNEL):\n",
    "    \n",
    "    all_channel_hb = np.empty((TOTAL_CHANNEL, delta_700.shape[0], 3))\n",
    "    \n",
    "    for ch in range(TOTAL_CHANNEL):\n",
    "        \n",
    "        ch_low_intensity = delta_700[:, ch]\n",
    "        ch_high_intensity = delta_830[:, ch]\n",
    "\n",
    "        ch_conc = np.array([ch_low_intensity, ch_high_intensity])\n",
    "        delta_C = np.dot(E_inv, ch_conc)\n",
    "\n",
    "        HbO, HbR = delta_C[0,:], delta_C[1,:]\n",
    "        \n",
    "        Hb = np.empty((len(HbO),3))\n",
    "        Hb[:,0] = HbO\n",
    "        Hb[:,1] = HbR\n",
    "        Hb[:,2] = HbO + HbR\n",
    "        \n",
    "        all_channel_hb[ch] = Hb\n",
    "    \n",
    "    return all_channel_hb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_if_there_is_replicated_file(files_HCS, files_MDD):\n",
    "    \n",
    "    arr = []\n",
    "    for f in files_HCS:\n",
    "        name = f[2:5]\n",
    "        if name in arr:\n",
    "            print(name)\n",
    "        else:\n",
    "            arr.append(name)\n",
    "    arr = []\n",
    "    for f in files_MDD:\n",
    "        name = f[2:5]\n",
    "        if name in arr:\n",
    "            print(name)\n",
    "        else:\n",
    "            arr.append(name)\n",
    "    \n",
    "    files_HCS = [f for f in files_HCS if f[-4:] == '.csv']\n",
    "    files_MDD = [f for f in files_MDD if f[-4:] == '.csv']\n",
    "    \n",
    "    return files_HCS, files_MDD\n",
    "    # arr_files_HCS = []\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "def convert_to_hb_using_MBLL(input_fold, output_fold, flag='cyrus'):\n",
    "    if flag == 'cyrus':\n",
    "        HCS_fold = input_fold + '/Controls/MES'\n",
    "        MDD_fold = input_fold + '/Patients/MES'\n",
    "        files_HCS = os.listdir(HCS_fold)\n",
    "        files_MDD = os.listdir(MDD_fold)\n",
    "        files_HCS, files_MDD = check_if_there_is_replicated_file(files_HCS, files_MDD)\n",
    "    else:\n",
    "        HCS_fold = MDD_fold = input_fold\n",
    "        files = os.listdir(input_fold)\n",
    "        files_HCS = [file for file in files if file.startswith('HC')]\n",
    "        files_MDD = [file for file in files if file.startswith('DEP')]\n",
    "\n",
    "    print(f\"Total number of HCs: {len(files_HCS)}\")\n",
    "    print(f\"Total number of MDDs: {len(files_MDD)}\")\n",
    "\n",
    "    HCS_LABEL = np.zeros(len(files_HCS))\n",
    "    MDD_LABEL = np.ones(len(files_MDD))\n",
    "    \n",
    "    HCS_DATA = np.empty((len(files_HCS), 52, 1701, 2))\n",
    "    MDD_DATA = np.empty((len(files_MDD), 52, 1701, 2))\n",
    "    \n",
    "    \n",
    "    for sub, file in enumerate(files_HCS):\n",
    "        \n",
    "        TOTAL_CHANNEL = 52\n",
    "        data = read_hb_from_file(HCS_fold + '/' + file)\n",
    "        E_inv = extinction_coefficient_of_Hb()\n",
    "        delta_700, delta_830 = compute_delta(data)\n",
    "        two_lights = np.concatenate((delta_700[..., np.newaxis], delta_830[..., np.newaxis]), axis = -1)\n",
    "        two_lights = np.transpose(two_lights, (1,0,2))\n",
    "        \n",
    "        HCS_DATA[sub] = two_lights\n",
    "\n",
    "    for sub, file in enumerate(files_MDD):\n",
    "        \n",
    "        TOTAL_CHANNEL = 52\n",
    "        data = read_hb_from_file(MDD_fold + '/' + file)\n",
    "        E_inv = extinction_coefficient_of_Hb()\n",
    "        delta_700, delta_830 = compute_delta(data)\n",
    "        two_lights = np.concatenate((delta_700[..., np.newaxis], delta_830[..., np.newaxis]), axis = -1)\n",
    "        two_lights = np.transpose(two_lights, (1,0,2))\n",
    "        \n",
    "        MDD_DATA[sub] = two_lights\n",
    "                        \n",
    "        \n",
    "    LABEL = np.concatenate((HCS_LABEL, MDD_LABEL), axis = 0)\n",
    "    DATA = np.concatenate((HCS_DATA, MDD_DATA), axis = 0)\n",
    "    print('Final - Data - shape', DATA.shape)\n",
    "    return LABEL, DATA\n",
    "\n",
    "input_fold_cyrus = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/Prerequisite/RawData/Baseline_fnirs'\n",
    "input_fold_fabeha = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/Prerequisite/RawData/Febeha_374_data'\n",
    "\n",
    "input_dict_name_data = {\n",
    "    'cyrus': input_fold_cyrus,\n",
    "    'fabeha': input_fold_fabeha\n",
    "}\n",
    "\n",
    "ALL_DATA, ALL_LABEL = [], []\n",
    "for type_name, input_fold in input_dict_name_data.items():\n",
    "    print(f\"Processing {type_name}\")\n",
    "\n",
    "    LABEL, DATA = convert_to_hb_using_MBLL(input_fold, output_fold, type_name)\n",
    "    ALL_DATA.append(DATA)\n",
    "    ALL_LABEL.append(LABEL)\n",
    "\n",
    "ALL_DATA, ALL_LABEL = np.concatenate(ALL_DATA, axis = 0), np.concatenate(ALL_LABEL, axis = 0)\n",
    "ALL_DATA = np.concatenate((ALL_DATA[...,0], ALL_DATA[...,1]), axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((514, 52, 3402), (514,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_DATA.shape, ALL_LABEL.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# DATA_0 = plt.plot(DATA[...,0].mean(axis = (0,1)))\n",
    "\n",
    "HC = ALL_DATA[ALL_LABEL == 0]\n",
    "MDD = ALL_DATA[ALL_LABEL == 1]\n",
    "\n",
    "\n",
    "plt.plot(HC.mean(axis = (0,1)), label='HC')\n",
    "plt.plot(MDD.mean(axis = (0,1)), label='MDD')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fold = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/allData/diagnosis514_light'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_fold + '/light_data.npy', ALL_DATA)\n",
    "np.save(output_fold + '/label.npy', ALL_LABEL)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
