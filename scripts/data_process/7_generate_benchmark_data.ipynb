{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 16:34:34.967457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-23 16:34:34.988058: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7704] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-23 16:34:34.988078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-23 16:34:34.988087: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1520] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-23 16:34:34.993050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 16:34:35.298943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.14.0-dev20230531). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "This file will generates some preprocessed data accroding to the existing studies:\n",
    "'chao_cfnn', 'li_svm', 'duan_rsfc', 'wang_alex', 'yu_gnn', 'zhu_xgboost'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import sys \n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.append('/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning')\n",
    "\n",
    "from utils.fnirs_utils import avg_every_ten_point_in_last_dimension\n",
    "from utils.fnirs_utils import get_chao_cfnn_novel_feature\n",
    "from utils.fnirs_utils import li_svm_compute_10_fetures\n",
    "from utils.fnirs_utils import get_duan_rsfc_data\n",
    "from utils.fnirs_utils import wang_alex_feature_selection\n",
    "from utils.fnirs_utils import temporal_feature_extract_yu_gnn_full\n",
    "from utils.fnirs_utils import get_10_features_xgboost_zhu\n",
    "\n",
    "# load data \n",
    "def get_avg_hbo_hbr():\n",
    "    data_fold = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/allData/prognosis/pre_treatment_hamd_reduction_50/'\n",
    "    data = np.load(data_fold + 'hb_data.npy')\n",
    "\n",
    "    hbo = data[..., 0:1250].copy()\n",
    "    hbr = data[..., 1252:].copy()\n",
    "\n",
    "    hbo = avg_every_ten_point_in_last_dimension(hbo)\n",
    "    hbr = avg_every_ten_point_in_last_dimension(hbr)\n",
    "\n",
    "    return hbo, hbr\n",
    "data_fold = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/allData/prognosis/pre_treatment_hamd_reduction_50/'\n",
    "\n",
    "label = data_fold + 'label.npy'\n",
    "label = np.load(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 52, 125)\n",
      "(65, 52, 125)\n",
      "(65, 52, 125)\n",
      "(65, 52, 125)\n",
      "chao_cfnn (65, 26000)\n",
      "li_svm (65, 520)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/utils/fnirs_utils.py:176: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_data[i, :] = (data[i, :] - mean) / std\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duan_rsfc (65, 52, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:1999: UserWarning: nperseg = 256 is greater than input length  = 125, using nperseg = 125\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wang_alex (65, 52, 66)\n",
      "yu_gnn (65, 52, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/utils/fnirs_utils.py:485: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  feature_skewness /= np.std(Hb, axis=2) ** 3\n",
      "/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/utils/fnirs_utils.py:497: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_kurtosis = (n * (n + 1) * fourth_moment) / ((n - 1) * (n - 2)\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/core/_methods.py:232: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/utils/fnirs_utils.py:176: RuntimeWarning: invalid value encountered in subtract\n",
      "  normalized_data[i, :] = (data[i, :] - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhu_xgboost (65, 520)\n"
     ]
    }
   ],
   "source": [
    "all_model_name = ['chao_cfnn', 'li_svm', 'duan_rsfc', 'wang_alex', 'yu_gnn', 'zhu_xgboost']\n",
    "MAIN_FOLD = '/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/allData/prognosis/pretreatment_benchmarks/' \n",
    "for model_name in all_model_name:\n",
    "    model_save_fold = MAIN_FOLD + model_name + '/'\n",
    "    if not os.path.exists(model_save_fold):\n",
    "        os.makedirs(model_save_fold)\n",
    "    \n",
    "    hbo, hbr = get_avg_hbo_hbr()\n",
    "    input = np.concatenate((hbo[..., np.newaxis], hbr[..., np.newaxis]), axis=-1)\n",
    "\n",
    "    if model_name == 'chao_cfnn':\n",
    "        feature = get_chao_cfnn_novel_feature(hbo, hbr)\n",
    "    if model_name == 'li_svm':\n",
    "        feature = li_svm_compute_10_fetures(hbo)\n",
    "        feature = feature.reshape(feature.shape[0], -1)   \n",
    "    if model_name == 'duan_rsfc':\n",
    "        feature = get_duan_rsfc_data(hbo)\n",
    "    if model_name == 'wang_alex':\n",
    "        feature = wang_alex_feature_selection(hbo, index_task_start=10,index_task_end=70,fs=1)\n",
    "    if model_name == 'yu_gnn':\n",
    "        feature = temporal_feature_extract_yu_gnn_full(input, index_start=10, index_end=70, hbo_type=0, hbr_type=1)\n",
    "    if model_name == 'zhu_xgboost':\n",
    "        feature = get_10_features_xgboost_zhu(hbo, index_start=10, task_duration=60)\n",
    "        feature = feature.reshape(feature.shape[0], -1)    \n",
    "        \n",
    "    print(model_name, feature.shape)\n",
    "    np.save(model_save_fold + 'data.npy', feature)\n",
    "    np.save(model_save_fold + 'label.npy', label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.174756038255552 -15.64890383615652 19.550877698071083 -15.845824740709954\n"
     ]
    }
   ],
   "source": [
    "hbo_data, hbr_data = get_avg_hbo_hbr()\n",
    "model_save_fold = MAIN_FOLD + 'yu_gnn' + '/'\n",
    "print(np.max(hbo_data), np.min(hbo_data), np.max(hbr_data), np.min(hbr_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "# 假设 hbo_data 和 hbr_data 是 numpy 数组，形状为 (100, 52, 125)\n",
    "fs = 1  # 设置您的采样频率\n",
    "# 定义一个函数来计算两个信号之间的 CSD\n",
    "\n",
    "def compute_csd(x, y, fs):\n",
    "    f, Pxy = signal.csd(x, y, fs)\n",
    "    return Pxy.mean()\n",
    "\n",
    "\n",
    "# 对于每个主题并行计算 CSD\n",
    "hbo_coh = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_csd)(hbo_data[i, j], hbo_data[i, k], fs)\n",
    "    for i in range(hbo_data.shape[0])\n",
    "    for j in range(hbo_data.shape[1])\n",
    "    for k in range(hbo_data.shape[1])\n",
    ")\n",
    "\n",
    "\n",
    "hbr_coh = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_csd)(hbr_data[i, j], hbr_data[i, k], fs)\n",
    "    for i in range(hbr_data.shape[0])\n",
    "    for j in range(hbr_data.shape[1])\n",
    "    for k in range(hbr_data.shape[1])\n",
    ")\n",
    "\n",
    "\n",
    "spatial_hbo_coh_output = np.array(hbo_coh).reshape(hbo_data.shape[0],hbo_data.shape[1],hbo_data.shape[1])\n",
    "spatial_hbr_coh_output = np.array(hbr_coh).reshape(hbr_data.shape[0],hbr_data.shape[1],hbr_data.shape[1])\n",
    "\n",
    "# Define a function to compute the Pearson's correlation coefficient\n",
    "def compute_correlation(x, y):\n",
    "    corr, _ = stats.pearsonr(x, y)\n",
    "    return corr\n",
    "\n",
    "\n",
    "# Compute correlation for hbo data\n",
    "hbo_cor = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_correlation)(hbo_data[i, j], hbo_data[i, k])\n",
    "    for i in range(hbo_data.shape[0])\n",
    "    for j in range(52)\n",
    "    for k in range(52)\n",
    ")\n",
    "\n",
    "# Compute correlation for hbr data\n",
    "hbr_cor = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_correlation)(hbr_data[i, j], hbr_data[i, k])\n",
    "    for i in range(hbr_data.shape[0])\n",
    "    for j in range(52)\n",
    "    for k in range(52)\n",
    ")\n",
    "\n",
    "\n",
    "spatial_hbo_cor_output = np.array(hbo_cor).reshape(hbo_data.shape[0],hbo_data.shape[1],hbo_data.shape[1])\n",
    "spatial_hbr_cor_output = np.array(hbr_cor).reshape(hbo_data.shape[0],hbo_data.shape[1],hbo_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 52, 52, 4)\n"
     ]
    }
   ],
   "source": [
    "def clear_Nan(A):\n",
    "    xA = A \n",
    "    xA[np.isinf(xA)] = np.nan\n",
    "    no_nan_A =  np.nan_to_num(xA, copy=False)\n",
    "    return no_nan_A\n",
    "\n",
    "nor_spatial_hbo_coh_output = np.abs(clear_Nan(spatial_hbo_coh_output))\n",
    "nor_spatial_hbo_cor_output = np.abs(clear_Nan(spatial_hbo_cor_output))\n",
    "nor_spatial_hbr_coh_output = np.abs(clear_Nan(spatial_hbr_coh_output))\n",
    "nor_spatial_hbr_cor_output = np.abs(clear_Nan(spatial_hbr_cor_output))\n",
    "A = np.concatenate((nor_spatial_hbo_coh_output[...,np.newaxis],\n",
    "                    nor_spatial_hbo_cor_output[...,np.newaxis],\n",
    "                    nor_spatial_hbr_coh_output[...,np.newaxis],\n",
    "                    nor_spatial_hbr_cor_output[...,np.newaxis]),\n",
    "                   axis=3)\n",
    "print(A.shape)\n",
    "\n",
    "np.save(model_save_fold + 'adj.npy', A)\n",
    "np.save(model_save_fold + 'adj_1.npy', A[..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 52, 52, 4)\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    # Iterate over each subject\n",
    "    normalized_data = np.empty_like(data)\n",
    "    for i in range(data.shape[0]):\n",
    "        # Calculate the mean and standard deviation for the current subject\n",
    "        mean = np.mean(data[i])\n",
    "        std = np.std(data[i])\n",
    "\n",
    "        # Perform z-normalization for the current subject\n",
    "        normalized_data[i] = (data[i] - mean) / std\n",
    "    return normalized_data\n",
    "def clear_Nan(A):\n",
    "    xA = A \n",
    "    xA[np.isinf(xA)] = np.nan\n",
    "    no_nan_A =  np.nan_to_num(xA, copy=False)\n",
    "    return no_nan_A\n",
    "\n",
    "nor_spatial_hbo_coh_output = np.abs(normalize(clear_Nan(spatial_hbo_coh_output)))\n",
    "nor_spatial_hbo_cor_output = np.abs(normalize(clear_Nan(spatial_hbo_cor_output)))\n",
    "nor_spatial_hbr_coh_output = np.abs(normalize(clear_Nan(spatial_hbr_coh_output)))\n",
    "nor_spatial_hbr_cor_output = np.abs(normalize(clear_Nan(spatial_hbr_cor_output)))\n",
    "A = np.concatenate((nor_spatial_hbo_coh_output[...,np.newaxis],\n",
    "                    nor_spatial_hbo_cor_output[...,np.newaxis],\n",
    "                    nor_spatial_hbr_coh_output[...,np.newaxis],\n",
    "                    nor_spatial_hbr_cor_output[...,np.newaxis]),\n",
    "                   axis=3)\n",
    "print(A.shape)\n",
    "\n",
    "np.save(model_save_fold + 'normalized_adj.npy', A)\n",
    "np.save(model_save_fold + 'normalized_adj_1.npy', A[..., 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
