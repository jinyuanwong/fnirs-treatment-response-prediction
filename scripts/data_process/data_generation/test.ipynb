{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanxiafeng/miniconda3/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/shanxiafeng/miniconda3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.14.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_subject -> 46\n",
      "all_subject_index -> [ 2  3  5  6  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 48 49 50 51 54]\n",
      "all_subject_index.shape ->  (46,)\n",
      "len(np.unique(all_subject_index)) ->  46\n",
      "Is there any replicated number in all_subject_index? False\n",
      "return replicated_indices None\n",
      "mdd_subject_base -> (46, 1251, 52, 2, 2)\n",
      "label_hamd -> (46, 2)\n",
      "demografic_data -> (46, 11)\n",
      "baseline_clinical_data -> (46, 6)\n",
      "all_involve_subject ['PT002', 'PT003', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT048', 'PT049', 'PT050', 'PT051', 'PT054']\n"
     ]
    }
   ],
   "source": [
    "# load \n",
    "\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction')\n",
    "\n",
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from utils.utils_mine import*\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pingouin as pg\n",
    "import subprocess\n",
    "import collections\n",
    "\n",
    "# path of data \n",
    "\n",
    "\n",
    "def read_from_file(example_path): # Open the file and read through the first few lines to find where the data starts\n",
    "    with open(example_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data_start_line = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Data' in line:  # This should match the unique identifier of the data section\n",
    "                data_start_line = i + 1\n",
    "                # print(lines[data_start_line])\n",
    "                break\n",
    "\n",
    "    if data_start_line is not None:\n",
    "\n",
    "        # Read the data section, skipping the lines of the metadata\n",
    "        data = pd.read_csv(example_path, skiprows=data_start_line)\n",
    "\n",
    "        # Now you have metadata and data as separate DataFrames\n",
    "        # print(data)\n",
    "    else:\n",
    "        print(\"Data section not found.\")\n",
    "        \n",
    "    np_data = data.to_numpy()\n",
    "    ch_data = np_data[:, 1:1+52]\n",
    "\n",
    "    return ch_data\n",
    "\n",
    "def get_file_name(path, rest):\n",
    "    file_pattern = os.path.join(path, rest)\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    return file_list\n",
    "\n",
    "def check_replicate_subject(all_subject):\n",
    "    all_subject_index = [i[3:] for i in all_subject]\n",
    "    all_subject_index = np.array(all_subject_index).astype(int)\n",
    "    print(f'all_subject_index -> {all_subject_index}')\n",
    "    print('all_subject_index.shape -> ', all_subject_index.shape)\n",
    "    print('len(np.unique(all_subject_index)) -> ', len(np.unique(all_subject_index)))\n",
    "    is_replicated = len(np.unique(all_subject_index)) != len(all_subject_index)\n",
    "    print(f\"Is there any replicated number in all_subject_index? {is_replicated}\")\n",
    "    if is_replicated:\n",
    "        replicated_elements = [item for item, count in collections.Counter(all_subject_index).items() if count > 1]\n",
    "        replicated_indices = np.where(np.isin(all_subject_index, replicated_elements))[0]\n",
    "        print(f\" Element {replicated_elements} shows up in the following indices: {replicated_indices}\")\n",
    "    else: return None\n",
    "    print(f' now will return replicated_indices[0::2]')\n",
    "    return replicated_indices[0::2]\n",
    "\n",
    "\n",
    "follow_up_fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/RawData'\n",
    "T8_path = follow_up_fold + '/T8_fnirs/Session 2_VFT'\n",
    "base_patient_path = follow_up_fold + '/Baseline_fnirs/Patients'\n",
    "cli_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/fNIRS x MDD Data_Demographics_Clinical.xlsx'\n",
    "\n",
    "cgi_sgs_data = pd.read_excel(cli_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "\n",
    "# cgi_sgs_data.iloc[:, 1:7]\n",
    "\n",
    "excel_data = pd.read_excel(cli_path, sheet_name='Summary T0T8_fNIRS Analysis')\n",
    "# cgi_sgs_data = pd.read_excel(cgi_sgs_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "label_hamd = []\n",
    "demografic_data = []\n",
    "baseline_clinical_data = []\n",
    "\n",
    "def check_existing_subject_in_fnirs_path(path):\n",
    "    for hb in ['_Oxy.csv', '_Deoxy.csv']:\n",
    "        tmp = 0\n",
    "        all_subject = []\n",
    "        for i in os.listdir(path):\n",
    "            if i[-len(hb):] == hb:\n",
    "                subject = i.split(' ')[0]\n",
    "                all_subject.append(subject)\n",
    "                file_pattern = os.path.join(path, subject+'*'+hb)\n",
    "                file_list = glob.glob(file_pattern)\n",
    "                if len(file_list) < 1:\n",
    "                    print(file_list)\n",
    "                tmp+=1\n",
    "        all_subject.sort()\n",
    "    return all_subject\n",
    "\n",
    "all_subject = check_existing_subject_in_fnirs_path(T8_path)\n",
    "print(f'all_subject -> {len(all_subject)}')\n",
    "\n",
    "# def get_file_name(path, rest):\n",
    "#     file_pattern = os.path.join(path, rest)\n",
    "#     file_list = glob.glob(file_pattern)\n",
    "#     return file_list\n",
    "# # according to the subject name of all_subject create array now \n",
    "\n",
    "mdd_subject_base_post = np.zeros((len(all_subject), 1251, 52, 2, 2))# []#np.zeros((len(all_subject), 1251, 52, 2)) # time, channel, hbo/hbr\n",
    "all_involve_subject = []\n",
    "for sub_index, subject in enumerate(all_subject):\n",
    "    hamd_of_id_t1 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T1)'].iloc[0]\n",
    "    hamd_of_id_t8 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T8)'].iloc[0]\n",
    "    demographic = excel_data[excel_data['Subject ID'] == subject].iloc[:, 2:13]\n",
    "    clinical = cgi_sgs_data[cgi_sgs_data['Subject ID'] == subject].iloc[:, 1:7]\n",
    "    if type(hamd_of_id_t8) is not int:\n",
    "        print(hamd_of_id_t8)\n",
    "        continue\n",
    "    all_involve_subject.append(subject)\n",
    "    sub_label = [hamd_of_id_t1, hamd_of_id_t8]\n",
    "    label_hamd.append(sub_label)\n",
    "    demografic_data.append(demographic)\n",
    "    baseline_clinical_data.append(clinical)\n",
    "    for hb_index, hb in enumerate(['_Oxy.csv', '_Deoxy.csv']):\n",
    "\n",
    "        base_hb_file = get_file_name(base_patient_path, subject+'*'+hb)\n",
    "        base_hb = read_from_file(base_hb_file[0])\n",
    "        \n",
    "        post_hb_file = get_file_name(T8_path, subject+'*'+hb)\n",
    "        post_hb = read_from_file(post_hb_file[0])\n",
    "        \n",
    "        mdd_subject_base_post[sub_index, :, :, hb_index, 0] = base_hb\n",
    "        mdd_subject_base_post[sub_index, :, :, hb_index, 1] = post_hb\n",
    "        # print(f\"sub :{sub_index} ({subject}) hb: {hb_index} was given base value {np.mean(base_hb)} and post value {np.mean(post_hb)}\")\n",
    "\n",
    "mdd_subject_base = np.array(mdd_subject_base_post)\n",
    "label_hamd = np.array(label_hamd)\n",
    "demografic_data = np.squeeze(np.array(demografic_data))\n",
    "baseline_clinical_data = np.squeeze(np.array(baseline_clinical_data))\n",
    "\n",
    "\n",
    "# check if there is any replicated subject, becasue there might be two files with same subject names\n",
    "replicated_indices = check_replicate_subject(all_subject)\n",
    "print(f'return replicated_indices {replicated_indices}')\n",
    "\n",
    "print(f'mdd_subject_base -> {mdd_subject_base.shape}')\n",
    "print(f'label_hamd -> {label_hamd.shape}')\n",
    "print(f'demografic_data -> {demografic_data.shape}')\n",
    "print(f'baseline_clinical_data -> {baseline_clinical_data.shape}')\n",
    "print('all_involve_subject', all_involve_subject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_subject 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['PT002', 'PT003', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010',\n",
       "        'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017',\n",
       "        'PT018', 'PT019', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025',\n",
       "        'PT026', 'PT027'],\n",
       "       ['PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034',\n",
       "        'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042',\n",
       "        'PT043', 'PT044', 'PT045', 'PT046', 'PT048', 'PT049', 'PT050',\n",
       "        'PT051', 'PT054']], dtype='<U5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('all_subject', len(all_subject))\n",
    "a = np.reshape(all_subject, (2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_data ->  (64, 52, 2500)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgi_sgs_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/fNIRS x MDD Data_Demographics_Clinical.xlsx'\n",
    "\n",
    "cgi_sgs_data = pd.read_excel(cgi_sgs_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "\n",
    "cgi_sgs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
