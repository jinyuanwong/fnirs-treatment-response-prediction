{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_subject -> ['PT001', 'PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT035', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT052', 'PT053', 'PT054', 'PT055', 'PT056', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n",
      "all_subject -> ['PT001', 'PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT035', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT052', 'PT053', 'PT054', 'PT055', 'PT056', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n",
      "all_subject -> 71\n",
      "[LOSS TO FOLLOW-UP]\n",
      "[LOSS TO FOLLOW-UP]\n",
      "[LOSS TO FOLLOW-UP]\n",
      "[LOSS TO FOLLOW-UP]\n",
      "[LOSS TO FOLLOW-UP]\n",
      "[LOSS TO FOLLOW-UP]\n",
      "all_subject_index -> [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 36 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71]\n",
      "all_subject_index.shape ->  (71,)\n",
      "len(np.unique(all_subject_index)) ->  70\n",
      "Is there any replicated number in all_subject_index? True\n",
      " Element [36] shows up in the following indices: [34 35]\n",
      " now will return replicated_indices[0::2]\n",
      "return replicated_indices [34]\n",
      "mdd_subject_base -> (64, 1251, 52, 2)\n",
      "label_hamd -> (64, 2)\n",
      "demografic_data -> (64, 11)\n",
      "baseline_clinical_data -> (64, 7)\n",
      "all_involve_subject ['PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT054', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n"
     ]
    }
   ],
   "source": [
    "# load \n",
    "\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction')\n",
    "\n",
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from utils.utils_mine import*\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pingouin as pg\n",
    "import subprocess\n",
    "import collections\n",
    "\n",
    "# path of data \n",
    "\n",
    "\n",
    "def read_from_file(example_path): # Open the file and read through the first few lines to find where the data starts\n",
    "    with open(example_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data_start_line = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Data' in line:  # This should match the unique identifier of the data section\n",
    "                data_start_line = i + 1\n",
    "                # print(lines[data_start_line])\n",
    "                break\n",
    "\n",
    "    if data_start_line is not None:\n",
    "\n",
    "        # Read the data section, skipping the lines of the metadata\n",
    "        data = pd.read_csv(example_path, skiprows=data_start_line)\n",
    "\n",
    "        # Now you have metadata and data as separate DataFrames\n",
    "        # print(data)\n",
    "    else:\n",
    "        print(\"Data section not found.\")\n",
    "        \n",
    "    np_data = data.to_numpy()\n",
    "    ch_data = np_data[:, 1:1+52]\n",
    "\n",
    "    return ch_data\n",
    "\n",
    "def get_file_name(path, rest):\n",
    "    file_pattern = os.path.join(path, rest)\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    return file_list\n",
    "\n",
    "def check_replicate_subject(all_subject):\n",
    "    all_subject_index = [i[3:] for i in all_subject]\n",
    "    all_subject_index = np.array(all_subject_index).astype(int)\n",
    "    print(f'all_subject_index -> {all_subject_index}')\n",
    "    print('all_subject_index.shape -> ', all_subject_index.shape)\n",
    "    print('len(np.unique(all_subject_index)) -> ', len(np.unique(all_subject_index)))\n",
    "    is_replicated = len(np.unique(all_subject_index)) != len(all_subject_index)\n",
    "    print(f\"Is there any replicated number in all_subject_index? {is_replicated}\")\n",
    "    if is_replicated:\n",
    "        replicated_elements = [item for item, count in collections.Counter(all_subject_index).items() if count > 1]\n",
    "        replicated_indices = np.where(np.isin(all_subject_index, replicated_elements))[0]\n",
    "        print(f\" Element {replicated_elements} shows up in the following indices: {replicated_indices}\")\n",
    "    print(f' now will return replicated_indices[0::2]')\n",
    "    return replicated_indices[0::2]\n",
    "\n",
    "\n",
    "follow_up_fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/RawData'\n",
    "T8_path = follow_up_fold + '/T8_fnirs/Session 2_VFT'\n",
    "base_patient_path = follow_up_fold + '/Baseline_fnirs/Patients'\n",
    "cli_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/fNIRS x MDD Data_Demographics_Clinical.xlsx'\n",
    "\n",
    "cgi_sgs_data = pd.read_excel(cli_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "\n",
    "# cgi_sgs_data.iloc[:, 1:7]\n",
    "\n",
    "excel_data = pd.read_excel(cli_path, sheet_name='Summary T0T8_fNIRS Analysis')\n",
    "# cgi_sgs_data = pd.read_excel(cgi_sgs_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "label_hamd = []\n",
    "demografic_data = []\n",
    "baseline_clinical_data = []\n",
    "\n",
    "\n",
    "for hb in ['_Oxy.csv', '_Deoxy.csv']:\n",
    "    tmp = 0\n",
    "    all_subject = []\n",
    "    for i in os.listdir(base_patient_path):\n",
    "        if i[-len(hb):] == hb:\n",
    "            subject = i.split(' ')[0]\n",
    "            all_subject.append(subject)\n",
    "            file_pattern = os.path.join(base_patient_path, subject+'*'+hb)\n",
    "            file_list = glob.glob(file_pattern)\n",
    "            if len(file_list) < 1:\n",
    "                print(file_list)\n",
    "            tmp+=1\n",
    "    all_subject.sort()\n",
    "    print(f'all_subject -> {all_subject}')\n",
    "print(f'all_subject -> {len(all_subject)}')\n",
    "\n",
    "# def get_file_name(path, rest):\n",
    "#     file_pattern = os.path.join(path, rest)\n",
    "#     file_list = glob.glob(file_pattern)\n",
    "#     return file_list\n",
    "# # according to the subject name of all_subject create array now \n",
    "\n",
    "mdd_subject_base = []#np.zeros((len(all_subject), 1251, 52, 2)) # time, channel, hbo/hbr\n",
    "all_involve_subject = []\n",
    "for sub_index, subject in enumerate(all_subject):\n",
    "    hamd_of_id_t1 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T1)'].iloc[0]\n",
    "    hamd_of_id_t8 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T8)'].iloc[0]\n",
    "    demographic = excel_data[excel_data['Subject ID'] == subject].iloc[:, 2:13]\n",
    "    clinical = cgi_sgs_data[cgi_sgs_data['Subject ID'] == subject].iloc[:, 1:7]\n",
    "    if type(hamd_of_id_t8) is not int:\n",
    "        print(hamd_of_id_t8)\n",
    "        continue\n",
    "    all_involve_subject.append(subject)\n",
    "    sub_label = [hamd_of_id_t1, hamd_of_id_t8]\n",
    "    label_hamd.append(sub_label)\n",
    "    demografic_data.append(demographic)\n",
    "    baseline_clinical_data.append(clinical)\n",
    "    hbo_hbr = np.zeros((1251, 52, 2))\n",
    "    for hb_index, hb in enumerate(['_Oxy.csv', '_Deoxy.csv']):\n",
    "\n",
    "        base_hb_file = get_file_name(base_patient_path, subject+'*'+hb)\n",
    "        base_hb = read_from_file(base_hb_file[0])\n",
    "        hbo_hbr[...,hb_index] = base_hb\n",
    "    mdd_subject_base.append(hbo_hbr)\n",
    "mdd_subject_base = np.array(mdd_subject_base)\n",
    "label_hamd = np.array(label_hamd)\n",
    "demografic_data = np.squeeze(np.array(demografic_data))\n",
    "baseline_clinical_data = np.squeeze(np.array(baseline_clinical_data))\n",
    "\n",
    "\n",
    "# check if there is any replicated subject, becasue there might be two files with same subject names\n",
    "replicated_indices = check_replicate_subject(all_subject)\n",
    "print(f'return replicated_indices {replicated_indices}')\n",
    "\n",
    "\n",
    "# delete the replicated subject\n",
    "mdd_subject_base = np.delete(mdd_subject_base, replicated_indices, axis=0)\n",
    "label_hamd = np.delete(label_hamd, replicated_indices, axis=0)\n",
    "demografic_data = np.delete(demografic_data, replicated_indices, axis=0)\n",
    "baseline_clinical_data = np.delete(baseline_clinical_data, replicated_indices, axis=0)\n",
    "\n",
    "\n",
    "# baseline HAMD will be added into the baseline_clinical_data \n",
    "baseline_clinical_data = np.concatenate((baseline_clinical_data, label_hamd[:, 0:1]), axis=1)\n",
    "\n",
    "\n",
    "print(f'mdd_subject_base -> {mdd_subject_base.shape}')\n",
    "print(f'label_hamd -> {label_hamd.shape}')\n",
    "print(f'demografic_data -> {demografic_data.shape}')\n",
    "print(f'baseline_clinical_data -> {baseline_clinical_data.shape}')\n",
    "print('all_involve_subject', all_involve_subject)\n",
    "\n",
    "# calculate remission \n",
    "label_remission = np.zeros(label_hamd.shape[0])\n",
    "for i, val in enumerate(label_hamd):\n",
    "    if val[0] >= 7 and val[1] <= 7:\n",
    "        if val[0] == 7: print(f\" val[0] == 7 : i -> {i} | val[1] -> {val[1]}\")\n",
    "        if val[1] == 7: print(f\" val[1] == 7 : i -> {i} | val[0] -> {val[0]}\")\n",
    "        label_remission[i] = 1\n",
    "        # print('label_responder[i] -> ', label_responder[i])\n",
    "        # print('val -> ',val)\n",
    "print(label_remission)\n",
    "count = np.count_nonzero(label_remission == 1)\n",
    "print(count)\n",
    "\n",
    "\n",
    "# modify the hb data to be like (subject, 52, 2500)\n",
    "mdd_subject_base = mdd_subject_base[:, :1250, :, :]\n",
    "mdd_subject_base = mdd_subject_base.transpose((0, 2, 1, 3))\n",
    "mdd_subject_base = mdd_subject_base.reshape((mdd_subject_base.shape[0], 52, -1))\n",
    "\n",
    "hb_data = mdd_subject_base\n",
    "print('hb_data -> ', hb_data.shape)\n",
    "\n",
    "output_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/pretreatment_remission'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "np.save(output_path + '/hb_data.npy', hb_data)\n",
    "np.save(output_path + '/label_hamd.npy', label_hamd)\n",
    "np.save(output_path + '/label_remission.npy', label_remission)\n",
    "np.save(output_path + '/demografic_data.npy', demografic_data)\n",
    "np.save(output_path + '/baseline_clinical_data.npy', baseline_clinical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_data ->  (64, 52, 2500)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgi_sgs_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/fNIRS x MDD Data_Demographics_Clinical.xlsx'\n",
    "\n",
    "cgi_sgs_data = pd.read_excel(cgi_sgs_path, sheet_name='SDS_CGI_All Timepoints')\n",
    "\n",
    "cgi_sgs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
