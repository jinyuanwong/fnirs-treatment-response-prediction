{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import mannwhitneyu    \n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# caculating_10_important_features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "# sig_data = DATA.reshape(DATA.shape[0], -1)\n",
    "def train_model_using_kfold(data, label, model, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def train_model_using_loocv(data, label, model):\n",
    "    loo = LeaveOneOut()\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in loo.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result), model\n",
    "\n",
    "stats_method = 'mannwhitneyu' # 'ranksums' or 'mannwhitneyu'\n",
    "def zero_diagnonal(arr):\n",
    "    # Loop over the first and last dimension\n",
    "    for i in range(arr.shape[0]):  # Loop over subjects\n",
    "        for j in range(arr.shape[-1]):  # Loop over views\n",
    "            np.fill_diagonal(arr[i, :, :, j], 0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def return_significant_FCvalue(adj, labels):\n",
    "    adj = zero_diagnonal(adj)\n",
    "    hc_adj = adj[np.where(labels==1)]\n",
    "    md_adj = adj[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    num_view = adj.shape[-1]\n",
    "    p_view = np.zeros((52,52,num_view))\n",
    "    effect_size = np.zeros((52,52,num_view))\n",
    "    stats = np.zeros((52,52,num_view))\n",
    "    for view in range(num_view):\n",
    "        for seed in range(52):\n",
    "            for target in range(52):\n",
    "                hc_val = hc_adj[:, seed, target, view]\n",
    "                md_val = md_adj[:, seed, target, view]\n",
    "                if stats_method == 'mannwhitneyu':\n",
    "                    stat, p1 = mannwhitneyu(hc_val,md_val)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('stats_method should be mannwhitneyu or ranksums')\n",
    "                p_view[seed, target, view] = p1\n",
    "                stats[seed, target, view] = stat\n",
    "    \n",
    "    adj = adj.reshape(adj.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    return adj[:, p_view<0.05]\n",
    "\n",
    "def get_activity_start_time(data, index_start):\n",
    "    gradient = np.gradient(data)\n",
    "    max_gradient = np.argmax(gradient[0:int(index_start*1.2)])  # 0:index_start*4 # current index_start = 400,\n",
    "    if max_gradient <= index_start:\n",
    "        max_gradient = index_start\n",
    "    return max_gradient\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the left slope based on the data sample time from activity_start_time and  activity_start_time + task_duration//2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_left_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start: activity_start+task_duration//2]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the right slope based on the data sample time from activity_start_time + task_duration//2 and activity_start_time + task_duration  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_right_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start+task_duration//2: activity_start+task_duration]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For calculating FWHM\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_FWHM(y_values, activity_start, task_duration):\n",
    "    # make sure the peak value is situated in the task duration period\n",
    "    task = y_values[activity_start:activity_start+task_duration]\n",
    "    max_task = np.max(task)  # Find the maximum y value\n",
    "    half_max_y = max_task / 2.0\n",
    "    max_index_task = np.argmax(task)\n",
    "    # if max_index_task is in the first two values, set left_index to 0\n",
    "    if max_index_task <= 1:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = find_nearest(y_values[:max_index_task], half_max_y)\n",
    "    # if max_index_task is in the last two values, set right_index to the last value\n",
    "    if max_index_task >= activity_start+task_duration-1:\n",
    "        right_index = task_duration-1\n",
    "    else:\n",
    "        right_index = find_nearest(\n",
    "            y_values[max_index_task:], half_max_y) + max_index_task\n",
    "\n",
    "    return right_index - left_index\n",
    "\n",
    "\n",
    "\"\"\"Get all 10 features\"\"\"\n",
    "\n",
    "\n",
    "def get_10_features(hbo, index_start, task_duration):\n",
    "    feature_shape = hbo.shape[:2]\n",
    "    # Feature 1 mean\n",
    "    mean = np.mean(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 2 variance\n",
    "    variance = np.std(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 3 activity_start_time\n",
    "    activity_start_time = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start_time[sub, ch] = get_activity_start_time(\n",
    "                hbo[sub, ch], index_start=index_start)\n",
    "\n",
    "    # # Feature 4 left_slope\n",
    "    left_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            left_slope[sub, ch] = get_left_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "    # # Feature 5  right_slope\n",
    "    right_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            right_slope[sub, ch] = get_right_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "\n",
    "    # # Feature 6 kurtosis\n",
    "    kurt = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            kurt[sub, ch] = kurtosis(hbo[sub, ch])\n",
    "    # There might be some nan in kurtosis calucaltion because of all 0-value array\n",
    "    kurt = np.nan_to_num(kurt)\n",
    "\n",
    "    # # Feature 7 skewness\n",
    "    skewness = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            skewness[sub, ch] = skew(hbo[sub, ch])\n",
    "    # There might be some nan in skewness calucaltion because of all 0-value array\n",
    "    skewness = np.nan_to_num(skewness)\n",
    "\n",
    "    # # Feature 8 area under the curve AUC Based on the sample time from activity_start_time + task_duration\n",
    "    AUC = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            AUC[sub, ch] = np.sum(\n",
    "                hbo[sub, ch][activity_start:activity_start+task_duration])\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(AUC[sub])\n",
    "\n",
    "    # # Feature 9 full width half maximum (FWHM)\n",
    "    # FWHM\n",
    "    FWHM = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            FWHM[sub, ch] = get_FWHM(\n",
    "                hbo[sub, ch], activity_start, task_duration)\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(FWHM[sub])\n",
    "\n",
    "    # # Feature 10 peak\n",
    "    peak = np.max(hbo, axis=2)\n",
    "\n",
    "    features = np.concatenate((mean, variance, activity_start_time,\n",
    "                              left_slope, right_slope, kurt, skewness, AUC, FWHM, peak), axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_significant_feature(data, labels):\n",
    "    hc_adj = data[np.where(labels==1)]\n",
    "    md_adj = data[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    p_view = np.zeros((hc_adj.shape[1:]))\n",
    "    for view in range(p_view.shape[-1]):\n",
    "        hc_val = hc_adj[:, view]\n",
    "        mdd_val = md_adj[:, view]\n",
    "        _, p1 = mannwhitneyu(hc_val,mdd_val)\n",
    "        p_view[view] = p1\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    data = data[:, p_view<0.05]\n",
    "    return data \n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # 明确指定labels参数\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    # 现在cm是一个2x2矩阵，即使数据只包含一个类别\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, f1\n",
    "\n",
    "def print_md_table(model_name, set, metrics):\n",
    "    print()\n",
    "    print('| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |')\n",
    "    print('|------------|----------|----------|-------------|-------------|----------|')\n",
    "    print(f'| {model_name} | {set} |', end = '')\n",
    "    for i in range(4):\n",
    "        print(f\" {metrics[i]:.4f} |\", end = '')\n",
    "    print()\n",
    "    print(''*10)\n",
    "\n",
    "def average_signal_and_reshape(data, numof_avgpoint=10):\n",
    "    try:\n",
    "        assert len(data.shape) == 3, \"wrong data shape, it should be like (n_sample, n_channel, n_timepoint)\"\n",
    "        print(\"The data shape length is 3.\")\n",
    "    except AssertionError as error:\n",
    "        print(error)\n",
    "    totalpoint = data.shape[-1] // numof_avgpoint\n",
    "    data = data[...,:int(totalpoint*numof_avgpoint)]\n",
    "    data = np.reshape(data, (data.shape[0], 52, -1, 10))\n",
    "    data = np.mean(data, axis=-1)\n",
    "    data = np.reshape(data, (data.shape[0], -1))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC diagnosis data shape: (140, 52, 52, 3)\n",
      "FC diagnosis label shape: (140,)\n",
      "FC prognosis data shape: (65, 52, 52, 3)\n",
      "FC prognosis label shape: (65,)\n"
     ]
    }
   ],
   "source": [
    "# load the pretreatment data \n",
    "FC_diagnosis_data_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/fc_data.npy'\n",
    "FC_diagnosis_label_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/label.npy'\n",
    "\n",
    "FC_prognosis_data_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/DMFC/pre_treatment_hamd_reduction_50/data.npy'\n",
    "FC_prognosis_label_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/DMFC/pre_treatment_hamd_reduction_50/label.npy'\n",
    "\n",
    "\n",
    "FC_diagnosis_data = np.load(FC_diagnosis_data_path)\n",
    "FC_diagnosis_label = np.load(FC_diagnosis_label_path)\n",
    "\n",
    "FC_prognosis_data = np.load(FC_prognosis_data_path)\n",
    "FC_prognosis_label = np.load(FC_prognosis_label_path)\n",
    "\n",
    "print('FC diagnosis data shape:', FC_diagnosis_data.shape)\n",
    "print('FC diagnosis label shape:', FC_diagnosis_label.shape)\n",
    "print('FC prognosis data shape:', FC_prognosis_data.shape)\n",
    "print('FC prognosis label shape:', FC_prognosis_label.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finding the best seed\n",
      "current seed: 1710231668\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.4714 | 0.4429 | 0.5000 | 0.4559 |\n",
      "\n",
      "current seed: 1710231720\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5214 | 0.5143 | 0.5286 | 0.5180 |\n",
      "\n",
      "current seed: 1710231767\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5500 | 0.5571 | 0.5429 | 0.5532 |\n",
      "\n",
      "current seed: 1710231815\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5000 | 0.5286 | 0.4714 | 0.5139 |\n",
      "\n",
      "current seed: 1710231854\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5071 | 0.5000 | 0.5143 | 0.5036 |\n",
      "\n",
      "current seed: 1710231895\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5286 | 0.5429 | 0.5143 | 0.5352 |\n",
      "\n",
      "current seed: 1710231942\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5000 | 0.5000 | 0.5000 | 0.5000 |\n",
      "\n",
      "current seed: 1710231984\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.4929 | 0.4857 | 0.5000 | 0.4892 |\n",
      "\n",
      "current seed: 1710232030\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5143 | 0.5143 | 0.5143 | 0.5143 |\n",
      "\n",
      "current seed: 1710232073\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5071 | 0.5143 | 0.5000 | 0.5106 |\n",
      "\n",
      "Start finding the best seed\n",
      "current seed: 1710232125\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.6308 | 0.1333 | 0.7800 | 0.1429 |\n",
      "\n",
      "current seed: 1710232130\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5692 | 0.0667 | 0.7200 | 0.0667 |\n",
      "\n",
      "current seed: 1710232134\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5846 | 0.1333 | 0.7200 | 0.1290 |\n",
      "\n",
      "current seed: 1710232139\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5538 | 0.2000 | 0.6600 | 0.1714 |\n",
      "\n",
      "current seed: 1710232144\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5231 | 0.0667 | 0.6600 | 0.0606 |\n",
      "\n",
      "current seed: 1710232148\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.6154 | 0.2000 | 0.7400 | 0.1935 |\n",
      "\n",
      "current seed: 1710232153\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.6154 | 0.0000 | 0.8000 | 0.0000 |\n",
      "\n",
      "current seed: 1710232158\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.6000 | 0.0667 | 0.7600 | 0.0714 |\n",
      "\n",
      "current seed: 1710232162\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.5231 | 0.1333 | 0.6400 | 0.1143 |\n",
      "\n",
      "current seed: 1710232167\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | test | 0.6000 | 0.1333 | 0.7400 | 0.1333 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, label = FC_diagnosis_data, FC_diagnosis_label\n",
    "\n",
    "def check_result(data, label):\n",
    "    data = np.reshape(data, (data.shape[0], -1))\n",
    "\n",
    "    print(\"Start finding the best seed\")\n",
    "    best_seed = 0\n",
    "    best_f1 = 0 \n",
    "    for _ in range(10):\n",
    "        seed = int(time.time())\n",
    "        print(f'current seed: {seed}')\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.random_state = seed\n",
    "\n",
    "        result,model = train_model_using_loocv(data, label, model)\n",
    "        res_metrics = get_metrics(result[:, 1], result[:, 0])\n",
    "        if res_metrics[-1] > best_f1: \n",
    "            best_f1 = res_metrics[-1]\n",
    "            best_seed = seed\n",
    "        print_md_table('Decision Tree', 'test', res_metrics)\n",
    "    return best_seed\n",
    "\n",
    "best_seed_for_diagnosis = check_result(FC_diagnosis_data, FC_diagnosis_label)\n",
    "best_seed_for_prognosis = check_result(FC_prognosis_data, FC_prognosis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_prognosis_data shape: (65, 52, 52, 3)\n"
     ]
    }
   ],
   "source": [
    "# extract time feature \n",
    "TF_prognosis_data = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/DMFC/pre_treatment_hamd_reduction_50/data.npy')\n",
    "print('TF_prognosis_data shape:', TF_prognosis_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
