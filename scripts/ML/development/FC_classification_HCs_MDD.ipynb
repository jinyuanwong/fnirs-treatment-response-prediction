{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import mannwhitneyu    \n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# caculating_10_important_features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "# sig_data = DATA.reshape(DATA.shape[0], -1)\n",
    "def train_model_using_kfold(data, label, model, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def train_model_using_loocv(data, label, model):\n",
    "    loo = LeaveOneOut()\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in loo.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result), model\n",
    "\n",
    "stats_method = 'mannwhitneyu' # 'ranksums' or 'mannwhitneyu'\n",
    "def zero_diagnonal(arr):\n",
    "    # Loop over the first and last dimension\n",
    "    for i in range(arr.shape[0]):  # Loop over subjects\n",
    "        for j in range(arr.shape[-1]):  # Loop over views\n",
    "            np.fill_diagonal(arr[i, :, :, j], 0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def return_significant_FCvalue(adj, labels):\n",
    "    adj = zero_diagnonal(adj)\n",
    "    hc_adj = adj[np.where(labels==1)]\n",
    "    md_adj = adj[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    num_view = adj.shape[-1]\n",
    "    p_view = np.zeros((52,52,num_view))\n",
    "    effect_size = np.zeros((52,52,num_view))\n",
    "    stats = np.zeros((52,52,num_view))\n",
    "    for view in range(num_view):\n",
    "        for seed in range(52):\n",
    "            for target in range(52):\n",
    "                hc_val = hc_adj[:, seed, target, view]\n",
    "                md_val = md_adj[:, seed, target, view]\n",
    "                if stats_method == 'mannwhitneyu':\n",
    "                    stat, p1 = mannwhitneyu(hc_val,md_val)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('stats_method should be mannwhitneyu or ranksums')\n",
    "                p_view[seed, target, view] = p1\n",
    "                stats[seed, target, view] = stat\n",
    "    \n",
    "    adj = adj.reshape(adj.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    return adj[:, p_view<0.05]\n",
    "\n",
    "def get_activity_start_time(data, index_start):\n",
    "    gradient = np.gradient(data)\n",
    "    max_gradient = np.argmax(gradient[0:int(index_start*1.2)])  # 0:index_start*4 # current index_start = 400,\n",
    "    if max_gradient <= index_start:\n",
    "        max_gradient = index_start\n",
    "    return max_gradient\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the left slope based on the data sample time from activity_start_time and  activity_start_time + task_duration//2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_left_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start: activity_start+task_duration//2]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the right slope based on the data sample time from activity_start_time + task_duration//2 and activity_start_time + task_duration  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_right_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start+task_duration//2: activity_start+task_duration]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For calculating FWHM\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_FWHM(y_values, activity_start, task_duration):\n",
    "    # make sure the peak value is situated in the task duration period\n",
    "    task = y_values[activity_start:activity_start+task_duration]\n",
    "    max_task = np.max(task)  # Find the maximum y value\n",
    "    half_max_y = max_task / 2.0\n",
    "    max_index_task = np.argmax(task)\n",
    "    # if max_index_task is in the first two values, set left_index to 0\n",
    "    if max_index_task <= 1:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = find_nearest(y_values[:max_index_task], half_max_y)\n",
    "    # if max_index_task is in the last two values, set right_index to the last value\n",
    "    if max_index_task >= activity_start+task_duration-1:\n",
    "        right_index = task_duration-1\n",
    "    else:\n",
    "        right_index = find_nearest(\n",
    "            y_values[max_index_task:], half_max_y) + max_index_task\n",
    "\n",
    "    return right_index - left_index\n",
    "\n",
    "\n",
    "\"\"\"Get all 10 features\"\"\"\n",
    "\n",
    "\n",
    "def get_10_features(hbo, index_start, task_duration):\n",
    "    feature_shape = hbo.shape[:2]\n",
    "    # Feature 1 mean\n",
    "    mean = np.mean(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 2 variance\n",
    "    variance = np.std(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 3 activity_start_time\n",
    "    activity_start_time = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start_time[sub, ch] = get_activity_start_time(\n",
    "                hbo[sub, ch], index_start=index_start)\n",
    "\n",
    "    # # Feature 4 left_slope\n",
    "    left_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            left_slope[sub, ch] = get_left_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "    # # Feature 5  right_slope\n",
    "    right_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            right_slope[sub, ch] = get_right_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "\n",
    "    # # Feature 6 kurtosis\n",
    "    kurt = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            kurt[sub, ch] = kurtosis(hbo[sub, ch])\n",
    "    # There might be some nan in kurtosis calucaltion because of all 0-value array\n",
    "    kurt = np.nan_to_num(kurt)\n",
    "\n",
    "    # # Feature 7 skewness\n",
    "    skewness = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            skewness[sub, ch] = skew(hbo[sub, ch])\n",
    "    # There might be some nan in skewness calucaltion because of all 0-value array\n",
    "    skewness = np.nan_to_num(skewness)\n",
    "\n",
    "    # # Feature 8 area under the curve AUC Based on the sample time from activity_start_time + task_duration\n",
    "    AUC = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            AUC[sub, ch] = np.sum(\n",
    "                hbo[sub, ch][activity_start:activity_start+task_duration])\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(AUC[sub])\n",
    "\n",
    "    # # Feature 9 full width half maximum (FWHM)\n",
    "    # FWHM\n",
    "    FWHM = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            FWHM[sub, ch] = get_FWHM(\n",
    "                hbo[sub, ch], activity_start, task_duration)\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(FWHM[sub])\n",
    "\n",
    "    # # Feature 10 peak\n",
    "    peak = np.max(hbo, axis=2)\n",
    "\n",
    "    features = np.concatenate((mean, variance, activity_start_time,\n",
    "                              left_slope, right_slope, kurt, skewness, AUC, FWHM, peak), axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_significant_feature(data, labels):\n",
    "    hc_adj = data[np.where(labels==1)]\n",
    "    md_adj = data[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    p_view = np.zeros((hc_adj.shape[1:]))\n",
    "    for view in range(p_view.shape[-1]):\n",
    "        hc_val = hc_adj[:, view]\n",
    "        mdd_val = md_adj[:, view]\n",
    "        _, p1 = mannwhitneyu(hc_val,mdd_val)\n",
    "        p_view[view] = p1\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    data = data[:, p_view<0.05]\n",
    "    return data \n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # 明确指定labels参数\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    # 现在cm是一个2x2矩阵，即使数据只包含一个类别\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, f1\n",
    "\n",
    "def print_md_table(model_name, set, metrics):\n",
    "    print()\n",
    "    print('| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |')\n",
    "    print('|------------|----------|----------|-------------|-------------|----------|')\n",
    "    print(f'| {model_name} | {set} |', end = '')\n",
    "    for i in range(4):\n",
    "        print(f\" {metrics[i]:.4f} |\", end = '')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training Decision Tree, num_time: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/results/ML_results'\n",
    "if not os.path.exists(output_fold):\n",
    "    os.makedirs(output_fold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "res = {}\n",
    "\n",
    "# 初始化模型，同时设置随机种子\n",
    "models = {\n",
    "    # \"Logistic Regression\": LogisticRegression(max_iter=150),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    # \"Random Forest\": RandomForestClassifier(),\n",
    "    # \"SVM\": SVC(),\n",
    "}\n",
    "for name, model in models.items():\n",
    "    # run multiple time, using different time stamp as random seed\n",
    "    for num_time in range(1):\n",
    "        print(f'current training {name}, num_time: {num_time}')\n",
    "\n",
    "        # 使用当前时间戳作为随机种子\n",
    "        current_time_seed = 1706166341\n",
    "\n",
    "        # build model\n",
    "        model.random_state = current_time_seed\n",
    "\n",
    "        hb_result = {}\n",
    "        HB_TYPE_accuraies = {}\n",
    "        HB_TYPE_y_pred_and_y_test = {}\n",
    "        for HB_TYPE in ['HbO', 'HbR', 'HbT']:\n",
    "            HB_TYPE_accuraies[HB_TYPE] = []\n",
    "            HB_TYPE_y_pred_and_y_test[HB_TYPE] = []\n",
    "\n",
    "            # read data\n",
    "            fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis'\n",
    "\n",
    "            hb = np.load(fold + '/fc_data.npy')\n",
    "            hb = np.nan_to_num(hb)\n",
    "            label = np.load(fold + '/label.npy')\n",
    "\n",
    "            if HB_TYPE == 'HbO':\n",
    "                hb = hb[..., 0]\n",
    "            elif HB_TYPE == 'HbR':\n",
    "                hb = hb[..., 1]\n",
    "            else:\n",
    "                hb = hb[..., 2]\n",
    "            hb_2d = np.reshape(hb, (hb.shape[0], -1))\n",
    "\n",
    "            # Apply LOOCV to train the model\n",
    "            # Initialize LeaveOneOut\n",
    "            loo = LeaveOneOut()\n",
    "\n",
    "            # 存储每个模型的准确率\n",
    "            accuracies = {}\n",
    "\n",
    "            # Loop over each train/test split\n",
    "            for train_index, test_index in loo.split(hb_2d):\n",
    "                # Split the data into training and testing sets\n",
    "                X_train, X_test = hb_2d[train_index], hb_2d[test_index]\n",
    "                y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "                # Train the classifier\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict the label for the test set\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Calculate the accuracy for the current fold\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Append the accuracy to the list\n",
    "                HB_TYPE_accuraies[HB_TYPE].append(accuracy)\n",
    "\n",
    "                HB_TYPE_y_pred_and_y_test[HB_TYPE].append([y_pred, y_test])\n",
    "            accuracies[HB_TYPE] = 1\n",
    "            accuracies[HB_TYPE] = np.mean(HB_TYPE_accuraies[HB_TYPE])\n",
    "\n",
    "        save_result = {}\n",
    "        save_result['accuracies'] = accuracies\n",
    "        save_result['model_accuraies'] = HB_TYPE_accuraies\n",
    "        save_result['current_time_seed'] = current_time_seed\n",
    "        save_result['num_time'] = num_time\n",
    "        save_result['HB_TYPE_y_pred_and_y_test'] = HB_TYPE_y_pred_and_y_test\n",
    "\n",
    "        res[f'{num_time}'] = save_result\n",
    "    # np.save(output_fold + f'/{name}_result_validate.npy', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbO | 0.5929 | 0.6000 | 0.5857 | 0.5957 |\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbR | 0.4571 | 0.3714 | 0.5429 | 0.4062 |\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbT | 0.2643 | 0.2571 | 0.2714 | 0.2590 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "for hb_type in ['HbO', 'HbR', 'HbT']:\n",
    "    res_true_pred = np.array(save_result['HB_TYPE_y_pred_and_y_test'][hb_type])\n",
    "\n",
    "    y_true = res_true_pred[:, 1]\n",
    "    y_pred = res_true_pred[:, 0]\n",
    "\n",
    "    res_metrics = get_metrics(y_true, y_pred)\n",
    "    print_md_table('Decision Tree', 'test' + hb_type, res_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "RS_FC = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/fc_data.npy')\n",
    "LABEL = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/label.npy')\n",
    "\n",
    "HB = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/hb_data.npy')\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import zscore\n",
    "demographic_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/demographic_data.npy'\n",
    "demo_data = np.load(demographic_path)\n",
    "demo_data_normalized = zscore(demo_data, axis=0)\n",
    "deleted_demo_data_normalized = np.delete(demo_data_normalized, 6, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hbo_10_features = get_10_features(HB[..., 0], 100, 600)\n",
    "hbr_10_features = get_10_features(HB[..., 1], 100, 600)\n",
    "hbt_10_features = get_10_features(HB[..., 2], 100, 600)\n",
    "# get the significant data \n",
    "significant_FC = return_significant_FCvalue(RS_FC, LABEL)\n",
    "sig_hbo_feature = get_significant_feature(hbo_10_features, LABEL)\n",
    "sig_hbr_feature = get_significant_feature(hbr_10_features, LABEL)\n",
    "sig_hbt_feature = get_significant_feature(hbt_10_features, LABEL)\n",
    "\n",
    "sig_hb = np.concatenate((sig_hbo_feature, sig_hbr_feature, sig_hbt_feature), axis=1)\n",
    "\n",
    "\n",
    "sig_data = np.concatenate((significant_FC, sig_hb), axis=1)\n",
    "sig_demographic_fc = np.concatenate((demo_data_normalized, significant_FC), axis=1)\n",
    "# using XGBoost to train the model\n",
    "\n",
    "# show the result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_demographic_fc = np.concatenate((deleted_demo_data_normalized, significant_FC), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 219)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_demographic_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m      5\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m      6\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m n_estimators,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 存储每个模型的准确率\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     result,model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_using_loocv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignificant_FC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     res_metrics \u001b[38;5;241m=\u001b[39m get_metrics(result[:, \u001b[38;5;241m1\u001b[39m], result[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# result = train_model_using_kfold(deleted_demo_data_normalized, LABEL, model)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# res_metrics = kfold_get_metrics(result)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 57\u001b[0m, in \u001b[0;36mtrain_model_using_loocv\u001b[0;34m(data, label, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m label[train_index], label[test_index]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Predict the label for the test set\u001b[39;00m\n\u001b[1;32m     60\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1491\u001b[0m (\n\u001b[1;32m   1492\u001b[0m     model,\n\u001b[1;32m   1493\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1517\u001b[0m )\n\u001b[0;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:2047\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtrain, DMatrix):\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid training matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtrain)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2047\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_dmatrix_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[1;32m   2051\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2053\u001b[0m         )\n\u001b[1;32m   2054\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:2933\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m   2932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m \u001b[38;5;241m=\u001b[39m ft\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(fn)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:1989\u001b[0m, in \u001b[0;36mBooster.feature_types\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;129m@feature_types\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_types\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: Optional[FeatureTypes]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_feature_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xgboost/core.py:1973\u001b[0m, in \u001b[0;36mBooster._set_feature_info\u001b[0;34m(self, features, field)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     _check_call(\n\u001b[1;32m   1965\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterSetStrFeatureInfo(\n\u001b[1;32m   1966\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m     )\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1973\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterSetStrFeatureInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    learning_rate = random.uniform(0.1, 0.6)\n",
    "    n_estimators = np.random.choice([1,5,50,100,200,500,1000])\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=20,\n",
    "        reg_alpha=10,\n",
    "        reg_lambda=15,\n",
    "        n_estimators= n_estimators,\n",
    "    )\n",
    "# 存储每个模型的准确率\n",
    "    result,model = train_model_using_loocv(significant_FC, LABEL, model)\n",
    "    res_metrics = get_metrics(result[:, 1], result[:, 0])\n",
    "    \n",
    "    # result = train_model_using_kfold(deleted_demo_data_normalized, LABEL, model)\n",
    "    # res_metrics = kfold_get_metrics(result)\n",
    "    print(f'learning_rate: {learning_rate}, n_estimators: {n_estimators}')\n",
    "    print_md_table('Decision Tree', 'test', res_metrics)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征重要性： [0.00734201 0.         0.         0.29543024 0.         0.16029395\n",
      " 0.01750298 0.44857877 0.         0.07085204]\n",
      "Feature: Age (years), Score: 0.00734, 0\n",
      "Feature: Sex, Score: 0.00000, 1\n",
      "Feature: Ethnicity, Score: 0.00000, 2\n",
      "Feature: Handedness, Score: 0.29543, 3\n",
      "Feature: Education (years), Score: 0.00000, 4\n",
      "Feature: Current/highest education level —  Coded, Score: 0.16029, 5\n",
      "Feature: Perceived social support, Score: 0.01750, 6\n",
      "Feature: Past trauma, Score: 0.44858, 7\n",
      "Feature: Current psychiatric comorbidities — Binary, Score: 0.00000, 8\n",
      "Feature: Current psychiatric comorbidities — Coded, Score: 0.07085, 9\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "print(\"特征重要性：\", feature_importances)\n",
    "\n",
    "demographic_name = np.array([\n",
    "    ['Age (years)', 'Sex', 'Ethnicity', 'Handedness', 'Education (years)', \n",
    "    'Current/highest education level —  Coded', 'Perceived social support', \n",
    "    'Past trauma', 'Current psychiatric comorbidities — Binary', \n",
    "    'Current psychiatric comorbidities — Coded', 'Family history of psychiatric illness']\n",
    "])\n",
    "\n",
    "for i,v in enumerate(feature_importances):\n",
    "    print('Feature: %s, Score: %.5f, %0d' % (demographic_name[0][i],v,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_get_metrics(result):\n",
    "    folds_result = []\n",
    "    for i in range(len(result)):\n",
    "        x = get_metrics(result[i,1], result[i,0])\n",
    "        folds_result.append(x)\n",
    "\n",
    "    folds_result = np.array(folds_result)\n",
    "    folds_result = np.mean(folds_result, axis=0)\n",
    "    return folds_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
