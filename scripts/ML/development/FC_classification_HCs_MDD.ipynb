{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import mannwhitneyu    \n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# caculating_10_important_features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "# sig_data = DATA.reshape(DATA.shape[0], -1)\n",
    "def train_model_using_kfold(data, label, model, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def train_model_using_loocv(data, label, model):\n",
    "    loo = LeaveOneOut()\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in loo.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result), model\n",
    "\n",
    "stats_method = 'mannwhitneyu' # 'ranksums' or 'mannwhitneyu'\n",
    "def zero_diagnonal(arr):\n",
    "    # Loop over the first and last dimension\n",
    "    for i in range(arr.shape[0]):  # Loop over subjects\n",
    "        for j in range(arr.shape[-1]):  # Loop over views\n",
    "            np.fill_diagonal(arr[i, :, :, j], 0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def return_significant_FCvalue(adj, labels):\n",
    "    adj = zero_diagnonal(adj)\n",
    "    hc_adj = adj[np.where(labels==1)]\n",
    "    md_adj = adj[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    num_view = adj.shape[-1]\n",
    "    p_view = np.zeros((52,52,num_view))\n",
    "    effect_size = np.zeros((52,52,num_view))\n",
    "    stats = np.zeros((52,52,num_view))\n",
    "    for view in range(num_view):\n",
    "        for seed in range(52):\n",
    "            for target in range(52):\n",
    "                hc_val = hc_adj[:, seed, target, view]\n",
    "                md_val = md_adj[:, seed, target, view]\n",
    "                if stats_method == 'mannwhitneyu':\n",
    "                    stat, p1 = mannwhitneyu(hc_val,md_val)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('stats_method should be mannwhitneyu or ranksums')\n",
    "                p_view[seed, target, view] = p1\n",
    "                stats[seed, target, view] = stat\n",
    "    \n",
    "    adj = adj.reshape(adj.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    return adj[:, p_view<0.05]\n",
    "\n",
    "def get_activity_start_time(data, index_start):\n",
    "    gradient = np.gradient(data)\n",
    "    max_gradient = np.argmax(gradient[0:int(index_start*1.2)])  # 0:index_start*4 # current index_start = 400,\n",
    "    if max_gradient <= index_start:\n",
    "        max_gradient = index_start\n",
    "    return max_gradient\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the left slope based on the data sample time from activity_start_time and  activity_start_time + task_duration//2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_left_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start: activity_start+task_duration//2]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the right slope based on the data sample time from activity_start_time + task_duration//2 and activity_start_time + task_duration  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_right_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start+task_duration//2: activity_start+task_duration]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For calculating FWHM\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_FWHM(y_values, activity_start, task_duration):\n",
    "    # make sure the peak value is situated in the task duration period\n",
    "    task = y_values[activity_start:activity_start+task_duration]\n",
    "    max_task = np.max(task)  # Find the maximum y value\n",
    "    half_max_y = max_task / 2.0\n",
    "    max_index_task = np.argmax(task)\n",
    "    # if max_index_task is in the first two values, set left_index to 0\n",
    "    if max_index_task <= 1:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = find_nearest(y_values[:max_index_task], half_max_y)\n",
    "    # if max_index_task is in the last two values, set right_index to the last value\n",
    "    if max_index_task >= activity_start+task_duration-1:\n",
    "        right_index = task_duration-1\n",
    "    else:\n",
    "        right_index = find_nearest(\n",
    "            y_values[max_index_task:], half_max_y) + max_index_task\n",
    "\n",
    "    return right_index - left_index\n",
    "\n",
    "\n",
    "\"\"\"Get all 10 features\"\"\"\n",
    "\n",
    "\n",
    "def get_10_features(hbo, index_start, task_duration):\n",
    "    feature_shape = hbo.shape[:2]\n",
    "    # Feature 1 mean\n",
    "    mean = np.mean(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 2 variance\n",
    "    variance = np.std(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 3 activity_start_time\n",
    "    activity_start_time = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start_time[sub, ch] = get_activity_start_time(\n",
    "                hbo[sub, ch], index_start=index_start)\n",
    "\n",
    "    # # Feature 4 left_slope\n",
    "    left_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            left_slope[sub, ch] = get_left_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "    # # Feature 5  right_slope\n",
    "    right_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            right_slope[sub, ch] = get_right_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "\n",
    "    # # Feature 6 kurtosis\n",
    "    kurt = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            kurt[sub, ch] = kurtosis(hbo[sub, ch])\n",
    "    # There might be some nan in kurtosis calucaltion because of all 0-value array\n",
    "    kurt = np.nan_to_num(kurt)\n",
    "\n",
    "    # # Feature 7 skewness\n",
    "    skewness = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            skewness[sub, ch] = skew(hbo[sub, ch])\n",
    "    # There might be some nan in skewness calucaltion because of all 0-value array\n",
    "    skewness = np.nan_to_num(skewness)\n",
    "\n",
    "    # # Feature 8 area under the curve AUC Based on the sample time from activity_start_time + task_duration\n",
    "    AUC = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            AUC[sub, ch] = np.sum(\n",
    "                hbo[sub, ch][activity_start:activity_start+task_duration])\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(AUC[sub])\n",
    "\n",
    "    # # Feature 9 full width half maximum (FWHM)\n",
    "    # FWHM\n",
    "    FWHM = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            FWHM[sub, ch] = get_FWHM(\n",
    "                hbo[sub, ch], activity_start, task_duration)\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(FWHM[sub])\n",
    "\n",
    "    # # Feature 10 peak\n",
    "    peak = np.max(hbo, axis=2)\n",
    "\n",
    "    features = np.concatenate((mean, variance, activity_start_time,\n",
    "                              left_slope, right_slope, kurt, skewness, AUC, FWHM, peak), axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_significant_feature(data, labels):\n",
    "    hc_adj = data[np.where(labels==1)]\n",
    "    md_adj = data[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    p_view = np.zeros((hc_adj.shape[1:]))\n",
    "    for view in range(p_view.shape[-1]):\n",
    "        hc_val = hc_adj[:, view]\n",
    "        mdd_val = md_adj[:, view]\n",
    "        _, p1 = mannwhitneyu(hc_val,mdd_val)\n",
    "        p_view[view] = p1\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    data = data[:, p_view<0.05]\n",
    "    return data \n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # 明确指定labels参数\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    # 现在cm是一个2x2矩阵，即使数据只包含一个类别\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, f1\n",
    "\n",
    "def print_md_table(model_name, set, metrics):\n",
    "    print()\n",
    "    print('| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |')\n",
    "    print('|------------|----------|----------|-------------|-------------|----------|')\n",
    "    print(f'| {model_name} | {set} |', end = '')\n",
    "    for i in range(4):\n",
    "        print(f\" {metrics[i]:.4f} |\", end = '')\n",
    "    print()\n",
    "def individual_normalization(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i] = (data[i] - np.mean(data[i])) #/ np.std(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training Decision Tree, num_time: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/results/ML_results'\n",
    "if not os.path.exists(output_fold):\n",
    "    os.makedirs(output_fold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "res = {}\n",
    "\n",
    "# 初始化模型，同时设置随机种子\n",
    "models = {\n",
    "    # \"Logistic Regression\": LogisticRegression(max_iter=150),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    # \"Random Forest\": RandomForestClassifier(),\n",
    "    # \"SVM\": SVC(),\n",
    "}\n",
    "for name, model in models.items():\n",
    "    # run multiple time, using different time stamp as random seed\n",
    "    for num_time in range(1):\n",
    "        print(f'current training {name}, num_time: {num_time}')\n",
    "\n",
    "        # 使用当前时间戳作为随机种子\n",
    "        current_time_seed = 1706166341\n",
    "\n",
    "        # build model\n",
    "        model.random_state = current_time_seed\n",
    "\n",
    "        hb_result = {}\n",
    "        HB_TYPE_accuraies = {}\n",
    "        HB_TYPE_y_pred_and_y_test = {}\n",
    "        for HB_TYPE in ['HbO', 'HbR', 'HbT']:\n",
    "            HB_TYPE_accuraies[HB_TYPE] = []\n",
    "            HB_TYPE_y_pred_and_y_test[HB_TYPE] = []\n",
    "\n",
    "            # read data\n",
    "            fold = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis'\n",
    "\n",
    "            hb = np.load(fold + '/hb_data.npy')\n",
    "            hb = np.nan_to_num(hb)\n",
    "            label = np.load(fold + '/label.npy')\n",
    "\n",
    "            if HB_TYPE == 'HbO':\n",
    "                hb = hb[..., 0]\n",
    "            elif HB_TYPE == 'HbR':\n",
    "                hb = hb[..., 1]\n",
    "            else:\n",
    "                hb = hb[..., 2]\n",
    "            hb_2d = np.reshape(hb, (hb.shape[0], -1))\n",
    "\n",
    "            # Apply LOOCV to train the model\n",
    "            # Initialize LeaveOneOut\n",
    "            loo = LeaveOneOut()\n",
    "\n",
    "            # 存储每个模型的准确率\n",
    "            accuracies = {}\n",
    "\n",
    "            # Loop over each train/test split\n",
    "            for train_index, test_index in loo.split(hb_2d):\n",
    "                # Split the data into training and testing sets\n",
    "                X_train, X_test = hb_2d[train_index], hb_2d[test_index]\n",
    "                y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "                # Train the classifier\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict the label for the test set\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Calculate the accuracy for the current fold\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Append the accuracy to the list\n",
    "                HB_TYPE_accuraies[HB_TYPE].append(accuracy)\n",
    "\n",
    "                HB_TYPE_y_pred_and_y_test[HB_TYPE].append([y_pred, y_test])\n",
    "            accuracies[HB_TYPE] = 1\n",
    "            accuracies[HB_TYPE] = np.mean(HB_TYPE_accuraies[HB_TYPE])\n",
    "\n",
    "        save_result = {}\n",
    "        save_result['accuracies'] = accuracies\n",
    "        save_result['model_accuraies'] = HB_TYPE_accuraies\n",
    "        save_result['current_time_seed'] = current_time_seed\n",
    "        save_result['num_time'] = num_time\n",
    "        save_result['HB_TYPE_y_pred_and_y_test'] = HB_TYPE_y_pred_and_y_test\n",
    "\n",
    "        res[f'{num_time}'] = save_result\n",
    "    # np.save(output_fold + f'/{name}_result_validate.npy', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbO | 0.6714 | 0.6000 | 0.7429 | 0.6462 |\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbR | 0.4714 | 0.3429 | 0.6000 | 0.3934 |\n",
      "\n",
      "| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "|------------|----------|----------|-------------|-------------|----------|\n",
      "| Decision Tree | testHbT | 0.5857 | 0.6000 | 0.5714 | 0.5915 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "for hb_type in ['HbO', 'HbR', 'HbT']:\n",
    "    res_true_pred = np.array(save_result['HB_TYPE_y_pred_and_y_test'][hb_type])\n",
    "\n",
    "    y_true = res_true_pred[:, 1]\n",
    "    y_pred = res_true_pred[:, 0]\n",
    "\n",
    "    res_metrics = get_metrics(y_true, y_pred)\n",
    "    print_md_table('Decision Tree', 'test' + hb_type, res_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0c28c40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have already trained the model and stored it in the variable 'model'\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances = feature_importances.reshape(52, -1)\n",
    "feature_importances = np.reshape(feature_importances[:,:1250], (52, -1, 10))\n",
    "feature_importances = np.mean(feature_importances, axis=-1)\n",
    "print(feature_importances.shape)\n",
    "plt.imshow(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.48076923076923"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(feature_importances)/52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/c5r3qbs52hg_qgrqgmmp61_m0000gn/T/ipykernel_70317/2961210513.py:206: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt[sub, ch] = kurtosis(hbo[sub, ch])\n",
      "/var/folders/dj/c5r3qbs52hg_qgrqgmmp61_m0000gn/T/ipykernel_70317/2961210513.py:214: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness[sub, ch] = skew(hbo[sub, ch])\n"
     ]
    }
   ],
   "source": [
    "# read data \n",
    "RS_FC = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/fc_data.npy')\n",
    "LABEL = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/label.npy')\n",
    "HB = np.load('/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/hb_data.npy')\n",
    "\n",
    "HB = individual_normalization(HB)\n",
    "        \n",
    "\n",
    "\n",
    "from scipy.stats import zscore\n",
    "demographic_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/demographic_data.npy'\n",
    "demo_data = np.load(demographic_path)\n",
    "demo_data_normalized = zscore(demo_data, axis=0)\n",
    "deleted_demo_data_normalized = np.delete(demo_data_normalized, 6, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hbo_10_features = get_10_features(HB[..., 0], 100, 600)\n",
    "hbr_10_features = get_10_features(HB[..., 1], 100, 600)\n",
    "hbt_10_features = get_10_features(HB[..., 2], 100, 600)\n",
    "# get the significant data \n",
    "significant_FC = return_significant_FCvalue(RS_FC, LABEL)\n",
    "sig_hbo_feature = get_significant_feature(hbo_10_features, LABEL)\n",
    "sig_hbr_feature = get_significant_feature(hbr_10_features, LABEL)\n",
    "sig_hbt_feature = get_significant_feature(hbt_10_features, LABEL)\n",
    "\n",
    "sig_hb = np.concatenate((sig_hbo_feature, sig_hbr_feature, sig_hbt_feature), axis=1)\n",
    "\n",
    "\n",
    "sig_data = np.concatenate((significant_FC, sig_hb), axis=1)\n",
    "sig_demographic_fc = np.concatenate((demo_data_normalized, significant_FC), axis=1)\n",
    "# using XGBoost to train the model\n",
    "\n",
    "# show the result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_demographic_fc = np.concatenate((deleted_demo_data_normalized, significant_FC), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    learning_rate = random.uniform(0.1, 0.6)\n",
    "    n_estimators = np.random.choice([1,5,50,100,200,500,1000])\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=20,\n",
    "        reg_alpha=10,\n",
    "        reg_lambda=15,\n",
    "        n_estimators= n_estimators,\n",
    "    )\n",
    "# 存储每个模型的准确率\n",
    "    result,model = train_model_using_loocv(significant_FC.reshape(significant_FC.shape[0],-1), LABEL, model)\n",
    "    res_metrics = get_metrics(result[:, 1], result[:, 0])\n",
    "    \n",
    "    # result = train_model_using_kfold(deleted_demo_data_normalized, LABEL, model)\n",
    "    # res_metrics = kfold_get_metrics(result)\n",
    "    print(f'learning_rate: {learning_rate}, n_estimators: {n_estimators}')\n",
    "    print_md_table('Decision Tree', 'test', res_metrics)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征重要性： [0.00734201 0.         0.         0.29543024 0.         0.16029395\n",
      " 0.01750298 0.44857877 0.         0.07085204]\n",
      "Feature: Age (years), Score: 0.00734, 0\n",
      "Feature: Sex, Score: 0.00000, 1\n",
      "Feature: Ethnicity, Score: 0.00000, 2\n",
      "Feature: Handedness, Score: 0.29543, 3\n",
      "Feature: Education (years), Score: 0.00000, 4\n",
      "Feature: Current/highest education level —  Coded, Score: 0.16029, 5\n",
      "Feature: Perceived social support, Score: 0.01750, 6\n",
      "Feature: Past trauma, Score: 0.44858, 7\n",
      "Feature: Current psychiatric comorbidities — Binary, Score: 0.00000, 8\n",
      "Feature: Current psychiatric comorbidities — Coded, Score: 0.07085, 9\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "print(\"特征重要性：\", feature_importances)\n",
    "\n",
    "demographic_name = np.array([\n",
    "    ['Age (years)', 'Sex', 'Ethnicity', 'Handedness', 'Education (years)', \n",
    "    'Current/highest education level —  Coded', 'Perceived social support', \n",
    "    'Past trauma', 'Current psychiatric comorbidities — Binary', \n",
    "    'Current psychiatric comorbidities — Coded', 'Family history of psychiatric illness']\n",
    "])\n",
    "\n",
    "for i,v in enumerate(feature_importances):\n",
    "    print('Feature: %s, Score: %.5f, %0d' % (demographic_name[0][i],v,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_get_metrics(result):\n",
    "    folds_result = []\n",
    "    for i in range(len(result)):\n",
    "        x = get_metrics(result[i,1], result[i,0])\n",
    "        folds_result.append(x)\n",
    "\n",
    "    folds_result = np.array(folds_result)\n",
    "    folds_result = np.mean(folds_result, axis=0)\n",
    "    return folds_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
