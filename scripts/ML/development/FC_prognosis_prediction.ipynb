{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import mannwhitneyu    \n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# caculating_10_important_features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "# sig_data = DATA.reshape(DATA.shape[0], -1)\n",
    "def train_model_using_kfold(data, label, model, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def train_model_using_loocv(data, label, model):\n",
    "    loo = LeaveOneOut()\n",
    "    result = []\n",
    "\n",
    "    # Loop over each train/test split\n",
    "    for train_index, test_index in loo.split(data):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "        # Train the classifier\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the label for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Append the accuracy to the list\n",
    "        result.append([y_pred, y_test])\n",
    "\n",
    "    return np.array(result), model\n",
    "\n",
    "stats_method = 'mannwhitneyu' # 'ranksums' or 'mannwhitneyu'\n",
    "def zero_diagnonal(arr):\n",
    "    # Loop over the first and last dimension\n",
    "    for i in range(arr.shape[0]):  # Loop over subjects\n",
    "        for j in range(arr.shape[-1]):  # Loop over views\n",
    "            np.fill_diagonal(arr[i, :, :, j], 0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def return_significant_FCvalue(adj, labels):\n",
    "    adj = zero_diagnonal(adj)\n",
    "    hc_adj = adj[np.where(labels==1)]\n",
    "    md_adj = adj[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    num_view = adj.shape[-1]\n",
    "    p_view = np.zeros((52,52,num_view))\n",
    "    effect_size = np.zeros((52,52,num_view))\n",
    "    stats = np.zeros((52,52,num_view))\n",
    "    for view in range(num_view):\n",
    "        for seed in range(52):\n",
    "            for target in range(52):\n",
    "                hc_val = hc_adj[:, seed, target, view]\n",
    "                md_val = md_adj[:, seed, target, view]\n",
    "                if stats_method == 'mannwhitneyu':\n",
    "                    stat, p1 = mannwhitneyu(hc_val,md_val)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('stats_method should be mannwhitneyu or ranksums')\n",
    "                p_view[seed, target, view] = p1\n",
    "                stats[seed, target, view] = stat\n",
    "    \n",
    "    adj = adj.reshape(adj.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    return adj[:, p_view<0.05]\n",
    "\n",
    "def get_activity_start_time(data, index_start):\n",
    "    gradient = np.gradient(data)\n",
    "    max_gradient = np.argmax(gradient[0:int(index_start*1.2)])  # 0:index_start*4 # current index_start = 400,\n",
    "    if max_gradient <= index_start:\n",
    "        max_gradient = index_start\n",
    "    return max_gradient\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the left slope based on the data sample time from activity_start_time and  activity_start_time + task_duration//2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_left_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start: activity_start+task_duration//2]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calulate the right slope based on the data sample time from activity_start_time + task_duration//2 and activity_start_time + task_duration  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_right_slope(data, activity_start, task_duration):\n",
    "    activity_start = int(activity_start)\n",
    "    data = data[activity_start+task_duration//2: activity_start+task_duration]\n",
    "    slope, _ = np.polyfit(np.arange(data.shape[0]), data, 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For calculating FWHM\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_FWHM(y_values, activity_start, task_duration):\n",
    "    # make sure the peak value is situated in the task duration period\n",
    "    task = y_values[activity_start:activity_start+task_duration]\n",
    "    max_task = np.max(task)  # Find the maximum y value\n",
    "    half_max_y = max_task / 2.0\n",
    "    max_index_task = np.argmax(task)\n",
    "    # if max_index_task is in the first two values, set left_index to 0\n",
    "    if max_index_task <= 1:\n",
    "        left_index = 0\n",
    "    else:\n",
    "        left_index = find_nearest(y_values[:max_index_task], half_max_y)\n",
    "    # if max_index_task is in the last two values, set right_index to the last value\n",
    "    if max_index_task >= activity_start+task_duration-1:\n",
    "        right_index = task_duration-1\n",
    "    else:\n",
    "        right_index = find_nearest(\n",
    "            y_values[max_index_task:], half_max_y) + max_index_task\n",
    "\n",
    "    return right_index - left_index\n",
    "\n",
    "\n",
    "\"\"\"Get all 10 features\"\"\"\n",
    "\n",
    "\n",
    "def get_10_features(hbo, index_start, task_duration):\n",
    "    feature_shape = hbo.shape[:2]\n",
    "    # Feature 1 mean\n",
    "    mean = np.mean(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 2 variance\n",
    "    variance = np.std(hbo, axis=2)  # feature shape is (subject, channel)\n",
    "\n",
    "    # Feature 3 activity_start_time\n",
    "    activity_start_time = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start_time[sub, ch] = get_activity_start_time(\n",
    "                hbo[sub, ch], index_start=index_start)\n",
    "\n",
    "    # # Feature 4 left_slope\n",
    "    left_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            left_slope[sub, ch] = get_left_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "    # # Feature 5  right_slope\n",
    "    right_slope = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            right_slope[sub, ch] = get_right_slope(\n",
    "                hbo[sub, ch], activity_start=activity_start_time[sub, ch], task_duration=task_duration)\n",
    "\n",
    "    # # Feature 6 kurtosis\n",
    "    kurt = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            kurt[sub, ch] = kurtosis(hbo[sub, ch])\n",
    "    # There might be some nan in kurtosis calucaltion because of all 0-value array\n",
    "    kurt = np.nan_to_num(kurt)\n",
    "\n",
    "    # # Feature 7 skewness\n",
    "    skewness = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            skewness[sub, ch] = skew(hbo[sub, ch])\n",
    "    # There might be some nan in skewness calucaltion because of all 0-value array\n",
    "    skewness = np.nan_to_num(skewness)\n",
    "\n",
    "    # # Feature 8 area under the curve AUC Based on the sample time from activity_start_time + task_duration\n",
    "    AUC = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            AUC[sub, ch] = np.sum(\n",
    "                hbo[sub, ch][activity_start:activity_start+task_duration])\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(AUC[sub])\n",
    "\n",
    "    # # Feature 9 full width half maximum (FWHM)\n",
    "    # FWHM\n",
    "    FWHM = np.empty_like(mean)\n",
    "    for sub in range(feature_shape[0]):\n",
    "        for ch in range(feature_shape[1]):\n",
    "            activity_start = int(activity_start_time[sub, ch])\n",
    "            FWHM[sub, ch] = get_FWHM(\n",
    "                hbo[sub, ch], activity_start, task_duration)\n",
    "    # for sub in range(10):\n",
    "    #     plt.plot(FWHM[sub])\n",
    "\n",
    "    # # Feature 10 peak\n",
    "    peak = np.max(hbo, axis=2)\n",
    "\n",
    "    features = np.concatenate((mean, variance, activity_start_time,\n",
    "                              left_slope, right_slope, kurt, skewness, AUC, FWHM, peak), axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_significant_feature(data, labels):\n",
    "    hc_adj = data[np.where(labels==1)]\n",
    "    md_adj = data[np.where(labels==0)]\n",
    "    count = 0\n",
    "\n",
    "    p_view = np.zeros((hc_adj.shape[1:]))\n",
    "    for view in range(p_view.shape[-1]):\n",
    "        hc_val = hc_adj[:, view]\n",
    "        mdd_val = md_adj[:, view]\n",
    "        _, p1 = mannwhitneyu(hc_val,mdd_val)\n",
    "        p_view[view] = p1\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    p_view = p_view.reshape(-1)\n",
    "    data = data[:, p_view<0.05]\n",
    "    return data \n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # 明确指定labels参数\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    # 现在cm是一个2x2矩阵，即使数据只包含一个类别\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, f1\n",
    "\n",
    "def print_md_table(model_name, set, metrics):\n",
    "    print()\n",
    "    print('| Model Name |   Set   |Accuracy | Sensitivity | Specificity | F1 Score |')\n",
    "    print('|------------|----------|----------|-------------|-------------|----------|')\n",
    "    print(f'| {model_name} | {set} |', end = '')\n",
    "    for i in range(4):\n",
    "        print(f\" {metrics[i]:.4f} |\", end = '')\n",
    "    print()\n",
    "def individual_normalization(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i] = (data[i] - np.mean(data[i])) #/ np.std(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/DMFC/pre_treatment_hamd_reduction_50'\n",
    "FC = np.load(FOLD + '/fc_data.npy')\n",
    "LABEL = np.load(FOLD + '/label.npy')\n",
    "\n",
    "\n",
    "# HB = individual_normalization(HB)\n",
    "# from scipy.stats import zscore\n",
    "# demographic_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/diagnosis/demographic_data.npy'\n",
    "# demo_data = np.load(demographic_path)\n",
    "# demo_data_normalized = zscore(demo_data, axis=0)\n",
    "# deleted_demo_data_normalized = np.delete(demo_data_normalized, 6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    learning_rate = random.uniform(0.1, 0.6)\n",
    "    n_estimators = np.random.choice([1,5,50,100,200,500,1000])\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=20,\n",
    "        reg_alpha=10,\n",
    "        reg_lambda=15,\n",
    "        n_estimators= n_estimators,\n",
    "    )\n",
    "# 存储每个模型的准确率\n",
    "    result,model = train_model_using_loocv(significant_FC.reshape(significant_FC.shape[0],-1), LABEL, model)\n",
    "    res_metrics = get_metrics(result[:, 1], result[:, 0])\n",
    "    \n",
    "    # result = train_model_using_kfold(deleted_demo_data_normalized, LABEL, model)\n",
    "    # res_metrics = kfold_get_metrics(result)\n",
    "    print(f'learning_rate: {learning_rate}, n_estimators: {n_estimators}')\n",
    "    print_md_table('Decision Tree', 'test', res_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
