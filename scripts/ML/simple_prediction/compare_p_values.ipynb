{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system is Ubuntu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from fine_tune_model import define_classifier_for_classification_for_response\n",
    "from validation_method import stratified_5_fold_classification, nested_cross_validation_classification, loocv_classification\n",
    "from utils_simple_prediction import add_task_change_data, load_data_for_classification, add_cgi, add_mddr, set_path, load_task_change_data, save_model_seed\n",
    "import time\n",
    "import pandas as pd\n",
    "set_path()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result[model][modality][...]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "models = ['Naive Bayes', 'Discriminant Analysis(LDA)', 'Random Forest', 'XGBoost', 'SVM', 'SGDClassifier']#['Discriminant Analysis(LDA)', 'Naive Bayes'] #  'KNN', 'MLP',  , 'Discriminant Analysis(QDA)'\n",
    "modalities = ['fnirs_modality_classification', 'clinical_data_and_fnirs_modality_classification', 'clinical_data_modality_classification'] # ['fnirs_modality_classification'] # \n",
    "modalities_to_name = {\n",
    "    'clinical_data_modality_classification': 'Tier 1', \n",
    "    'fnirs_modality_classification': 'Tier 2', \n",
    "    'clinical_data_and_fnirs_modality_classification': 'Tier 3'\n",
    "\n",
    "}\n",
    "main_fold = 'results/ML_results/simple_prediction_response/'\n",
    "\n",
    "\n",
    "auc_result = result = {model: {modality: {} for modality in modalities} for model in models}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare if it is better than chance\n",
    "all_auc_fnirs = []\n",
    "all_auc_fnirs_cli = []\n",
    "all_bacc_fnirs_cli = []\n",
    "all_bacc_cli = []\n",
    "\n",
    "for model in models:\n",
    "    if model == 'SGDClassifier':\n",
    "        continue    \n",
    "    for modality in modalities:\n",
    "        result_pth = main_fold + modality + '/' + model + '.csv'\n",
    "        df = pd.read_csv(result_pth)\n",
    "        \n",
    "        \n",
    "        if modality == 'fnirs_modality_classification':\n",
    "            all_auc_fnirs.append(df['test_auc'].tolist())\n",
    "        \n",
    "        if modality == 'clinical_data_and_fnirs_modality_classification':\n",
    "            all_auc_fnirs_cli.append(df['test_auc'].tolist())\n",
    "\n",
    "        \n",
    "        if modality == 'clinical_data_modality_classification':\n",
    "            all_bacc_cli.append(df['test_acc'].tolist())\n",
    "        else:\n",
    "            all_bacc_fnirs_cli.append(df['test_acc'].tolist())\n",
    "\n",
    "\n",
    "all_auc_fnirs = np.array(all_auc_fnirs).reshape(-1).tolist()\n",
    "all_auc_fnirs_cli = np.array(all_auc_fnirs_cli).reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7524, 0.7456, 0.7823, 0.7701, 0.7483, 0.7551, 0.7673, 0.7442, 0.7619, 0.7878, 0.7986, 0.7565, 0.7646, 0.7592, 0.7864, 0.7456, 0.7456, 0.7401, 0.7565, 0.7646, 0.7946, 0.7959, 0.7633, 0.7687, 0.7864, 0.7578, 0.7442, 0.7986, 0.7483, 0.7592, 0.7878, 0.7837, 0.7442, 0.7429, 0.7592, 0.7714, 0.7401, 0.7456, 0.7946, 0.7456, 0.7537, 0.7878, 0.7918, 0.8027, 0.7796, 0.7701, 0.7741, 0.7633, 0.8014, 0.7633, 0.7633, 0.7565, 0.7864, 0.7456, 0.7442, 0.7456, 0.7918, 0.766, 0.7551, 0.7782, 0.766, 0.7796, 0.7769, 0.7537, 0.7633, 0.7537, 0.7701, 0.7619, 0.7578, 0.7633, 0.785, 0.7918, 0.7891, 0.7483, 0.7755, 0.7605, 0.7864, 0.7374, 0.7769, 0.7442, 0.7769, 0.751, 0.785, 0.7374, 0.7864, 0.7551, 0.7986, 0.7469, 0.7701, 0.7796, 0.7755, 0.7537, 0.7959, 0.7728, 0.7605, 0.7592, 0.7891, 0.7578, 0.7592, 0.7796, 0.6993, 0.717, 0.7061, 0.698, 0.7102, 0.7075, 0.7034, 0.7048, 0.7034, 0.7102, 0.6966, 0.698, 0.7034, 0.6966, 0.7211, 0.7034, 0.7102, 0.7129, 0.698, 0.7116, 0.702, 0.7061, 0.7184, 0.7116, 0.698, 0.7265, 0.702, 0.7075, 0.7034, 0.6993, 0.7129, 0.7075, 0.702, 0.7129, 0.7088, 0.6313, 0.7061, 0.7116, 0.7075, 0.7184, 0.698, 0.702, 0.7061, 0.6993, 0.7088, 0.7156, 0.717, 0.7034, 0.7034, 0.7034, 0.7279, 0.7075, 0.7075, 0.717, 0.7224, 0.7184, 0.7061, 0.7088, 0.7156, 0.702, 0.7184, 0.702, 0.7048, 0.7184, 0.7184, 0.7102, 0.702, 0.6993, 0.7143, 0.7075, 0.698, 0.7034, 0.702, 0.7034, 0.7034, 0.7034, 0.7007, 0.7061, 0.7075, 0.717, 0.702, 0.7088, 0.7224, 0.7007, 0.7224, 0.702, 0.7238, 0.7143, 0.698, 0.7048, 0.7061, 0.7034, 0.7075, 0.6653, 0.717, 0.7224, 0.7034, 0.7265, 0.7075, 0.7061, 0.6939, 0.6966, 0.6313, 0.6966, 0.6327, 0.6993, 0.6776, 0.6912, 0.7088, 0.6898, 0.6735, 0.6871, 0.7211, 0.698, 0.6925, 0.6857, 0.6871, 0.6816, 0.683, 0.6912, 0.6966, 0.6952, 0.6844, 0.7075, 0.6776, 0.6993, 0.6857, 0.6844, 0.7102, 0.6912, 0.6789, 0.6884, 0.6816, 0.6939, 0.7184, 0.6857, 0.6762, 0.7224, 0.7007, 0.6776, 0.6871, 0.7075, 0.6993, 0.6966, 0.698, 0.7075, 0.702, 0.6925, 0.698, 0.6844, 0.6789, 0.6884, 0.6844, 0.6966, 0.683, 0.7156, 0.6857, 0.702, 0.7088, 0.6816, 0.6925, 0.6993, 0.6789, 0.6694, 0.6571, 0.6966, 0.6925, 0.6776, 0.6857, 0.698, 0.698, 0.7088, 0.7088, 0.6789, 0.6993, 0.6925, 0.702, 0.6993, 0.6898, 0.6912, 0.702, 0.6789, 0.7116, 0.6857, 0.7184, 0.6857, 0.698, 0.6952, 0.6952, 0.7075, 0.6762, 0.6857, 0.7129, 0.7088, 0.6993, 0.6803, 0.7197, 0.6925, 0.6857, 0.7197, 0.6231, 0.615, 0.6068, 0.6463, 0.5837, 0.6136, 0.6585, 0.6259, 0.6803, 0.6367, 0.6041, 0.6272, 0.6435, 0.6531, 0.6803, 0.6776, 0.6408, 0.615, 0.6707, 0.5864, 0.585, 0.6245, 0.6503, 0.6245, 0.6122, 0.6245, 0.6082, 0.6476, 0.6707, 0.6367, 0.5442, 0.5388, 0.634, 0.6367, 0.6449, 0.6585, 0.6626, 0.5918, 0.6694, 0.6707, 0.6435, 0.6, 0.6639, 0.668, 0.6367, 0.6517, 0.6367, 0.634, 0.6231, 0.5823, 0.6952, 0.5646, 0.6286, 0.6082, 0.634, 0.6735, 0.6395, 0.6422, 0.683, 0.6218, 0.6122, 0.6395, 0.6259, 0.6354, 0.649, 0.6857, 0.6286, 0.5483, 0.6068, 0.5905, 0.6599, 0.6327, 0.6585, 0.6313, 0.585, 0.5782, 0.5537, 0.6517, 0.6333, 0.6531, 0.5986, 0.5918, 0.6585, 0.6272, 0.6871, 0.6667, 0.6381, 0.6395, 0.634, 0.6531, 0.7034, 0.5633, 0.649, 0.668, 0.6667, 0.6231, 0.6599, 0.6803, 0.6503, 0.702, 0.6272, 0.649, 0.6721, 0.6163, 0.634, 0.6422, 0.6735, 0.717, 0.6667, 0.7306, 0.6367, 0.5224, 0.6408, 0.6639, 0.615, 0.619, 0.5361, 0.6408, 0.6231, 0.5918, 0.6463, 0.6204, 0.551, 0.6177, 0.6639, 0.6, 0.6898, 0.6626, 0.6463, 0.6163, 0.6544, 0.5973, 0.7197, 0.6721, 0.5986, 0.4531, 0.6204, 0.5932, 0.6408, 0.6612, 0.5905, 0.7184, 0.6054, 0.5864, 0.6585, 0.5687, 0.615, 0.6408, 0.5048, 0.6367, 0.5306, 0.649, 0.6517, 0.5088, 0.5891, 0.6054, 0.6871, 0.7061, 0.7184, 0.5673, 0.6476, 0.6667, 0.6122, 0.5673, 0.6231, 0.5429, 0.6626, 0.6925, 0.6327, 0.6653, 0.6218, 0.6952, 0.5918, 0.5633, 0.6912, 0.6095, 0.6626, 0.5973, 0.6299, 0.6599, 0.6585, 0.6259, 0.483, 0.6286, 0.6095, 0.7048, 0.6735, 0.5946, 0.6422, 0.6395, 0.6327, 0.6014, 0.6558, 0.6449, 0.6286, 0.6626, 0.6395, 0.6816, 0.6245, 0.6136, 0.5523809523809524, 0.6285714285714286, 0.5768707482993196, 0.6027210884353742, 0.5074829931972789, 0.5510204081632653, 0.545578231292517, 0.6027210884353742, 0.508843537414966, 0.5714285714285715]\n",
      "[0.6599, 0.7048, 0.7143, 0.6014, 0.7102, 0.7129, 0.6857, 0.6952, 0.7156, 0.6857, 0.698, 0.6939, 0.6912, 0.7211, 0.666, 0.6966, 0.6878, 0.6701, 0.7211, 0.6952, 0.6857, 0.6952, 0.7197, 0.702, 0.7388, 0.6966, 0.6898, 0.681, 0.7184, 0.7061, 0.6898, 0.7224, 0.7197, 0.7347, 0.6884, 0.6864, 0.7007, 0.7177, 0.6898, 0.717, 0.7048, 0.7306, 0.6905, 0.7116, 0.7007, 0.6939, 0.6714, 0.717, 0.7224, 0.7116, 0.6653, 0.7061, 0.7075, 0.7238, 0.7075, 0.6823, 0.7075, 0.7211, 0.6939, 0.6952, 0.7048, 0.7129, 0.717, 0.6714, 0.7048, 0.6313, 0.7034, 0.7211, 0.7129, 0.6755, 0.6939, 0.6769, 0.6857, 0.6952, 0.7048, 0.6864, 0.6741, 0.7007, 0.7034, 0.698, 0.6925, 0.7306, 0.6646, 0.6803, 0.7197, 0.7306, 0.7116, 0.685, 0.6218, 0.681, 0.6537, 0.7197, 0.6898, 0.6878, 0.6973, 0.702, 0.6993, 0.702, 0.6884, 0.6565, 0.5891, 0.5755, 0.5769, 0.581, 0.585, 0.4463, 0.5864, 0.5823, 0.5823, 0.5769, 0.5769, 0.434, 0.5891, 0.5891, 0.585, 0.5782, 0.5796, 0.5714, 0.5837, 0.5755, 0.5728, 0.5782, 0.5823, 0.4109, 0.581, 0.3986, 0.5823, 0.5891, 0.4068, 0.581, 0.4204, 0.581, 0.5837, 0.585, 0.5837, 0.5796, 0.5796, 0.5782, 0.4993, 0.5796, 0.4476, 0.5918, 0.3878, 0.4177, 0.5728, 0.5837, 0.5782, 0.5741, 0.4408, 0.566, 0.5728, 0.4735, 0.5837, 0.483, 0.5823, 0.5837, 0.3755, 0.3959, 0.5918, 0.5782, 0.317, 0.5755, 0.4531, 0.4503, 0.5769, 0.5796, 0.5823, 0.5701, 0.5769, 0.5796, 0.5728, 0.3769, 0.4558, 0.4449, 0.5837, 0.5891, 0.581, 0.5755, 0.4599, 0.4327, 0.5823, 0.5905, 0.5782, 0.4299, 0.5714, 0.5769, 0.5769, 0.4259, 0.5823, 0.415, 0.581, 0.5782, 0.585, 0.5823, 0.585, 0.5796, 0.5796, 0.3605, 0.419, 0.581, 0.6585, 0.6816, 0.6667, 0.6476, 0.6381, 0.6463, 0.6599, 0.6517, 0.6599, 0.6476, 0.6599, 0.6735, 0.668, 0.649, 0.6653, 0.6803, 0.6639, 0.6544, 0.6599, 0.6585, 0.6612, 0.6558, 0.6639, 0.6463, 0.6667, 0.6639, 0.6503, 0.6571, 0.6653, 0.649, 0.6748, 0.6571, 0.6639, 0.6463, 0.6259, 0.6449, 0.6381, 0.6707, 0.6707, 0.6571, 0.6721, 0.6694, 0.6476, 0.6571, 0.6803, 0.6571, 0.6667, 0.6286, 0.6626, 0.6639, 0.6571, 0.6639, 0.6286, 0.6626, 0.6503, 0.6694, 0.6531, 0.6585, 0.6259, 0.6653, 0.6735, 0.6327, 0.6599, 0.6612, 0.6558, 0.6272, 0.6667, 0.6599, 0.6544, 0.6435, 0.6639, 0.6503, 0.6707, 0.6381, 0.6789, 0.6517, 0.6517, 0.6585, 0.6599, 0.6585, 0.6544, 0.6735, 0.6639, 0.6558, 0.6531, 0.668, 0.6626, 0.6762, 0.6612, 0.6558, 0.6218, 0.6653, 0.668, 0.6136, 0.668, 0.6816, 0.6476, 0.6653, 0.6626, 0.6626, 0.6177, 0.619, 0.5905, 0.5633, 0.6027, 0.6327, 0.6054, 0.615, 0.5823, 0.6299, 0.581, 0.5946, 0.5864, 0.5673, 0.6041, 0.6558, 0.566, 0.6204, 0.5823, 0.6163, 0.6204, 0.6204, 0.585, 0.6095, 0.5878, 0.619, 0.566, 0.5551, 0.619, 0.6177, 0.6408, 0.5946, 0.585, 0.6367, 0.5782, 0.5932, 0.5361, 0.5918, 0.6027, 0.5891, 0.6122, 0.6231, 0.6667, 0.6272, 0.6313, 0.5891, 0.5551, 0.5878, 0.6177, 0.6177, 0.5898, 0.5986, 0.5973, 0.6, 0.5442, 0.5687, 0.5714, 0.5796, 0.585, 0.6, 0.6068, 0.5755, 0.6218, 0.5701, 0.5918, 0.5755, 0.5769, 0.5537, 0.6014, 0.6313, 0.5687, 0.5823, 0.5456, 0.6286, 0.6381, 0.6082, 0.5701, 0.5986, 0.5469, 0.6054, 0.5946, 0.5728, 0.619, 0.5878, 0.5701, 0.5605, 0.6231, 0.566, 0.6585, 0.5905, 0.6299, 0.5959, 0.5966, 0.5204, 0.5741, 0.5714, 0.5782, 0.5633, 0.5864, 0.6422, 0.3565, 0.3361, 0.2245, 0.3211, 0.2367, 0.2082, 0.3075, 0.4245, 0.102, 0.1102, 0.1279, 0.3673, 0.3673, 0.1197, 0.2558, 0.2082, 0.1592, 0.1932, 0.449, 0.1714, 0.3034, 0.3592, 0.4422, 0.4381, 0.2871, 0.332, 0.1034, 0.4898, 0.3075, 0.1796, 0.083, 0.2993, 0.4585, 0.4381, 0.0803, 0.0449, 0.3061, 0.1973, 0.3469, 0.2435, 0.3755, 0.166, 0.3333, 0.3483, 0.0707, 0.3918, 0.1497, 0.185, 0.2068, 0.4803, 0.1565, 0.2245, 0.2544, 0.3265, 0.3551, 0.3796, 0.3129, 0.2041, 0.2381, 0.2993, 0.366, 0.3782, 0.1347, 0.2068, 0.1973, 0.3374, 0.3184, 0.1347, 0.1891, 0.0993, 0.4435, 0.3687, 0.3048, 0.3932, 0.3401, 0.1429, 0.4122, 0.1088, 0.3755, 0.4068, 0.4395, 0.415, 0.3143, 0.283, 0.2272, 0.3361, 0.419, 0.2163, 0.1986, 0.4122, 0.2068, 0.4286, 0.3483, 0.4639, 0.483, 0.2884, 0.4272, 0.1796, 0.132, 0.0884, 0.5102040816326531, 0.5904761904761905, 0.6190476190476191, 0.5510204081632654, 0.6136054421768706, 0.5319727891156463, 0.5700680272108843, 0.6068027210884354, 0.5836734693877551, 0.5224489795918367]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "fnirs_cli_transformer_pth = 'results/gnn_transformer_with_cli_demo_v1/prognosis_mix_hb/pretreatment_response/ten-fold-evaluate/y_pred_test_all.npy'\n",
    "fnirs_transformer_pth = 'results/gnn_transformer/prognosis_mix_hb/pretreatment_response/ten-fold-evaluate/y_pred_test_all.npy'\n",
    "label_path = 'allData/prognosis_mix_hb/pretreatment_response/label.npy'\n",
    "\n",
    "def add_into_all_auc(all_auc, MMDR_path, label_path):\n",
    "    data = np.load(MMDR_path)\n",
    "    y_true = np.load(label_path)\n",
    "    for i in range(data.shape[0]):\n",
    "        y_pred = data[i,:,1]\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        all_auc.append(roc_auc)\n",
    "    print(all_auc)\n",
    "    return all_auc\n",
    "\n",
    "all_auc_fnirs = add_into_all_auc(all_auc_fnirs, fnirs_transformer_pth, label_path)\n",
    "all_auc_fnirs_cli = add_into_all_auc(all_auc_fnirs_cli, fnirs_cli_transformer_pth, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When comparing to chance\n",
      "Using fNIRS or fNIRS+Cli data(1.0015559486427974e-166, 'Reject the null hypothesis. Classifier performance is significantly better than chance (p-value: 0.0000).')\n",
      "Using Cli data(0.9822716809936585, 'Fail to reject the null hypothesis. No significant difference from chance (p-value: 0.9823).')\n",
      "When comparing fnirs and fnirs+cli data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.571190906607724e-109,\n",
       " 'Reject the null hypothesis. unimodal classifiers significantly outperform bimodal classifiers (p-value: 0.0000).')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "all_bacc_fnirs_cli = np.array(all_bacc_fnirs_cli).reshape(-1)\n",
    "all_bacc_cli = np.array(all_bacc_cli).reshape(-1)\n",
    "\n",
    "# all_auc_fnirs = np.array(all_auc_fnirs).reshape(-1)\n",
    "# all_auc_fnirs_cli = np.array(all_auc_fnirs_cli).reshape(-1)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "def test_classifier_performance(bAcc_scores, chance_level=0.5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a one-tailed binomial test to compare classifier performance to chance.\n",
    "\n",
    "    Parameters:\n",
    "    bAcc_scores (list or np.array): List or array of balanced accuracy scores of the classifiers.\n",
    "    chance_level (float): The chance level performance (default is 0.5 for balanced accuracy).\n",
    "    alpha (float): Significance level for the hypothesis test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    p_value (float): The p-value from the one-tailed binomial test.\n",
    "    result (str): The result of the hypothesis test.\n",
    "    \"\"\"\n",
    "    # Convert bAcc_scores to a numpy array if it's not already\n",
    "    bAcc_scores = np.array(bAcc_scores)\n",
    "    \n",
    "    # Calculate the number of successes (i.e., scores greater than chance_level)\n",
    "    successes = np.sum(bAcc_scores > chance_level)\n",
    "    \n",
    "    # Total number of trials\n",
    "    n = len(bAcc_scores)\n",
    "    \n",
    "    # Perform one-tailed binomial test\n",
    "    p_value = binom_test(successes, n, chance_level, alternative='greater')\n",
    "    \n",
    "    # Determine the result based on the p-value and significance level\n",
    "    if p_value < alpha:\n",
    "        result = f\"Reject the null hypothesis. Classifier performance is significantly better than chance (p-value: {p_value:.4f}).\"\n",
    "    else:\n",
    "        result = f\"Fail to reject the null hypothesis. No significant difference from chance (p-value: {p_value:.4f}).\"\n",
    "    \n",
    "    return p_value, result\n",
    "\n",
    "\n",
    "\n",
    "def test_unimodal_vs_bimodal(unimodal_scores, bimodal_scores, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a one-tailed sign test to compare unimodal and bimodal classifier performances.\n",
    "\n",
    "    Parameters:\n",
    "    unimodal_scores (list or np.array): List or array of AUC scores of the unimodal classifiers.\n",
    "    bimodal_scores (list or np.array): List or array of AUC scores of the bimodal classifiers.\n",
    "    alpha (float): Significance level for the hypothesis test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    p_value (float): The p-value from the one-tailed sign test.\n",
    "    result (str): The result of the hypothesis test.\n",
    "    \"\"\"\n",
    "    # Convert scores to numpy arrays if they are not already\n",
    "    unimodal_scores = np.array(unimodal_scores)\n",
    "    bimodal_scores = np.array(bimodal_scores)\n",
    "    \n",
    "    # Ensure the scores arrays have the same length\n",
    "    if unimodal_scores.shape[0] != bimodal_scores.shape[0]:\n",
    "        raise ValueError(\"The unimodal and bimodal scores must have the same length.\")\n",
    "    \n",
    "    # Calculate the number of successes (i.e., bimodal scores greater than unimodal scores)\n",
    "    successes = np.sum(unimodal_scores > bimodal_scores)\n",
    "    \n",
    "    # Total number of trials (excluding ties)\n",
    "    n = np.sum(bimodal_scores != unimodal_scores)\n",
    "    \n",
    "    # Perform one-tailed sign test using binomial test\n",
    "    p_value = binom_test(successes, n, 0.5, alternative='greater')\n",
    "    \n",
    "    # Determine the result based on the p-value and significance level\n",
    "    if p_value < alpha:\n",
    "        result = f\"Reject the null hypothesis. unimodal classifiers significantly outperform bimodal classifiers (p-value: {p_value:.4f}).\"\n",
    "    else:\n",
    "        result = f\"Fail to reject the null hypothesis. No significant difference in performance (p-value: {p_value:.4f}).\"\n",
    "    \n",
    "    return p_value, result\n",
    "\n",
    "\n",
    "print(\"When comparing to chance\")\n",
    "\n",
    "print('Using fNIRS or fNIRS+Cli data', end='')\n",
    "print(test_classifier_performance(all_bacc_fnirs_cli))\n",
    "\n",
    "print('Using Cli data', end='')\n",
    "print(test_classifier_performance(all_bacc_cli))\n",
    "\n",
    "\n",
    "\n",
    "print(\"When comparing fnirs and fnirs+cli data\")\n",
    "test_unimodal_vs_bimodal(all_auc_fnirs, all_auc_fnirs_cli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
