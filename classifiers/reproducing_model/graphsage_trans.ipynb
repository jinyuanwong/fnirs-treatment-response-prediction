{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanxiafeng/miniconda3/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/shanxiafeng/miniconda3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.14.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from sklearn.metrics import auc, accuracy_score, roc_curve, recall_score\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs -> (10, 5, 5, 4)\n",
      "Shape of inputs: (10, 5, 5, 4)\n",
      "Shape of aggregated representation: (10, 5, 5)\n",
      "tf.Tensor(\n",
      "[[[ 0.14682235  0.29236728  0.19625227  1.1891878   0.2711566 ]\n",
      "  [-1.3149323   0.12830108 -0.23348776 -0.21816854  0.02447005]\n",
      "  [-0.6903639   0.11053761  0.2368384   0.1969232  -0.00595765]\n",
      "  [ 0.5013736   0.3270502   0.6146605   0.11175621  0.09466723]\n",
      "  [ 0.06489319 -0.08328563  0.40416318  0.37804925  0.7366225 ]]\n",
      "\n",
      " [[-0.37081495 -0.21770597 -1.4158242   0.12318621  0.32984906]\n",
      "  [-0.616259    0.6813698   1.0692375   0.0424664   0.05229074]\n",
      "  [ 0.07170535  0.27779216 -0.27730748 -0.6043559  -0.06504062]\n",
      "  [-1.0289661   0.8543185   0.36709952 -0.6010849  -0.37804586]\n",
      "  [-0.40803087  0.09522653  0.20614402 -0.5239665   0.6885087 ]]\n",
      "\n",
      " [[-1.244364   -0.4229477  -0.610304   -0.12081103  1.1612086 ]\n",
      "  [ 0.24314263  1.4712534  -0.47533157  0.465911   -0.06506082]\n",
      "  [ 0.11330339 -1.2681531  -0.26203465  0.30726242  0.03871648]\n",
      "  [ 0.23435465  0.1849189  -0.2935584  -0.23248768 -0.60585463]\n",
      "  [ 0.27636713  1.9061202  -0.5252021  -0.3542207   1.2605257 ]]\n",
      "\n",
      " [[ 0.33815736 -0.14720151 -0.42365894  0.7412738   0.13258164]\n",
      "  [ 1.002974   -0.79691124 -0.55702186 -0.43751773  0.4145577 ]\n",
      "  [-0.80833983 -0.2399677   0.05434738 -3.1106677   0.41551483]\n",
      "  [-1.0052344   0.00410816 -0.31385255  0.14948416  1.2865078 ]\n",
      "  [-0.6318305   0.2649038  -0.9249021   0.3227105   0.50858617]]\n",
      "\n",
      " [[-0.34072775 -0.6153426   0.35508406 -1.4330075  -0.6845832 ]\n",
      "  [-1.5842935   0.8225569   0.3958818  -1.6502535   0.09615546]\n",
      "  [ 0.21264906 -0.28611088 -0.0492775   0.21263894  0.2595685 ]\n",
      "  [ 0.32486346  1.2097645  -0.16253594 -0.20289251  0.771827  ]\n",
      "  [-0.26038325  0.3070944   0.59681916 -0.3548667   0.1462556 ]]\n",
      "\n",
      " [[ 0.09225878  0.5448289  -1.02618    -1.0665115   0.60371333]\n",
      "  [-0.41593954  0.7426515   0.14259638 -0.09329006  1.0325688 ]\n",
      "  [ 0.36443934  0.24820185 -0.8081627  -0.33244455 -0.25220066]\n",
      "  [-0.22851288  0.7715824   0.5374149   0.27455026  0.81139326]\n",
      "  [-0.23663944 -0.49769813  0.01386862  0.4105007   0.01260418]]\n",
      "\n",
      " [[ 0.06054749  0.03915155 -0.41042554  0.06511688 -0.50764036]\n",
      "  [ 0.02773726  0.20057526  1.1883332   1.0190027  -0.29233956]\n",
      "  [-0.19339721 -0.05051752 -0.27963126 -0.01684425  0.59685963]\n",
      "  [ 0.59382176 -0.02850378 -0.07783423 -0.2656079   1.1199331 ]\n",
      "  [-0.2879695   0.82275534  0.25712112  0.09524442  0.8058112 ]]\n",
      "\n",
      " [[ 0.1420546   1.3485389  -0.10821401 -0.15238854  0.5771633 ]\n",
      "  [-0.10675877  0.31340572  0.9524572   0.3974125   0.3087595 ]\n",
      "  [-0.7828614   0.21112643  0.4038688  -0.6757629   0.2271523 ]\n",
      "  [ 0.31023225 -0.2669949  -0.06191912  0.06504562  0.01160344]\n",
      "  [-0.07775527  0.40126193 -1.235641   -0.8455491   0.65608346]]\n",
      "\n",
      " [[-1.1633686   0.4698712  -0.22016245 -0.77403593  1.4003298 ]\n",
      "  [-0.3528716  -1.2651345  -0.7408471   0.60032296 -0.64420587]\n",
      "  [-0.0174477  -0.3081843  -0.6442974  -0.37471467  0.32856166]\n",
      "  [ 0.31826398  0.43594134 -0.96292067 -0.01439601 -0.6302817 ]\n",
      "  [-0.284477    0.28978038  0.3689078  -0.18532982  1.5625235 ]]\n",
      "\n",
      " [[-0.93111134  0.6871474   0.81620085 -0.4881826   1.2077371 ]\n",
      "  [ 0.07438907  0.5919264   0.32343668  0.63077027  1.2465073 ]\n",
      "  [-0.65652114  0.5995431  -0.72515565  0.55326676  0.5410969 ]\n",
      "  [ 0.51947075  0.02565789 -0.44731975 -0.8122798   0.6848289 ]\n",
      "  [ 0.23617004  0.17127283 -0.92911863 -1.276137   -0.3334909 ]]], shape=(10, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiViewAggregator(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiViewAggregator, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Assuming input_shape is (subjects, node_seed, node_target, number_of_views)\n",
    "        self.num_views = input_shape[-1]\n",
    "        feat_dim = input_shape[-2]  # node_target dimension is considered as feature dimension\n",
    "\n",
    "        # This is the learnable weight vector b̅ mentioned in the formula (6)\n",
    "        self.weight_vector = self.add_weight(shape=(feat_dim,),\n",
    "                                             initializer='glorot_uniform',\n",
    "                                             name='weight_vector')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(f'inputs -> {inputs.shape}')\n",
    "        # inputs shape is (subjects, node_seed, node_target, number_of_views)\n",
    "        # Compute the dot product between weight vector and node_target features for each view\n",
    "        attention_scores = tf.tensordot(inputs, self.weight_vector, axes=[2, 0])\n",
    "        \n",
    "        # attention_scores shape is (subjects, node_seed, number_of_views)\n",
    "        # Apply softmax over the number_of_views dimension to get the attention coefficients\n",
    "        attention_coefficients = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # Expand dims of attention_coefficients for element-wise multiplication\n",
    "        attention_coefficients_expanded = tf.expand_dims(attention_coefficients, axis=2)\n",
    "\n",
    "        # Multiply the inputs with the attention coefficients and sum over the views\n",
    "        weighted_sum = tf.reduce_sum(inputs * attention_coefficients_expanded, axis=-1)\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output shape will be the input shape without the number_of_views dimension\n",
    "        return input_shape[:-1]\n",
    "\n",
    "# Test the MultiViewAggregator\n",
    "# Define the input shape parameters\n",
    "num_subjects = 10\n",
    "num_node_seed = 5\n",
    "num_node_target = 5\n",
    "num_views = 4\n",
    "\n",
    "# Generate random data to simulate the inputs\n",
    "inputs = tf.random.normal((num_subjects, num_node_seed, num_node_target, num_views))\n",
    "\n",
    "# Create the MultiViewAggregator layer\n",
    "aggregator = MultiViewAggregator()\n",
    "\n",
    "# Call the aggregator on the test inputs\n",
    "aggregated_representation = aggregator(inputs)\n",
    "\n",
    "# Print the shapes to verify the output\n",
    "print(f\"Shape of inputs: {inputs.shape}\")\n",
    "print(f\"Shape of aggregated representation: {aggregated_representation.shape}\")\n",
    "# Output the actual aggregated representation for inspection\n",
    "print(aggregated_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: (10, 52, 125)\n",
      "Shape of aggregated representation: (10, 52, 64)\n",
      "tf.Tensor(\n",
      "[[[ 0.          0.          6.3000364  ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 6.777787    0.          0.         ...  8.4236      0.\n",
      "    0.        ]\n",
      "  [17.504349    3.576948    0.         ...  0.         12.221742\n",
      "    7.5417676 ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ... 24.122377    0.\n",
      "    0.        ]\n",
      "  [11.16812     0.          1.3118283  ...  0.          2.1922848\n",
      "    0.        ]\n",
      "  [ 0.         12.110241    0.         ...  0.          4.3504896\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.55670196  4.906233    0.         ... 10.509783    7.5247827\n",
      "    0.        ]\n",
      "  [ 0.          0.          2.6273222  ...  0.          0.\n",
      "    6.6759257 ]\n",
      "  [ 0.          0.          6.5667315  ...  2.2529225  13.30647\n",
      "    0.03883433]\n",
      "  ...\n",
      "  [15.294917    1.71078     7.650405   ...  0.          0.\n",
      "   11.589546  ]\n",
      "  [ 0.          5.3014593   0.         ...  0.18327388  4.9906635\n",
      "    7.1529317 ]\n",
      "  [ 4.3943405   1.6992395   1.1199014  ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 2.055426   13.4917345   8.117723   ...  0.          4.415367\n",
      "    0.        ]\n",
      "  [ 1.6527251   3.4283345   3.391984   ...  0.          5.8416038\n",
      "    1.0158782 ]\n",
      "  [ 4.1677656   0.7988568   4.2865167  ...  9.929538    5.556097\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 5.9896855   0.          0.         ...  6.015866    0.\n",
      "    7.810473  ]\n",
      "  [ 0.          0.          0.         ...  8.554801    0.\n",
      "    1.8562108 ]\n",
      "  [ 0.          0.         15.705234   ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.          0.          0.         ...  8.71621    10.563864\n",
      "   11.257543  ]\n",
      "  [ 2.2897408   0.          4.416362   ...  0.34013623  0.11730579\n",
      "    4.67955   ]\n",
      "  [ 0.          0.          6.598947   ...  0.         11.706344\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.         18.612253   ...  2.8587737   4.091224\n",
      "    0.        ]\n",
      "  [10.691098    0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          5.237633   ...  2.4268956  11.3516655\n",
      "   12.956443  ]]\n",
      "\n",
      " [[ 1.8564687   0.          4.359846   ...  1.9584459   0.\n",
      "    8.984634  ]\n",
      "  [ 0.          6.7666197   1.1462244  ...  0.          9.637179\n",
      "   13.569389  ]\n",
      "  [ 0.          0.53520364 10.17868    ...  0.          0.81895065\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 7.4917607   6.0355635   0.         ...  0.          0.\n",
      "    0.45335373]\n",
      "  [ 0.          2.472293    4.3004546  ...  0.25990152  0.\n",
      "    0.        ]\n",
      "  [ 6.632763   12.780105    4.8331213  ... 18.990816    1.6532673\n",
      "    6.7336    ]]\n",
      "\n",
      " [[ 0.6104968   0.          0.         ...  0.          9.328175\n",
      "    3.9767869 ]\n",
      "  [ 0.          1.2647882   6.6269197  ...  0.4567479   0.\n",
      "    0.        ]\n",
      "  [ 2.3747878   0.          6.0692067  ...  7.112987    0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          3.9733274   2.9463155  ...  9.615098    4.2926564\n",
      "    0.        ]\n",
      "  [ 2.1673057   0.6357359   9.414056   ... 11.756129    0.\n",
      "    0.        ]\n",
      "  [ 0.          0.         14.814769   ...  3.604872    9.501433\n",
      "    0.        ]]], shape=(10, 52, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "        \n",
    "class GraphSAGE(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 feature_dim, embed_dim,\n",
    "                 activation='relu'):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = tf.keras.layers.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = tf.keras.layers.Activation('sigmoid')\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tf.keras.layers.Activation('tanh')\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = tf.keras.layers.Activation('prelu')\n",
    "        else:\n",
    "            raise ValueError('Provide a valid activation for GNN')\n",
    "        \n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.feat_dim = feature_dim\n",
    "        self.weight = self.add_weight(shape=(embed_dim, self.feat_dim),\n",
    "                                       initializer=initializers.GlorotUniform(),\n",
    "                                       trainable=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    This normalization exists some problem to generate NaN value. Please amend. 23 Nov 2023 by JY\n",
    "    \"\"\"\n",
    "    def normalize_adjacency(self, adj):\n",
    "        d = tf.reduce_sum(adj, axis=-1)\n",
    "        d_sqrt_inv = tf.pow(d, -0.5)\n",
    "        d_sqrt_inv = tf.where(tf.math.is_inf(d_sqrt_inv), 0., d_sqrt_inv)\n",
    "        d_mat_inv_sqrt = tf.linalg.diag(d_sqrt_inv)\n",
    "        return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        # adj_normalized = self.normalize_adjacency(adj)\n",
    "        # print(adj_normalized)\n",
    "        combined = tf.linalg.matmul(adj, inputs)\n",
    "        outputs = tf.tensordot(combined, self.weight, axes=[[2], [1]])\n",
    "        \"\"\"\n",
    "        combined = tf.random.normal((40,52,125))\n",
    "        weight = tf.random.normal((64,125))\n",
    "        combined_transformed = tf.tensordot(combined, weight, axes=[[2], [1]])\n",
    "        print(combined_transformed.shape) # (40, 52, 64)\n",
    "        \"\"\"\n",
    "        return self.activation(outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Test the MultiViewAggregator\n",
    "# Define the input shape parameters\n",
    "num_subjects = 10\n",
    "num_node_seed = 5\n",
    "num_node_target = 5\n",
    "num_views = 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_dim = 125\n",
    "channel = 52\n",
    "embed_dim = 64 \n",
    "\n",
    "input = tf.random.normal((num_subjects, channel, feature_dim))\n",
    "# Generate random data to simulate the inputs\n",
    "adj = tf.random.normal((num_subjects, channel, channel))\n",
    "\n",
    "# Create the MultiViewAggregator layer\n",
    "aggregator = GraphSAGE(feature_dim=feature_dim, embed_dim=embed_dim)\n",
    "\n",
    "# Call the aggregator on the test inputs\n",
    "aggregated_representation = aggregator(input, adj)\n",
    "\n",
    "# Print the shapes to verify the output\n",
    "print(f\"Shape of inputs: {input.shape}\")\n",
    "print(f\"Shape of aggregated representation: {aggregated_representation.shape}\")\n",
    "# Output the actual aggregated representation for inspection\n",
    "print(aggregated_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, None, 52, 64)\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)       [(None, 52, 250)]            0         []                            \n",
      "                                                                                                  \n",
      " input_50 (InputLayer)       [(None, 52, 52)]             0         []                            \n",
      "                                                                                                  \n",
      " graph_sage_80 (GraphSAGE)   (None, 52, 64)               16000     ['input_49[0][0]',            \n",
      "                                                                     'input_50[0][0]']            \n",
      "                                                                                                  \n",
      " graph_sage_81 (GraphSAGE)   (None, 52, 64)               4096      ['graph_sage_80[0][0]',       \n",
      "                                                                     'input_50[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_5 (Transformer  (None, 52, 64)               199936    ['graph_sage_81[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " graph_sage_82 (GraphSAGE)   (None, 52, 64)               4096      ['transformer_5[0][0]',       \n",
      "                                                                     'input_50[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_6 (Transformer  (None, 52, 64)               199936    ['graph_sage_82[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_41 (TFOpLam  (1, None, 52, 64)            0         ['graph_sage_80[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.expand_dims_42 (TFOpLam  (1, None, 52, 64)            0         ['transformer_5[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.expand_dims_43 (TFOpLam  (1, None, 52, 64)            0         ['transformer_6[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_12 (TFOpLambda)   (3, None, 52, 64)            0         ['tf.expand_dims_41[0][0]',   \n",
      "                                                                     'tf.expand_dims_42[0][0]',   \n",
      "                                                                     'tf.expand_dims_43[0][0]']   \n",
      "                                                                                                  \n",
      " tf.reshape_9 (TFOpLambda)   (3, None, 3328)              0         ['tf.concat_12[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)               (3, None, 64)                868608    ['tf.reshape_9[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFO  (None, 64)                   0         ['lstm_9[0][0]']              \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 64)                   128       ['tf.math.reduce_mean_3[0][0]'\n",
      " erNormalization)                                                   ]                             \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 256)                  16640     ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 128)                  32896     ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 64)                   8256      ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 2)                    130       ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1350722 (5.15 MB)\n",
      "Trainable params: 1350722 (5.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 7s 613ms/step - loss: 1.0469 - accuracy: 0.4943 - val_loss: 0.9249 - val_accuracy: 0.4600\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 0.8238 - accuracy: 0.4943 - val_loss: 0.7317 - val_accuracy: 0.5400\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.7302 - accuracy: 0.5114 - val_loss: 0.7302 - val_accuracy: 0.4600\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 285ms/step - loss: 0.7288 - accuracy: 0.5000 - val_loss: 0.7228 - val_accuracy: 0.5400\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.7259 - accuracy: 0.5286 - val_loss: 0.7198 - val_accuracy: 0.5800\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 1s 301ms/step - loss: 0.7271 - accuracy: 0.4886 - val_loss: 0.7193 - val_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 0.7150 - accuracy: 0.5743 - val_loss: 0.7253 - val_accuracy: 0.5400\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 0.7172 - accuracy: 0.5743 - val_loss: 0.7238 - val_accuracy: 0.5400\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.7168 - accuracy: 0.5886 - val_loss: 0.7181 - val_accuracy: 0.5800\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 0.7038 - accuracy: 0.6000 - val_loss: 0.7160 - val_accuracy: 0.5800\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.7017 - accuracy: 0.6286 - val_loss: 0.7116 - val_accuracy: 0.5800\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.6855 - accuracy: 0.6429 - val_loss: 0.7191 - val_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 1s 282ms/step - loss: 0.6944 - accuracy: 0.6200 - val_loss: 0.7294 - val_accuracy: 0.5600\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 1s 289ms/step - loss: 0.6883 - accuracy: 0.6286 - val_loss: 0.7361 - val_accuracy: 0.5600\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 0.6808 - accuracy: 0.6343 - val_loss: 0.7063 - val_accuracy: 0.6200\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 1s 275ms/step - loss: 0.6693 - accuracy: 0.6600 - val_loss: 0.7126 - val_accuracy: 0.6200\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 0.6725 - accuracy: 0.6571 - val_loss: 0.7102 - val_accuracy: 0.6600\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.6756 - accuracy: 0.6343 - val_loss: 0.7640 - val_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.7080 - accuracy: 0.6286 - val_loss: 0.7236 - val_accuracy: 0.6000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 1s 273ms/step - loss: 0.6980 - accuracy: 0.5314 - val_loss: 0.6992 - val_accuracy: 0.6400\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 1s 273ms/step - loss: 0.6627 - accuracy: 0.6543 - val_loss: 0.6820 - val_accuracy: 0.6400\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 1s 286ms/step - loss: 0.6701 - accuracy: 0.6457 - val_loss: 0.6702 - val_accuracy: 0.6200\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.6699 - accuracy: 0.6343 - val_loss: 0.6886 - val_accuracy: 0.6000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 1s 287ms/step - loss: 0.6725 - accuracy: 0.6514 - val_loss: 0.6887 - val_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 0.6891 - accuracy: 0.6429 - val_loss: 0.6955 - val_accuracy: 0.5800\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 1s 279ms/step - loss: 0.6794 - accuracy: 0.6371 - val_loss: 0.7015 - val_accuracy: 0.6000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.6737 - accuracy: 0.6429 - val_loss: 0.6862 - val_accuracy: 0.6200\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.6576 - accuracy: 0.6543 - val_loss: 0.6915 - val_accuracy: 0.6200\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.6560 - accuracy: 0.6629 - val_loss: 0.7021 - val_accuracy: 0.5800\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 0.6630 - accuracy: 0.6457 - val_loss: 0.7272 - val_accuracy: 0.5600\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 1s 271ms/step - loss: 0.6780 - accuracy: 0.6343 - val_loss: 0.7077 - val_accuracy: 0.6400\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.6638 - accuracy: 0.6457 - val_loss: 0.6945 - val_accuracy: 0.6200\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 1s 290ms/step - loss: 0.6619 - accuracy: 0.6371 - val_loss: 0.6848 - val_accuracy: 0.6200\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.6627 - accuracy: 0.6457 - val_loss: 0.6723 - val_accuracy: 0.6200\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 1s 290ms/step - loss: 0.6567 - accuracy: 0.6486 - val_loss: 0.6806 - val_accuracy: 0.6000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.6494 - accuracy: 0.6514 - val_loss: 0.6930 - val_accuracy: 0.5800\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 1s 285ms/step - loss: 0.6504 - accuracy: 0.6629 - val_loss: 0.6941 - val_accuracy: 0.6000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 1s 297ms/step - loss: 0.6441 - accuracy: 0.6543 - val_loss: 0.7106 - val_accuracy: 0.6000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 0.6634 - accuracy: 0.6400 - val_loss: 0.7236 - val_accuracy: 0.5800\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.6732 - accuracy: 0.6086 - val_loss: 0.7150 - val_accuracy: 0.5600\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 1s 283ms/step - loss: 0.6661 - accuracy: 0.6286 - val_loss: 0.7154 - val_accuracy: 0.5400\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.6831 - accuracy: 0.5800 - val_loss: 0.6985 - val_accuracy: 0.5600\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 1s 324ms/step - loss: 0.6817 - accuracy: 0.6057 - val_loss: 0.6963 - val_accuracy: 0.5800\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 1s 319ms/step - loss: 0.6560 - accuracy: 0.6371 - val_loss: 0.7063 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 1s 301ms/step - loss: 0.6610 - accuracy: 0.6486 - val_loss: 0.6985 - val_accuracy: 0.6000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 1s 288ms/step - loss: 0.6582 - accuracy: 0.6543 - val_loss: 0.7228 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.6568 - accuracy: 0.6714 - val_loss: 0.6914 - val_accuracy: 0.5800\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.6555 - accuracy: 0.6514 - val_loss: 0.7156 - val_accuracy: 0.5600\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 1s 296ms/step - loss: 0.6472 - accuracy: 0.6600 - val_loss: 0.7272 - val_accuracy: 0.5800\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.6504 - accuracy: 0.6629 - val_loss: 0.7171 - val_accuracy: 0.5800\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.6457 - accuracy: 0.6571 - val_loss: 0.7025 - val_accuracy: 0.5800\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.6466 - accuracy: 0.6657 - val_loss: 0.7032 - val_accuracy: 0.5600\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 1s 293ms/step - loss: 0.6550 - accuracy: 0.6657 - val_loss: 0.6878 - val_accuracy: 0.5800\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 1s 272ms/step - loss: 0.6398 - accuracy: 0.6657 - val_loss: 0.7015 - val_accuracy: 0.5800\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.6471 - accuracy: 0.6629 - val_loss: 0.6907 - val_accuracy: 0.6200\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.6452 - accuracy: 0.6514 - val_loss: 0.6676 - val_accuracy: 0.5800\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.6400 - accuracy: 0.6571 - val_loss: 0.6786 - val_accuracy: 0.6000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.6468 - accuracy: 0.6600 - val_loss: 0.6985 - val_accuracy: 0.6000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 0.6442 - accuracy: 0.6286 - val_loss: 0.8247 - val_accuracy: 0.5400\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.7698 - accuracy: 0.5714 - val_loss: 0.7194 - val_accuracy: 0.5400\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 1s 289ms/step - loss: 0.7132 - accuracy: 0.5686 - val_loss: 0.7008 - val_accuracy: 0.6200\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 1s 309ms/step - loss: 0.6944 - accuracy: 0.5571 - val_loss: 0.7030 - val_accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.6704 - accuracy: 0.6143 - val_loss: 0.6867 - val_accuracy: 0.5800\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 1s 287ms/step - loss: 0.6703 - accuracy: 0.6543 - val_loss: 0.7010 - val_accuracy: 0.5800\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.6525 - accuracy: 0.6457 - val_loss: 0.6913 - val_accuracy: 0.5800\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.6453 - accuracy: 0.6571 - val_loss: 0.6962 - val_accuracy: 0.6200\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.6607 - accuracy: 0.6486 - val_loss: 0.7123 - val_accuracy: 0.6200\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 1s 290ms/step - loss: 0.6614 - accuracy: 0.6514 - val_loss: 0.7217 - val_accuracy: 0.6000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.6678 - accuracy: 0.6343 - val_loss: 0.7164 - val_accuracy: 0.6000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.6647 - accuracy: 0.6343 - val_loss: 0.7162 - val_accuracy: 0.5800\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 0.6676 - accuracy: 0.6257 - val_loss: 0.7142 - val_accuracy: 0.6000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 1s 294ms/step - loss: 0.6627 - accuracy: 0.6371 - val_loss: 0.7068 - val_accuracy: 0.6000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.6607 - accuracy: 0.6371 - val_loss: 0.7027 - val_accuracy: 0.5600\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 1s 301ms/step - loss: 0.6605 - accuracy: 0.6429 - val_loss: 0.7093 - val_accuracy: 0.5800\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.6676 - accuracy: 0.6400 - val_loss: 0.7179 - val_accuracy: 0.5800\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.6756 - accuracy: 0.6114 - val_loss: 0.7105 - val_accuracy: 0.5800\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.6805 - accuracy: 0.6229 - val_loss: 0.6843 - val_accuracy: 0.6400\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.6699 - accuracy: 0.6486 - val_loss: 0.6902 - val_accuracy: 0.6200\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 1s 294ms/step - loss: 0.6750 - accuracy: 0.6286 - val_loss: 0.6806 - val_accuracy: 0.6200\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.6693 - accuracy: 0.6343 - val_loss: 0.6886 - val_accuracy: 0.6000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 1s 321ms/step - loss: 0.6730 - accuracy: 0.6257 - val_loss: 0.7082 - val_accuracy: 0.5400\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 1s 286ms/step - loss: 0.6739 - accuracy: 0.6400 - val_loss: 0.6807 - val_accuracy: 0.6800\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 0.6711 - accuracy: 0.6286 - val_loss: 0.6739 - val_accuracy: 0.5800\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 1s 296ms/step - loss: 0.6692 - accuracy: 0.6286 - val_loss: 0.6764 - val_accuracy: 0.6000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.6661 - accuracy: 0.6029 - val_loss: 0.6719 - val_accuracy: 0.6800\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.6685 - accuracy: 0.6143 - val_loss: 0.6823 - val_accuracy: 0.6600\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 1s 298ms/step - loss: 0.6648 - accuracy: 0.6143 - val_loss: 0.6890 - val_accuracy: 0.5400\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 1s 305ms/step - loss: 0.6745 - accuracy: 0.6200 - val_loss: 0.6863 - val_accuracy: 0.6000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.6628 - accuracy: 0.6314 - val_loss: 0.7105 - val_accuracy: 0.5800\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.6502 - accuracy: 0.6514 - val_loss: 0.6830 - val_accuracy: 0.6200\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.6518 - accuracy: 0.6343 - val_loss: 0.7041 - val_accuracy: 0.5800\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.6484 - accuracy: 0.6457 - val_loss: 0.6956 - val_accuracy: 0.6000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 0.6919 - accuracy: 0.6429 - val_loss: 0.7159 - val_accuracy: 0.5600\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 1s 288ms/step - loss: 0.6809 - accuracy: 0.6143 - val_loss: 0.6990 - val_accuracy: 0.5400\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.6871 - accuracy: 0.5943 - val_loss: 0.6973 - val_accuracy: 0.5400\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.6887 - accuracy: 0.6200 - val_loss: 0.6964 - val_accuracy: 0.5800\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.6862 - accuracy: 0.6314 - val_loss: 0.6947 - val_accuracy: 0.6000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 1s 320ms/step - loss: 0.6781 - accuracy: 0.6371 - val_loss: 0.6929 - val_accuracy: 0.6000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.6764 - accuracy: 0.6457 - val_loss: 0.6909 - val_accuracy: 0.5800\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.6730 - accuracy: 0.6286 - val_loss: 0.6927 - val_accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "class GraphSAGE(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 feature_dim, embed_dim,\n",
    "                 activation='relu'):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = tf.keras.layers.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = tf.keras.layers.Activation('sigmoid')\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tf.keras.layers.Activation('tanh')\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = tf.keras.layers.Activation('prelu')\n",
    "        else:\n",
    "            raise ValueError('Provide a valid activation for GNN')\n",
    "        \n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.feat_dim = feature_dim\n",
    "        self.weight = self.add_weight(shape=(embed_dim, self.feat_dim),\n",
    "                                       initializer=initializers.GlorotUniform(),\n",
    "                                       trainable=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    This normalization exists some problem to generate NaN value. Please amend. 23 Nov 2023 by JY\n",
    "    \"\"\"\n",
    "    def normalize_adjacency(self, adj):\n",
    "        d = tf.reduce_sum(adj, axis=-1)\n",
    "        d_sqrt_inv = tf.pow(d, -0.5)\n",
    "        d_sqrt_inv = tf.where(tf.math.is_inf(d_sqrt_inv), 0., d_sqrt_inv)\n",
    "        d_mat_inv_sqrt = tf.linalg.diag(d_sqrt_inv)\n",
    "        return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        # adj_normalized = self.normalize_adjacency(adj)\n",
    "        # print(adj_normalized)\n",
    "        combined = tf.linalg.matmul(adj, inputs)\n",
    "        outputs = tf.tensordot(combined, self.weight, axes=[[2], [1]])\n",
    "        \"\"\"\n",
    "        combined = tf.random.normal((40,52,125))\n",
    "        weight = tf.random.normal((64,125))\n",
    "        combined_transformed = tf.tensordot(combined, weight, axes=[[2], [1]])\n",
    "        print(combined_transformed.shape) # (40, 52, 64)\n",
    "        \"\"\"\n",
    "        return self.activation(outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class Classifier_GraphSAGE_Transformer():\n",
    "    def __init__(self, output_directory, callbacks, input_shape, epochs, sweep_config, info):\n",
    "        # input_shape = (200, 52, 128, 1)\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # 随机给定超参数进行训练\n",
    "        self.info = info\n",
    "        self.batch_size = sweep_config['batch_size'] if sweep_config else 128\n",
    "        d_model = 64  # 125# # random.choice([64, 128, 256])\n",
    "        kernel_size_1 = (4, 5)  # 2, 3, 4\n",
    "        stride_size_1 = (1, 2)\n",
    "        kernel_size_2 = (1, 5)  # 2: random.randint(2,8)  (2,5 are the best)\n",
    "        stride_size_2 = (1, 2)\n",
    "        kernel_size = [kernel_size_1, kernel_size_2]\n",
    "        stride_size = [stride_size_1, stride_size_2]\n",
    "        adam_beta_1, adam_beta_2 = 0.9, 0.999\n",
    "        num_class = 2  # 2\n",
    "        learning_rate = 0.01\n",
    "        num_of_view = 5 \n",
    "        depth = 3\n",
    "        feature_dim = 250\n",
    "        channel = 52\n",
    "        num_of_last_dense=3\n",
    "        FFN_units = 256\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate,\n",
    "                                              beta_1=adam_beta_1,\n",
    "                                              beta_2=adam_beta_2,\n",
    "                                              epsilon=1e-9)\n",
    "\n",
    "        # If you change these two hyperparameters, remember to change the  self.hyperparameters\n",
    "        inputs = tf.keras.Input(shape=input_shape[1:])\n",
    "        input_adj = tf.keras.Input(shape=(input_shape[1], input_shape[1]))\n",
    "        \n",
    "        conc_outputs = []\n",
    "        outputs = GraphSAGE(feature_dim=feature_dim, embed_dim=d_model)(inputs, input_adj)\n",
    "        conc_outputs.append(tf.expand_dims(outputs, axis=0))\n",
    "\n",
    "        if depth > 1: \n",
    "            for k in range(depth-1):\n",
    "                outputs = GraphSAGE(feature_dim=d_model, embed_dim=d_model)(outputs, input_adj)\n",
    "                # outputs = ClsPositionEncodingLayer(\n",
    "                #     input_channel=input_shape[1], kenerl_size=kernel_size[0], strides=stride_size[0], d_model=d_model, dropout_rate=0.5, name=f'CLS_pos_encoding_{depth}')(outputs)\n",
    "                outputs = Transformer(0.01,\n",
    "                                4,\n",
    "                                FFN_units,\n",
    "                                4,\n",
    "                                'relu')(outputs)\n",
    "                conc_outputs.append(tf.expand_dims(outputs, axis=0))\n",
    "        outputs = tf.concat(conc_outputs, axis=0)\n",
    "        print(outputs.shape)\n",
    "        outputs = tf.reshape(outputs, (depth, -1, d_model * channel))\n",
    "        outputs = layers.LSTM(units=d_model, return_sequences=True, return_state=False)(outputs)\n",
    "        outputs = tf.math.reduce_mean(outputs, axis=0)\n",
    "        outputs = layers.LayerNormalization(epsilon=1e-6)(outputs)\n",
    "\n",
    "        \"Doing this in here is to get the layer[-2] feature\"\n",
    "        for i in range(num_of_last_dense):\n",
    "            outputs = layers.Dense(FFN_units/(2**i),\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(0.0001))(outputs)\n",
    "        outputs = layers.Dense(num_class, activation='softmax')(outputs)\n",
    "                \n",
    "        # if depth > 1: \n",
    "        #     for k in range(depth-1):\n",
    "        #         outputs = GraphSAGE(feature_dim=d_model, embed_dim=d_model)(outputs, input_adj)\n",
    "        #         conc_outputs.append(tf.expand_dims(outputs, axis=0))\n",
    "        # outputs = tf.concat(conc_outputs, axis=0)\n",
    "        # print(outputs.shape)\n",
    "        # outputs = tf.reshape(outputs, (depth, -1, embed_dim * channel))\n",
    "        # outputs = layers.LSTM(units=d_model, return_sequences=True, return_state=False)(outputs)\n",
    "        # outputs = tf.math.reduce_mean(outputs, axis=0)\n",
    "        # print(outputs.shape)\n",
    "        # # outputs = layers.Dense(units=d_model)(outputs)\n",
    "        # # outputs = layers.GlobalAveragePooling1D(\n",
    "        # #     data_format='channels_first', keepdims=False)(outputs)\n",
    "        # outputs = layers.Dense(num_class, activation='softmax')(outputs)\n",
    "        model = tf.keras.Model(inputs=[inputs, input_adj], outputs=outputs)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_val, Y_val, X_test, Y_test, adj_train, adj_val, adj_test):\n",
    "        hist = self.model.fit(\n",
    "            x=[X_train, adj_train],\n",
    "            y=Y_train,\n",
    "            validation_data=([X_val, adj_val], Y_val),\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            verbose=True,\n",
    "            shuffle=True  # Set shuffle to True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# hbo_data = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/HbO-All-HC-MDD/correct_channel_data.npy')\n",
    "hbo_data = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/HbO-All-HC-MDD/correct_channel_data.npy')\n",
    "\n",
    "hbo_data = np.transpose(hbo_data, (0,2,1))\n",
    "# hbr_data = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/HbR-All-HC-MDD/correct_channel_data.npy')\n",
    "hbr_data = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/HbR-All-HC-MDD/correct_channel_data.npy')\n",
    "\n",
    "hbr_data = np.transpose(hbr_data, (0,2,1))\n",
    "labels = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/HbO-All-HC-MDD/label.npy')\n",
    "\n",
    "def normalize(data):\n",
    "    # Iterate over each subject\n",
    "    normalized_data = np.empty_like(data)\n",
    "    # Calculate the mean and standard deviation for the current subject\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    # Perform z-normalization for the current subject\n",
    "    normalized_data= (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Concatenate the HbO: (, , 125); HbR: (, , 125) -> Con_HbO_HbR: (, , 250)\n",
    "hb_input = normalize(np.concatenate((hbo_data,hbr_data),axis=2))\n",
    "connectivity = np.load('/Users/shanxiafeng/Documents/Code/python/fnirs_DL/JinyuanWang_pythonCode/allData/Output_npy/twoDoctor/Hb-All-HC-MDD/adj_matrix.npy') \n",
    "adj = connectivity[...,0]\n",
    "\n",
    "classifer = Classifier_GraphSAGE_Transformer(None, None, hb_input.shape, epochs=epochs, sweep_config=None, info=None)\n",
    "\n",
    "def onehotEncode(x):\n",
    "    t = np.zeros((x.size, x.max()+1))\n",
    "    t[np.arange(x.size), x] = 1\n",
    "    return t.astype(int)\n",
    "onehot_labels = onehotEncode(labels)\n",
    "\n",
    "X_train, Y_train = hb_input[:350], onehot_labels[:350]\n",
    "X_val, Y_val = hb_input[350:400], onehot_labels[350:400]\n",
    "X_test, Y_test = hb_input[400:], onehot_labels[400:]\n",
    "adj_train, adj_val, adj_test = adj[:350], adj[350:400], adj[400:]\n",
    "\n",
    "classifer.fit(X_train, Y_train, X_val, Y_val, X_test, Y_test, adj_train, adj_val, adj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 52, 250)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_fot_product_attention(queries, keys, values):\n",
    "\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    key_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(key_dim)\n",
    "\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "\n",
    "    def __init__(self, n_heads, name='multi_head_attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "    def split_proj(self, inputs, batch_size):  # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.n_heads,\n",
    "                 self.d_head)\n",
    "\n",
    "        # outputs: (batch_size, seq_length, nb_proj, d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape)\n",
    "        # outputs: (batch_size, nb_proj, seq_length,  d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, queries, keys, values):\n",
    "\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "\n",
    "        attention = scaled_fot_product_attention(queries, keys, values)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):  # pos: (seq_length, 1) i: (1, d_model)\n",
    "        # 2*(i//2) => if i = 5 -> ans = 4\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles  # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # input shape batch_size, seq_length, d_model\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        # Calculate the angles given the input\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        # Calculate the positional encodings\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        # Expand the encodings with a new dimension\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FFN_units, n_heads, dropout_rate, activation, name='encoder_layer'):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        self.FFN_units = FFN_units\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn1_relu_gelu = layers.Dense(\n",
    "            units=self.FFN_units, activation=self.activation)\n",
    "        self.ffn2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention = self.multi_head_attention(inputs, inputs, inputs)\n",
    "        attention = self.dropout_1(attention)\n",
    "        attention = self.norm_1(attention+inputs)\n",
    "\n",
    "        outputs = self.ffn1_relu_gelu(attention)\n",
    "        outputs = self.ffn2(outputs)\n",
    "        outputs = self.dropout_2(outputs)\n",
    "\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class EmbeddingLayer(layers.Layer):\n",
    "    def __init__(self, d_model, filters, kernel_size, strides, l2_rate, name=\"EmbeddingLayer\"):\n",
    "        super(EmbeddingLayer, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride_size = strides\n",
    "        self.d_model = d_model\n",
    "        self.l2_rate = l2_rate\n",
    "    # Why we must have a input_shape but we can not use it, or it will have issues.\n",
    "\n",
    "    def build(self, input_shape):  # input_shape : batch, channel_dimension, sample_points, HbO/HbR(1,2)\n",
    "        self.cnn_1 = layers.Conv2D(filters=self.filters,\n",
    "                                   kernel_size=self.kernel_size,\n",
    "                                   strides=self.stride_size)\n",
    "\n",
    "        # self.size_1 = (input_shape[1] - self.kernel_size[0]) // self.stride_size[0] + 1\n",
    "        # print(f'here size_1 = {self.size_1}')\n",
    "        self.out_dimension = (\n",
    "            input_shape[2] - self.kernel_size[1]) // self.stride_size[1] + 1  # {(𝑛 + 2𝑝 − 𝑓 + 1) / 𝑠} + 1 |n=len, p=padding, f=kernel, s=stride ;\n",
    "\n",
    "        # check_shape = (None, 52, x * y) # using the einsum can be more elegant\n",
    "        # equal to layers.Reshape((-1, self.out_dimension * self.filters)) , batch_size is ignored\n",
    "        self.flatten = layers.Reshape((-1, self.out_dimension * self.filters))\n",
    "        self.lin = layers.Dense(\n",
    "            self.d_model, kernel_regularizer=tf.keras.regularizers.l2(self.l2_rate))\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.cnn_1(inputs)\n",
    "\n",
    "        # # check here updated at 14 July 2023 by adding the transpose operation if you can not have good result from this time,\n",
    "        # outputs = tf.transpose(outputs, perm=[0, 2, 1, 3]) this step change the dimension of channel and sample point, which is not a good choice because you will get (None, 128, channel * output_channel_of_CNN), you lose the comparison of different channel.\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.lin(outputs)\n",
    "        outputs = self.norm(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 activation,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(\n",
    "            FFN_units, n_heads, dropout_rate, activation) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        for i in range(self.n_layers):\n",
    "            outputs = self.enc_layers[i](outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ClsPositionEncodingLayer(layers.Layer):\n",
    "    def __init__(self, input_channel, kenerl_size, strides, d_model, dropout_rate, name=\"ClsPositionEncodingLayer\"):\n",
    "        super(ClsPositionEncodingLayer, self).__init__(name=name)\n",
    "\n",
    "        patch = (input_channel - kenerl_size[0]) // strides[0] + 1\n",
    "        self.cls_token_patch = tf.Variable(tf.random.normal((1, 1, d_model)))\n",
    "        self.pos_embedding = PositionalEncoding()\n",
    "        self.dropout_patch = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cls_token_patch_tiled = tf.tile(\n",
    "            self.cls_token_patch, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "        outputs = tf.concat([cls_token_patch_tiled, inputs], axis=1)\n",
    "        outputs = self.pos_embedding(outputs)\n",
    "        outputs = self.dropout_patch(outputs)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "        \n",
    "class GraphSAGE(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 feature_dim, embed_dim,\n",
    "                 activation='relu'):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = tf.keras.layers.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = tf.keras.layers.Activation('sigmoid')\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tf.keras.layers.Activation('tanh')\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = tf.keras.layers.Activation('prelu')\n",
    "        else:\n",
    "            raise ValueError('Provide a valid activation for GNN')\n",
    "        \n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.feat_dim = feature_dim\n",
    "        self.weight = self.add_weight(shape=(embed_dim, self.feat_dim),\n",
    "                                       initializer=initializers.GlorotUniform(),\n",
    "                                       trainable=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    This normalization exists some problem to generate NaN value. Please amend. 23 Nov 2023 by JY\n",
    "    \"\"\"\n",
    "    def normalize_adjacency(self, adj):\n",
    "        d = tf.reduce_sum(adj, axis=-1)\n",
    "        d_sqrt_inv = tf.pow(d, -0.5)\n",
    "        d_sqrt_inv = tf.where(tf.math.is_inf(d_sqrt_inv), 0., d_sqrt_inv)\n",
    "        d_mat_inv_sqrt = tf.linalg.diag(d_sqrt_inv)\n",
    "        return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        # adj_normalized = self.normalize_adjacency(adj)\n",
    "        # print(adj_normalized)\n",
    "        combined = tf.linalg.matmul(adj, inputs)\n",
    "        outputs = tf.tensordot(combined, self.weight, axes=[[2], [1]])\n",
    "        \"\"\"\n",
    "        combined = tf.random.normal((40,52,125))\n",
    "        weight = tf.random.normal((64,125))\n",
    "        combined_transformed = tf.tensordot(combined, weight, axes=[[2], [1]])\n",
    "        print(combined_transformed.shape) # (40, 52, 64)\n",
    "        \"\"\"\n",
    "        return self.activation(outputs)\n",
    "    \n",
    "    \n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    # input_shape = (None, channel_size, sample_point, datapoint)\n",
    "    def __init__(self,\n",
    "                 dropout_rate,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 activation):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_layers,\n",
    "                               FFN_units,\n",
    "                               n_heads,\n",
    "                               dropout_rate,\n",
    "                               activation,\n",
    "                               name=\"encoder_1\")\n",
    "\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D(\n",
    "            data_format='channels_first', keepdims=False)\n",
    "\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        output_1 = self.encoder(inputs)  # self.encoder_1(output_1)\n",
    "\n",
    "        # output_1 = self.global_average_pooling(output_1)\n",
    "\n",
    "        return output_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
