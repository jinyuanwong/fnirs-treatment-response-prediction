{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 07:46:08.095752: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-03 07:46:08.117143: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7704] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-03 07:46:08.117171: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-03 07:46:08.117180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1520] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-03 07:46:08.121802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 07:46:08.632226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/jy/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.14.0-dev20230531). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "import sys\n",
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from utils.utils_mine import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "import wandb\n",
    "import config\n",
    "import gc\n",
    "from classifiers.classifier_factory import create_classifier\n",
    "from scripts.plot.DL.read_LOO_nestedCV_gnntr import get_sorted_loo_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 2)\n",
      "[7 3 2 8 5 6 9 4 0 1]\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "data = np.arange(100).reshape(10,5,2)\n",
    "# data = np.arange(10)\n",
    "label= np.arange(10)\n",
    "\n",
    "\n",
    "# Shuffle data and label arrays\n",
    "combined = list(zip(data, label))\n",
    "random.shuffle(combined)\n",
    "data, label = zip(*combined)\n",
    "\n",
    "# Convert back to numpy arrays\n",
    "data = np.array(data)\n",
    "label = np.array(label)\n",
    "\n",
    "# Print shuffled data\n",
    "print(data.shape)\n",
    "print(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_0/stratified_nested_5_CV_fold-0    '''\n",
    "\n",
    "\n",
    "def get_sorted_loo_array(model, model_params):\n",
    "    ALL_TOTAL_ITERATION = []\n",
    "    TOTAL_Subject = 65\n",
    "    K_FOLD = 5\n",
    "    validation_method_external = 'LOO_nested_CV'\n",
    "    validation_method_inner = 'stratified_nested_5_CV_fold'\n",
    "    DATASET = 'prognosis/pre_treatment_hamd_reduction_50'\n",
    "    RESULT_FILE_NAME = 'val_acc.txt'\n",
    "    val_fold_path = f'results/{model}/{DATASET}/{model_params}/{validation_method_external}'\n",
    "    total_subjects  = 46 if DATASET[:8] == 'pre_post' else TOTAL_Subject # '65' or '46\n",
    "\n",
    "\n",
    "    for subject in range(total_subjects):\n",
    "        for fold in range(K_FOLD):\n",
    "            fold_path = f'{val_fold_path}/LOO_{subject}/{validation_method_inner}-{fold}'\n",
    "            try:\n",
    "                with open(f'{fold_path}/{RESULT_FILE_NAME}', 'r') as f:\n",
    "                    best_iteration = int(f.read())\n",
    "                    ALL_TOTAL_ITERATION.append(best_iteration)\n",
    "            except:\n",
    "                print(f'Error: {fold_path}/best_iteration.txt not found')\n",
    "            \n",
    "    loo_toal_itr = np.array(ALL_TOTAL_ITERATION).copy()\n",
    "    loo_toal_itr = loo_toal_itr.reshape(-1, 5)\n",
    "    loo_toal_itr = np.mean(loo_toal_itr, axis=1)\n",
    "    sorted_indices = np.argsort(loo_toal_itr)\n",
    "    sorted_indices = sorted_indices.tolist()\n",
    "    print(\"Sorted indices:\", sorted_indices, \"Sorted values:\", loo_toal_itr[sorted_indices])\n",
    "    return sorted_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_path results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-0\n",
      "results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-0/val_acc.txt will be set to 0 because it has not been trained yet\n",
      "fold_path results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-1\n",
      "results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-1/val_acc.txt will be set to 0 because it has not been trained yet\n",
      "fold_path results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-2\n",
      "results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-2/val_acc.txt will be set to 0 because it has not been trained yet\n",
      "fold_path results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-3\n",
      "results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-3/val_acc.txt will be set to 0 because it has not been trained yet\n",
      "fold_path results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-4\n",
      "results/yu_gnn/prognosis/pre_treatment_hamd_reduction_50/v1/LOO_nested_CV/LOO_64/stratified_nested_5_CV_fold-4/val_acc.txt will be set to 0 because it has not been trained yet\n",
      "[64, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 33, 63, 32, 30, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = 'yu_gnn'\n",
    "model_params = 'v1'\n",
    "\n",
    "def avg_total_itr_for_each_fold(ALL_TOTAL_ITERATION):\n",
    "        loo_toal_itr = np.array(ALL_TOTAL_ITERATION).copy()\n",
    "        loo_toal_itr = loo_toal_itr.reshape(-1, 5)\n",
    "        loo_toal_itr = np.mean(loo_toal_itr, axis=1)\n",
    "        return loo_toal_itr\n",
    "def get_sorted_loo_array(model, model_params):\n",
    "\n",
    "    ALL_TOTAL_ITERATION = [] # store all the total iteration for each fold\n",
    "    TOTAL_Subject = 65 # number of subjects in the dataset for LOOCV in external testing set\n",
    "    K_FOLD = 5 # number of k folds in inner CV\n",
    "    validation_method_external = 'LOO_nested_CV' # external validation method\n",
    "    validation_method_inner = 'stratified_nested_5_CV_fold' # inner validation method\n",
    "    DATASET = 'prognosis/pre_treatment_hamd_reduction_50' # dataset name\n",
    "    RESULT_FILE_NAME = 'val_acc.txt' # result file name\n",
    "    val_fold_path = f'results/{model}/{DATASET}/{model_params}/{validation_method_external}'\n",
    "    total_subjects  = 46 if DATASET[:8] == 'pre_post' else TOTAL_Subject # '65' or '46\n",
    "\n",
    "\n",
    "    for subject in range(total_subjects):\n",
    "        for fold in range(K_FOLD):\n",
    "            fold_path = f'{val_fold_path}/LOO_{subject}/{validation_method_inner}-{fold}'\n",
    "            try:\n",
    "                with open(f'{fold_path}/{RESULT_FILE_NAME}', 'r') as f:\n",
    "\n",
    "                    total_lines = len(f.readlines())\n",
    "                    ALL_TOTAL_ITERATION.append(total_lines)\n",
    "            except:\n",
    "                # if the fold has not been created yet, then the total iteration is 0\n",
    "                print('fold_path', fold_path)\n",
    "                ALL_TOTAL_ITERATION.append(0)\n",
    "                print(f'{fold_path}/{RESULT_FILE_NAME} will be set to 0 because it has not been trained yet')\n",
    "    # average the total iteration for each fold\n",
    "    loo_toal_itr = avg_total_itr_for_each_fold(ALL_TOTAL_ITERATION)\n",
    "    sorted_indices = np.argsort(loo_toal_itr)\n",
    "    sorted_indices = sorted_indices.tolist()\n",
    "    \n",
    "    # print(\"Sorted indices:\", sorted_indices, \"Sorted values:\", loo_toal_itr[sorted_indices])\n",
    "    return sorted_indices\n",
    "\n",
    "sorted_indices = get_sorted_loo_array(model, model_params)\n",
    "print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42887446, 0.57112557], [0.8382789, 0.16172114], [0.5024056, 0.4975944]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_float(value):\n",
    "    value = np.array(value)\n",
    "    try:\n",
    "        final_value = value.astype(float)\n",
    "        np.nan_to_num(final_value, nan=0.0)\n",
    "    except ValueError:\n",
    "        final_value = 0.0\n",
    "    return final_value\n",
    "\n",
    "\n",
    "def read_file_metric_y_pred(path):\n",
    "    pattern = r\"Y_pred_in_test: \\[\\[(.*?)\\]\\]\"\n",
    "    with open(path, 'r') as f:\n",
    "        content = f.read()\n",
    "        y_pred = re.findall(pattern, content)\n",
    "    numbers_list = np.array([float(num) for s in y_pred for num in s.split()]).reshape(-1, 2).tolist()\n",
    "\n",
    "    return numbers_list\n",
    "\n",
    "str_list = read_file_metric_y_pred('/home/jy/Documents/fnirs/treatment_response/fnirs-depression-deeplearning/results/gnn_transformer/prognosis/pre_treatment_hamd_reduction_50/v2_repeat_3l1_rate_0.01_l2_rate_0.01_d_model_16_batch_size_64_n_layers_6/LOO_nested_CV/LOO_0/stratified_nested_5_CV_fold-1/test_acc.txt')\n",
    "print(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42887446, 0.57112557], [0.8382789, 0.16172114], [0.5024056, 0.4975944]]\n"
     ]
    }
   ],
   "source": [
    "numbers_list = np.array([float(num) for s in str_list for num in s.split()]).reshape(-1, 2).tolist()\n",
    "print(numbers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_fold_path results/gnn_transformer/prognosis/pre_treatment_hamd_reduction_50/v2_repeat_1l1_rate_0.01_l2_rate_0.01_d_model_16_batch_size_64_n_layers_6/LOO_nested_CV/LOO_0\n",
      "num_of_cv_folds 5\n",
      "y_test [0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1]\n",
      "MAX_ITR: 999 ranging ( 3 ~ 10 )\n",
      "Model name: gnn_transformer\n",
      "| Model Name | Testing Set |             |             |             | Validation Set |             |             |             |\n",
      "|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "|            | Balanced Accuracy | Sensitivity | Specificity | F1 Score | Balanced Accuracy | Sensitivity | Specificity | F1 Score |\n",
      "| gnn_transformer   | 65.0000  | 40.0000  | 90.0000  | 46.1538  | 83.2257  | 76.4617  | 89.9898  | 86.8218  |\n",
      "Sorted indices: [48, 33, 52, 51, 50, 49, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 0, 31, 54, 55, 56, 57, 58, 59, 60, 61, 62, 47, 30, 64, 28, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 29, 15, 17, 27, 26, 25, 24, 16, 22, 23, 19, 18, 53, 20, 35, 34, 42, 32, 63, 21] Sorted values: [3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      " 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      " 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      " 4.  4.  4.  4.2 5.  5.  5.  6.  6.  7.  7.8]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gnn_transformer'\n",
    "params = 'v2_repeat_1l1_rate_0.01_l2_rate_0.01_d_model_16_batch_size_64_n_layers_6'\n",
    "\n",
    "\n",
    "x = get_sorted_loo_array(model_name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 statistic: 22.384496032954758, p-value: 5.4253098815494074e-05\n",
      "Expected frequencies: [[47.9270073   2.44525547 14.67153285  1.95620438]\n",
      " [50.0729927   2.55474453 15.32846715  2.04379562]]\n",
      "responders {'age': [47, 24, 35, 24, 29, 32, 24, 21, 24, 23, 37, 23, 21, 41, 40], 'sex': [1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2], 'ethinicity': [1, 3, 2, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1], 'handness': [3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1], 'education': [16, 16, 16, 16, 13, 16, 16, 16, 13, 13, 13, 13, 16, 13, 16], 'pretreatment HAM-D score': [23, 15, 22, 25, 26, 22, 16, 31, 23, 21, 14, 23, 14, 13, 7], 'posttreatment HAM-D score': [10, 5, 3, 6, 13, 8, 5, 14, 10, 8, 4, 10, 7, 6, 0]}\n",
      "nonresponders {'age': [40, 23, 23, 21, 29, 42, 34, 38, 36, 21, 22, 48, 22, 23, 28, 30, 23, 38, 38, 41, 22, 43, 23, 23, 24, 23, 22, 32, 26, 25, 23, 25, 25, 25, 27, 23, 25, 37, 24, 23, 28, 26, 23, 25, 35, 31, 23, 36, 24], 'sex': [2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2], 'ethinicity': [1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 3, 2, 1, 3, 1, 1, 2, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1], 'handness': [3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1], 'education': [16, 16, 16, 13, 16, 16, 16, 16, 10, 13, 16, 10, 13, 16, 10, 16, 16, 13, 13, 13, 12, 16, 16, 16, 13, 13, 16, 13, 13, 10, 13, 16, 13, 16, 13, 16, 16, 16, 16, 16, 13, 16, 13, 13, 16, 16, 16, 16, 16], 'pretreatment HAM-D score': [25, 21, 27, 20, 17, 10, 15, 21, 22, 21, 21, 21, 19, 15, 21, 19, 27, 15, 19, 25, 24, 30, 24, 26, 20, 23, 16, 20, 24, 25, 22, 21, 26, 16, 12, 23, 15, 26, 18, 19, 16, 10, 24, 17, 14, 8, 11, 8, 19], 'posttreatment HAM-D score': [19, 14, 20, 15, 27, 15, 17, 29, 19, 17, 15, 16, 29, 14, 13, 19, 15, 9, 24, 16, 16, 17, 20, 15, 15, 22, 18, 15, 22, 23, 14, 20, 25, 16, 14, 20, 16, 17, 24, 31, 10, 16, 21, 14, 16, 12, 11, 6, 13]}\n",
      "show metrics of  HC\n",
      "age mean + std 28.426470588235293 7.236175819065556\n",
      "sex mean + std 1.7647058823529411 0.42418250299576354\n",
      "ethinicity mean + std 1.5147058823529411 0.776353600812677\n",
      "handness mean + std 1.1176470588235294 0.47058823529411764\n",
      "education mean + std 15.544117647058824 1.1811487868797075\n",
      "-------------------\n",
      "show metrics of  MDD\n",
      "age mean + std 28.32857142857143 7.164837608709891\n",
      "sex mean + std 1.7714285714285714 0.41991252733425916\n",
      "ethinicity mean + std 1.4714285714285715 0.7506459803096944\n",
      "handness mean + std 1.3142857142857143 0.7278708115918284\n",
      "education mean + std 14.528571428571428 1.8261702985164536\n",
      "-------------------\n",
      "show metrics of  RESPOND\n",
      "age mean + std 29.666666666666668 8.129507296810115\n",
      "sex mean + std 1.8 0.4\n",
      "ethinicity mean + std 1.4666666666666666 0.7180219742846006\n",
      "handness mean + std 1.5333333333333334 0.8844332774281066\n",
      "education mean + std 14.8 1.4696938456699067\n",
      "-------------------\n",
      "show metrics of  NONRESPOND\n",
      "age mean + std 28.387755102040817 7.0038366352660155\n",
      "sex mean + std 1.7755102040816326 0.4172458836787933\n",
      "ethinicity mean + std 1.4489795918367347 0.7575800487049219\n",
      "handness mean + std 1.2857142857142858 0.6998542122237652\n",
      "education mean + std 14.448979591836734 1.9489528793980933\n",
      "-------------------\n",
      "HCs vs MDDs\n",
      "age t_stat: 0.07927488132323106, p_value: 0.9369305126451928\n",
      "sex chi2_stat: 0.0, p_value: 1.0\n",
      "ethinicity chi2_stat: 0.1287661671003244, p_value: 0.9881748524803977\n",
      "handness chi2_stat: 2.5016212338593973, p_value: 0.11372916717403242\n",
      "education t_stat: 3.838757621074156, p_value: 0.0001887226662066213\n",
      "-------------------\n",
      "responders vs nonresponders\n",
      "age t_stat: 0.5856960944382984, p_value: 0.5602057796350506\n",
      "sex chi2_stat: 0.0, p_value: 1.0\n",
      "ethinicity chi2_stat: 0.4376358335542009, p_value: 0.9323626379750612\n",
      "handness chi2_stat: 0.5199099193708359, p_value: 0.4708801103487895\n",
      "education t_stat: 0.6336272766540696, p_value: 0.5286547495660197\n",
      "pretreatment HAM-D score t_stat: 0.07156294197781815, p_value: 0.9431800812223856\n",
      "posttreatment HAM-D score t_stat: -7.1356577370678655, p_value: 1.2518618228519112e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cortical haemodynamic response during the verbal fluency task in patients with bipolar disorder and borderline personality disorder: a preliminary functional near- infrared spectroscopy study\\n\\nTo determine if activation during the VFT occurred for each diagnostic group at each ROI, Student’s paired t- test was used to compare mean oxy-haemoglobin during the pre-task baseline period and task period. The effect of diagnostic group on categorical variables was deter- mined using chi-square test, while Student’s t-test or one-way analysis of variance (ANOVA) with Bonferroni corrected post-hoc pairwise comparisons, was used to determine the effect of diagnostic group on continuous variables. Categorical variables were gender, ethnicity, handedness, family psychiatric history, past admission to psychiatric ward and treatment with psychotropic drugs. Psychotropic drugs were further classified into antide- pressants, anxiolytics and sedatives, antipsychotics and mood stabilisers (Supplementary Table 1). Continuous variables were age, years of education, number of words generated, mean oxy-haemoglobin and mean deoxy- haemoglobin at each ROI, GAF score, HAM-D score, YMRS score, BPQ score, age at psychiatric illness onset, duration of psychiatric illness and equivalent doses of antidepressants, anxiolytics and sedatives, as well as anti- psychotics. Equivalent doses were calculated based on published mean dose ratios. Reference drugs were fluox- etine, diazepam and chlorpromazine, for antidepressants, anxiolytics and sedatives, and antipsychotics, respectively\\n\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is used to plot the demography of the population.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "data = [ [58, 4, 5, 0], [40, 1, 25, 4] ]\n",
    "chi2, p, dof, expected = stats.chi2_contingency(data)\n",
    "\n",
    "print(f\"Chi2 statistic: {chi2}, p-value: {p}\")\n",
    "print(\"Expected frequencies:\", expected)\n",
    "\n",
    "# Read the Excel file\n",
    "demography_path = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/fNIRS x MDD Data_Demographics_Clinical.xlsx'\n",
    "\n",
    "total_num_of_people = 72\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "because the excel file is not well organized, we need to find the correct column for each demographic data\n",
    "\"\"\"\n",
    "dict_demography = {\n",
    "    'age': 'Demographic Data',\n",
    "    'sex': 'Unnamed: 3',\n",
    "    'ethinicity': 'Unnamed: 4',\n",
    "    'handness': 'Unnamed: 5',\n",
    "    'education': 'Unnamed: 6',\n",
    "}\n",
    "\n",
    "dict_demography_treatment_response = {\n",
    "    'age': 'Demographic Data',\n",
    "    'sex': 'Unnamed: 3',\n",
    "    'ethinicity': 'Unnamed: 4',\n",
    "    'handness': 'Unnamed: 5',\n",
    "    'education': 'Unnamed: 6',\n",
    "    'pretreatment HAM-D score':'HAM-D Questionnaire (T1)',\n",
    "    'posttreatment HAM-D score':'HAM-D Questionnaire (T8)'\n",
    "    \n",
    "}\n",
    "\n",
    "excel_data = pd.read_excel(demography_path, sheet_name='Summary T0T8_fNIRS Analysis')\n",
    "all_involved_subject_id = []\n",
    "# MDD = excel_data[excel_data['Subject ID'][:2] == 'PT']\n",
    "\n",
    "def find_group_demography(type, specify_group=None):\n",
    "    res ={}\n",
    "    \n",
    "    if specify_group is not None:\n",
    "        for sub in specify_group:\n",
    "            for key in dict_demography_treatment_response:\n",
    "                if key not in res:\n",
    "                    res[key] = []\n",
    "                subject = excel_data['Subject ID'] == sub\n",
    "                value = excel_data[subject][dict_demography_treatment_response[key]].iloc[0]\n",
    "                if pd.isna(value):\n",
    "                    res[key].append(value)\n",
    "                else:\n",
    "                    res[key].append(int(value))\n",
    "        return res\n",
    "\n",
    "    for i in range(0, 1+total_num_of_people):\n",
    "        # find 'CT001' or 'CT010' or 'PT001' or 'PT010'\n",
    "        sub_id = type + '00' + str(i) if i < 10 else type + '0' + str(i)\n",
    "        sub = excel_data['Subject ID'] == sub_id\n",
    "        \n",
    "        # sub will be an array of [True, False, False, ...]\n",
    "        if True in sub.values:\n",
    "            if type == 'PT': all_involved_subject_id.append(sub_id)\n",
    "            for key in dict_demography:\n",
    "                if key not in res:\n",
    "                    res[key] = []\n",
    "                value = excel_data[sub][dict_demography[key]].iloc[0]\n",
    "\n",
    "                if pd.isna(value):\n",
    "                   res[key].append(value)\n",
    "                else:\n",
    "                    res[key].append(int(value))\n",
    "    return res \n",
    "\n",
    "def identify_responders_nonresponders():\n",
    "    responders_id, nonresponders_id = [], []\n",
    "    for subject in excel_data['Subject ID'][2:]:\n",
    "        # print('subject - >', subject)\n",
    "        \n",
    "        hamd_of_id_t1 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T1)'].iloc[0]\n",
    "        hamd_of_id_t8 = excel_data[excel_data['Subject ID'] == subject]['HAM-D Questionnaire (T8)'].iloc[0]\n",
    "        if type(hamd_of_id_t8) is not int:\n",
    "            # print(f\"subject: {subject} | hamd_of_id_t8: {hamd_of_id_t8} is not a number\")\n",
    "            continue\n",
    "        reduction_percentage = (hamd_of_id_t8 - hamd_of_id_t1) / hamd_of_id_t1\n",
    "        if reduction_percentage <= -0.5:\n",
    "            responders_id.append(subject)\n",
    "        else:\n",
    "            nonresponders_id.append(subject)\n",
    "    return responders_id, nonresponders_id\n",
    "\n",
    "def address_nan_value(data):\n",
    "    for key, value in data.items():\n",
    "\n",
    "        # find the nan value\n",
    "        nan_indice = pd.isna(value)\n",
    "        # get the median value \n",
    "        median = np.nanmedian(value)\n",
    "        for i, v in enumerate(value):\n",
    "            if pd.isna(v):\n",
    "                value[i] = int(median)\n",
    "        # data[key] = value\n",
    "    return data \n",
    "\n",
    "def show_metrics(data, name):\n",
    "    print('show metrics of ', name)\n",
    "    for key in dict_demography:\n",
    "        print(key, 'mean + std', np.mean(data[key]), np.std(data[key]))\n",
    "\n",
    "def compare_two_groups(g1, g2, name):\n",
    "    print(name)\n",
    "    if name == 'responders vs nonresponders':   \n",
    "        compare_dict_demography = dict_demography_treatment_response\n",
    "    else:\n",
    "        compare_dict_demography = dict_demography\n",
    "    for key in compare_dict_demography:\n",
    "        if key in ['age', 'education', 'pretreatment HAM-D score', 'posttreatment HAM-D score']:\n",
    "            t_stat, p_value = stats.ttest_ind(g1[key], g2[key])  # For normally distributed \n",
    "            print(f\"{key} t_stat: {t_stat}, p_value: {p_value}\")\n",
    "        if key in ['sex', 'ethinicity', 'handness']:\n",
    "            all_counter = Counter(g1[key] + g2[key])\n",
    "            g1_counter = Counter(g1[key])\n",
    "            g2_counter = Counter(g2[key])\n",
    "            g1_group = [g1_counter[i] for i, _ in all_counter.items()]\n",
    "            g2_group = [g2_counter[i] for i, _ in all_counter.items()]\n",
    "            stat, p_value, _, _ = stats.chi2_contingency([g1_group, g2_group])  # Use chi2_contingency for categorical variables\n",
    "            print(f\"{key} chi2_stat: {stat}, p_value: {p_value}\")\n",
    "\n",
    "\"\"\"\n",
    "HC, MDD, RESPOND, NONRESPOND have forms like this \n",
    "{\n",
    "    'age': [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
    "    'sex': [1, 2, 1, 2, 1, 2, 1, 2, 1, 2], # 1 is Male, 2 is Female \n",
    "    'ethinicity': [1, 2, 3, 4, 1, 2, 3, 3, 4, 4], # 1 is Chinese, 2 is Malay, 3 is Indian, 4 is Others \n",
    "    'Handedness': [1, 2, 3, 3, 1, 2, 3, 2, 1, 2], # 1 is Right, 2 is Left, 3 is Ambidextrous\n",
    "    'education': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21], # years of education\n",
    "}\n",
    "\"\"\"\n",
    "HC = find_group_demography('CT')\n",
    "MDD = find_group_demography('PT')\n",
    "MDD = address_nan_value(MDD)\n",
    "# print(excel_data)\n",
    "resp, nonresp = identify_responders_nonresponders()\n",
    "RESPOND = find_group_demography(None, resp)\n",
    "NONRESPOND = address_nan_value(find_group_demography(None, nonresp))\n",
    "print('responders', RESPOND)\n",
    "print('nonresponders', NONRESPOND)\n",
    "\n",
    "show_metrics(HC,'HC')\n",
    "print('-------------------')\n",
    "show_metrics(MDD,'MDD')\n",
    "print('-------------------')\n",
    "show_metrics(RESPOND,'RESPOND')\n",
    "print('-------------------')\n",
    "show_metrics(NONRESPOND,'NONRESPOND')\n",
    "\n",
    "\n",
    "print('-------------------')\n",
    "compare_two_groups(HC, MDD, 'HCs vs MDDs')\n",
    "print('-------------------')\n",
    "compare_two_groups(RESPOND, NONRESPOND, 'responders vs nonresponders')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Cortical haemodynamic response during the verbal fluency task in patients with bipolar disorder and borderline personality disorder: a preliminary functional near- infrared spectroscopy study\n",
    "\n",
    "To determine if activation during the VFT occurred for each diagnostic group at each ROI, Student’s paired t- test was used to compare mean oxy-haemoglobin during the pre-task baseline period and task period. The effect of diagnostic group on categorical variables was deter- mined using chi-square test, while Student’s t-test or one-way analysis of variance (ANOVA) with Bonferroni corrected post-hoc pairwise comparisons, was used to determine the effect of diagnostic group on continuous variables. Categorical variables were gender, ethnicity, handedness, family psychiatric history, past admission to psychiatric ward and treatment with psychotropic drugs. Psychotropic drugs were further classified into antide- pressants, anxiolytics and sedatives, antipsychotics and mood stabilisers (Supplementary Table 1). Continuous variables were age, years of education, number of words generated, mean oxy-haemoglobin and mean deoxy- haemoglobin at each ROI, GAF score, HAM-D score, YMRS score, BPQ score, age at psychiatric illness onset, duration of psychiatric illness and equivalent doses of antidepressants, anxiolytics and sedatives, as well as anti- psychotics. Equivalent doses were calculated based on published mean dose ratios. Reference drugs were fluox- etine, diazepam and chlorpromazine, for antidepressants, anxiolytics and sedatives, and antipsychotics, respectively\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "continues = ['age', 'education', 'pretreatment HAM-D score', 'posttreatment HAM-D score']\n",
    "categorical = ['sex', 'ethinicity', 'handness']\n",
    "\n",
    "ethinicity = {\n",
    "    1: 'Chinese',\n",
    "    2: 'Malay',\n",
    "    3: 'Indian',\n",
    "    4: 'Others'\n",
    "}\n",
    "handness = {\n",
    "    1: 'Right',\n",
    "    2: 'Left',\n",
    "    3: 'Ambidextrous'\n",
    "}\n",
    "\n",
    "sex_category = {\n",
    "    1: 'Male',\n",
    "    2: 'Female'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def print_demo_numer(data):\n",
    "    for key, value in data.items():\n",
    "        if key in continues:\n",
    "            mean = np.mean(value)\n",
    "            std = np.std(value)\n",
    "            print(\"{} mean = {:.2f}, std = {:.2f}\".format(key, mean, std))\n",
    "        if key in categorical:\n",
    "            counter = Counter(value)\n",
    "            Total = sum(counter.values())\n",
    "\n",
    "            # for ehinicity:\n",
    "            if key == 'ethinicity':\n",
    "                for k, v in counter.items():\n",
    "                    print(f\"{ethinicity[k]} N = {v}, % = {v/Total*100:.2f}\")\n",
    "                    \n",
    "            # for handness\n",
    "            if key == 'handness':\n",
    "                for k, b in counter.items():\n",
    "                    print(f\"{handness[k]} N = {b}, % = {b/Total*100:.2f}\")\n",
    "                    \n",
    "            # fir sex_category\n",
    "            if key =='sex':\n",
    "                for k, b in counter.items():\n",
    "                    print(f\"{sex_category[k]} N = {b}, % = {b/Total*100:.2f}\")\n",
    "            print('-')\n",
    "print(\"For responders, total subject is \", len(RESPOND['age']))\n",
    "print_demo_numer(RESPOND)\n",
    "\n",
    "print('-------------------')\n",
    "print(\"For nonresponders, total subject is \", len(NONRESPOND['age']))\n",
    "print_demo_numer(NONRESPOND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': [47, 24, 35, 24, 29, 32, 24, 21, 24, 23, 37, 23, 21, 41, 40], 'sex': [1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2], 'ethinicity': [1, 3, 2, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1], 'handness': [3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1], 'education': [16, 16, 16, 16, 13, 16, 16, 16, 13, 13, 13, 13, 16, 13, 16], 'pretreatment HAM-D score': [23, 15, 22, 25, 26, 22, 16, 31, 23, 21, 14, 23, 14, 13, 7], 'posttreatment HAM-D score': [10, 5, 3, 6, 13, 8, 5, 14, 10, 8, 4, 10, 7, 6, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(RESPOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_involve_subject65= ['PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT054', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PT001', 'PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT035', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT052', 'PT053', 'PT054', 'PT055', 'PT056', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n"
     ]
    }
   ],
   "source": [
    "all_subject_71 = ['PT001', 'PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT035', 'PT036', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT052', 'PT053', 'PT054', 'PT055', 'PT056', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n",
    "print(all_subject_71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PT001', 'PT002', 'PT003', 'PT004', 'PT005', 'PT006', 'PT008', 'PT009', 'PT010', 'PT011', 'PT012', 'PT013', 'PT014', 'PT015', 'PT016', 'PT017', 'PT018', 'PT019', 'PT020', 'PT021', 'PT022', 'PT023', 'PT024', 'PT025', 'PT026', 'PT027', 'PT028', 'PT029', 'PT030', 'PT031', 'PT032', 'PT033', 'PT034', 'PT035', 'PT036', 'PT037', 'PT038', 'PT039', 'PT040', 'PT041', 'PT042', 'PT043', 'PT044', 'PT045', 'PT046', 'PT047', 'PT048', 'PT049', 'PT050', 'PT051', 'PT052', 'PT053', 'PT054', 'PT055', 'PT056', 'PT057', 'PT058', 'PT059', 'PT060', 'PT061', 'PT062', 'PT063', 'PT064', 'PT065', 'PT066', 'PT067', 'PT068', 'PT069', 'PT070', 'PT071']\n"
     ]
    }
   ],
   "source": [
    "print(all_involved_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "71\n",
      " 0 - all_involved_subject_id: PT001 - all_subject_71: PT001\n",
      " 1 - all_involved_subject_id: PT002 - all_subject_71: PT002\n",
      " 2 - all_involved_subject_id: PT003 - all_subject_71: PT003\n",
      " 3 - all_involved_subject_id: PT004 - all_subject_71: PT004\n",
      " 4 - all_involved_subject_id: PT005 - all_subject_71: PT005\n",
      " 5 - all_involved_subject_id: PT006 - all_subject_71: PT006\n",
      " 6 - all_involved_subject_id: PT008 - all_subject_71: PT008\n",
      " 7 - all_involved_subject_id: PT009 - all_subject_71: PT009\n",
      " 8 - all_involved_subject_id: PT010 - all_subject_71: PT010\n",
      " 9 - all_involved_subject_id: PT011 - all_subject_71: PT011\n",
      " 10 - all_involved_subject_id: PT012 - all_subject_71: PT012\n",
      " 11 - all_involved_subject_id: PT013 - all_subject_71: PT013\n",
      " 12 - all_involved_subject_id: PT014 - all_subject_71: PT014\n",
      " 13 - all_involved_subject_id: PT015 - all_subject_71: PT015\n",
      " 14 - all_involved_subject_id: PT016 - all_subject_71: PT016\n",
      " 15 - all_involved_subject_id: PT017 - all_subject_71: PT017\n",
      " 16 - all_involved_subject_id: PT018 - all_subject_71: PT018\n",
      " 17 - all_involved_subject_id: PT019 - all_subject_71: PT019\n",
      " 18 - all_involved_subject_id: PT020 - all_subject_71: PT020\n",
      " 19 - all_involved_subject_id: PT021 - all_subject_71: PT021\n",
      " 20 - all_involved_subject_id: PT022 - all_subject_71: PT022\n",
      " 21 - all_involved_subject_id: PT023 - all_subject_71: PT023\n",
      " 22 - all_involved_subject_id: PT024 - all_subject_71: PT024\n",
      " 23 - all_involved_subject_id: PT025 - all_subject_71: PT025\n",
      " 24 - all_involved_subject_id: PT026 - all_subject_71: PT026\n",
      " 25 - all_involved_subject_id: PT027 - all_subject_71: PT027\n",
      " 26 - all_involved_subject_id: PT028 - all_subject_71: PT028\n",
      " 27 - all_involved_subject_id: PT029 - all_subject_71: PT029\n",
      " 28 - all_involved_subject_id: PT030 - all_subject_71: PT030\n",
      " 29 - all_involved_subject_id: PT031 - all_subject_71: PT031\n",
      " 30 - all_involved_subject_id: PT032 - all_subject_71: PT032\n",
      " 31 - all_involved_subject_id: PT033 - all_subject_71: PT033\n",
      " 32 - all_involved_subject_id: PT034 - all_subject_71: PT034\n",
      " 33 - all_involved_subject_id: PT035 - all_subject_71: PT035\n",
      " 34 - all_involved_subject_id: PT036 - all_subject_71: PT036\n",
      " 35 - all_involved_subject_id: PT037 - all_subject_71: PT036\n",
      " 36 - all_involved_subject_id: PT038 - all_subject_71: PT037\n",
      " 37 - all_involved_subject_id: PT039 - all_subject_71: PT038\n",
      " 38 - all_involved_subject_id: PT040 - all_subject_71: PT039\n",
      " 39 - all_involved_subject_id: PT041 - all_subject_71: PT040\n",
      " 40 - all_involved_subject_id: PT042 - all_subject_71: PT041\n",
      " 41 - all_involved_subject_id: PT043 - all_subject_71: PT042\n",
      " 42 - all_involved_subject_id: PT044 - all_subject_71: PT043\n",
      " 43 - all_involved_subject_id: PT045 - all_subject_71: PT044\n",
      " 44 - all_involved_subject_id: PT046 - all_subject_71: PT045\n",
      " 45 - all_involved_subject_id: PT047 - all_subject_71: PT046\n",
      " 46 - all_involved_subject_id: PT048 - all_subject_71: PT047\n",
      " 47 - all_involved_subject_id: PT049 - all_subject_71: PT048\n",
      " 48 - all_involved_subject_id: PT050 - all_subject_71: PT049\n",
      " 49 - all_involved_subject_id: PT051 - all_subject_71: PT050\n",
      " 50 - all_involved_subject_id: PT052 - all_subject_71: PT051\n",
      " 51 - all_involved_subject_id: PT053 - all_subject_71: PT052\n",
      " 52 - all_involved_subject_id: PT054 - all_subject_71: PT053\n",
      " 53 - all_involved_subject_id: PT055 - all_subject_71: PT054\n",
      " 54 - all_involved_subject_id: PT056 - all_subject_71: PT055\n",
      " 55 - all_involved_subject_id: PT057 - all_subject_71: PT056\n",
      " 56 - all_involved_subject_id: PT058 - all_subject_71: PT057\n",
      " 57 - all_involved_subject_id: PT059 - all_subject_71: PT058\n",
      " 58 - all_involved_subject_id: PT060 - all_subject_71: PT059\n",
      " 59 - all_involved_subject_id: PT061 - all_subject_71: PT060\n",
      " 60 - all_involved_subject_id: PT062 - all_subject_71: PT061\n",
      " 61 - all_involved_subject_id: PT063 - all_subject_71: PT062\n",
      " 62 - all_involved_subject_id: PT064 - all_subject_71: PT063\n",
      " 63 - all_involved_subject_id: PT065 - all_subject_71: PT064\n",
      " 64 - all_involved_subject_id: PT066 - all_subject_71: PT065\n",
      " 65 - all_involved_subject_id: PT067 - all_subject_71: PT066\n",
      " 66 - all_involved_subject_id: PT068 - all_subject_71: PT067\n",
      " 67 - all_involved_subject_id: PT069 - all_subject_71: PT068\n",
      " 68 - all_involved_subject_id: PT070 - all_subject_71: PT069\n",
      " 69 - all_involved_subject_id: PT071 - all_subject_71: PT070\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_subject_71))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m71\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - all_involved_subject_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_involved_subject_id[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - all_subject_71: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_subject_71[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(len(all_involved_subject_id))\n",
    "print(len(all_subject_71))\n",
    "\n",
    "\n",
    "for i in range(71):\n",
    "    print(f\" {i} - all_involved_subject_id: {all_involved_subject_id[i]} - all_subject_71: {all_subject_71[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 52, 2502)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '/Users/shanxiafeng/Documents/Project/Research/fnirs-prognosis/code/fnirs-treatment-response-prediction/allData/prognosis/pre_treatment_hamd_reduction_50/hb_data.npy'\n",
    "data = np.load(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Subject ID  Group Demographic Data Unnamed: 3 Unnamed: 4  Unnamed: 5  \\\n",
      "0          NaN    NaN              NaN        NaN        NaN         NaN   \n",
      "1          NaN    NaN      Age (years)        Sex  Ethnicity  Handedness   \n",
      "2        CT001    0.0               36          2          1           1   \n",
      "3        CT002    0.0               36          1          1           1   \n",
      "4        CT003    0.0               21          2          1           1   \n",
      "..         ...    ...              ...        ...        ...         ...   \n",
      "137      PT067    1.0               31          2          1           1   \n",
      "138      PT068    1.0               40          2          1           1   \n",
      "139      PT069    1.0               23          2          1           1   \n",
      "140      PT070    1.0               36          2          1           1   \n",
      "141      PT071    1.0               24          2          1           1   \n",
      "\n",
      "            Unnamed: 6                                Unnamed: 7  \\\n",
      "0                  NaN                                       NaN   \n",
      "1    Education (years)  Current/highest education level —  Coded   \n",
      "2                   16                                         6   \n",
      "3                   16                                         6   \n",
      "4                   16                                         6   \n",
      "..                 ...                                       ...   \n",
      "137                 16                                         6   \n",
      "138                 16                                         6   \n",
      "139                 16                                         6   \n",
      "140                 16                                         6   \n",
      "141                 16                                         6   \n",
      "\n",
      "                   Unnamed: 8 Psychiatric History  ...  \\\n",
      "0                         NaN                 NaN  ...   \n",
      "1    Perceived social support         Past trauma  ...   \n",
      "2                           3                   0  ...   \n",
      "3                           3                   1  ...   \n",
      "4                           3                   0  ...   \n",
      "..                        ...                 ...  ...   \n",
      "137                         3                   1  ...   \n",
      "138                         3                   0  ...   \n",
      "139                         3                   1  ...   \n",
      "140                         3                   0  ...   \n",
      "141                         2                   0  ...   \n",
      "\n",
      "                       Unnamed: 30                     Unnamed: 31  \\\n",
      "0                              NaN                             NaN   \n",
      "1    HAM-D insomnia dominant (> 3)  HAM-D anxiety dominant \\n(> 4)   \n",
      "2                                0                               0   \n",
      "3                                0                               0   \n",
      "4                                0                               0   \n",
      "..                             ...                             ...   \n",
      "137                              0                               0   \n",
      "138                              0                               0   \n",
      "139                              0                               0   \n",
      "140                              0                               0   \n",
      "141                              0                               0   \n",
      "\n",
      "                        Unnamed: 32                          Unnamed: 33  \\\n",
      "0                               NaN                                  NaN   \n",
      "1    HAM-D somatic dominant \\n(> 2)  HAM-D melancholia dominant \\n(> 11)   \n",
      "2                                 0                                    0   \n",
      "3                                 0                                    0   \n",
      "4                                 0                                    0   \n",
      "..                              ...                                  ...   \n",
      "137                               0                                    0   \n",
      "138                               0                                    0   \n",
      "139                               0                                    0   \n",
      "140                               0                                    0   \n",
      "141                               1                                    0   \n",
      "\n",
      "                                Unnamed: 34 HAM-D Questionnaire (T8)  \\\n",
      "0                                       NaN                      NaN   \n",
      "1    HAM-D response-based dominant \\n(> 13)              Total score   \n",
      "2                                         0                      NaN   \n",
      "3                                         0                      NaN   \n",
      "4                                         0                      NaN   \n",
      "..                                      ...                      ...   \n",
      "137                                       0                       12   \n",
      "138                                       0                        0   \n",
      "139                                       0                       11   \n",
      "140                                       0                        6   \n",
      "141                                       0                       13   \n",
      "\n",
      "    Unnamed: 36                          Unnamed: 37     Unnamed: 38  \\\n",
      "0           NaN                                  NaN             NaN   \n",
      "1         T8/T0  Percentage reduction of HAM-D score  Response group   \n",
      "2           NaN                                  NaN             NaN   \n",
      "3           NaN                                  NaN             NaN   \n",
      "4           NaN                                  NaN             NaN   \n",
      "..          ...                                  ...             ...   \n",
      "137         150                                  -50               3   \n",
      "138           0                                  100               1   \n",
      "139         100                                    0               3   \n",
      "140          75                                   25               2   \n",
      "141       68.42                                31.58               2   \n",
      "\n",
      "        Unnamed: 39  \n",
      "0               NaN  \n",
      "1    Response speed  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "..              ...  \n",
      "137             NaN  \n",
      "138               1  \n",
      "139             NaN  \n",
      "140               1  \n",
      "141               3  \n",
      "\n",
      "[142 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(excel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              NaN\n",
      "1      Total score\n",
      "2                0\n",
      "3                0\n",
      "4                0\n",
      "          ...     \n",
      "137              8\n",
      "138              7\n",
      "139             11\n",
      "140              8\n",
      "141             19\n",
      "Name: HAM-D Questionnaire (T1), Length: 142, dtype: object\n"
     ]
    }
   ],
   "source": [
    "excel_data = pd.read_excel(demography_path, sheet_name='Summary T0T8_fNIRS Analysis')\n",
    "# 'pretreatment HAM-D score':'HAM-D Questionnaire (T1)'\n",
    "# 'posttreatment HAM-D score':'HAM-D Questionnaire (T8)'\n",
    "NAME = 'HAM-D Questionnaire (T1)'\n",
    "print(excel_data[NAME])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
